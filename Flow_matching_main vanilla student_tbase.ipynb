{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e499c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import flow_matching\n",
    "from flow_matching.path.scheduler import CondOTScheduler\n",
    "from flow_matching.path import AffineProbPath\n",
    "from flow_matching.solver import Solver, ODESolver\n",
    "from flow_matching.utils import ModelWrapper\n",
    "from Distributions2 import *\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "from flow import build_ttf_m\n",
    "from network import MLP\n",
    "\n",
    "# To avoide meshgrid warning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8596bb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    print('Using gpu')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using cpu.')\n",
    "torch.manual_seed(42)\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c373ee",
   "metadata": {},
   "source": [
    "## Sampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a006f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINT MIX OF STUDENT-T\n",
    "dof_og=1\n",
    "ST_DATA,objectz=samplestudentT_4(50,dof_og,40000)\n",
    "# plt.scatter(ST_DATA[:,0],ST_DATA[:,1],c=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "201f9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETTING THE Data split and TAIL INDEX ESTIMATES\n",
    "from generate_splits import generate_data_split\n",
    "from Distributions2 import sample_stdentt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f07677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:53<00:00, 26.96s/it]\n"
     ]
    }
   ],
   "source": [
    "Data_Splt=generate_data_split('gppg', seed, 'dummy',ST_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003f5138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dfs': [0.9689141215408199, 0.9461591723366531],\n",
       " 'pos_dfs': [0.0, 1.0584125863547804],\n",
       " 'neg_dfs': [0.7025663869297953, 0.870418293978243],\n",
       " 'mean': [-2.426814, -0.6382829],\n",
       " 'std': [350.42422, 127.15294],\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_Splt['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9616848e",
   "metadata": {},
   "source": [
    "## Building the Tail Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a71a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ttf_rqs(dim, dfs):\n",
    "#     return flows.build_ttf_m(\n",
    "#         dim,\n",
    "#         model_kwargs=dict(\n",
    "#             fix_tails=False,\n",
    "#             pos_tail_init=[\n",
    "#                 float(t.cpu()) for t in torch.distributions.Uniform(low=0.05, high=1.0).sample([dim])\n",
    "#             ],\n",
    "#             neg_tail_init=[\n",
    "#                 float(t.cpu()) for t in torch.distributions.Uniform(low=0.05, high=1.0).sample([dim])\n",
    "#             ]\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "\n",
    "# def ttf_rqs_fix(dim, dfs):\n",
    "#     return build_ttf_m(\n",
    "#         dim,\n",
    "#         model_kwargs=dict(\n",
    "#             fix_tails=False,#ADITYA CHANGED THIS\n",
    "#             pos_tail_init=[1 / df if df != 0.0 else 1e-4 for df in dfs['metadata']['pos_dfs']],  #DIFFFERENT FROM MAIN CODE\n",
    "#             neg_tail_init=[1 / df if df != 0.0 else 1e-4 for df in dfs['metadata']['neg_dfs']],  #DIFFFERENT FROM MAIN CODE\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27bb3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(ttf_rqs_fix(2,Data_Splt).parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a59eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "lr = 0.001\n",
    "batch_size = 4096\n",
    "iterations = 800\n",
    "print_every = 1\n",
    "hidden_dim = 512\n",
    "dimension=2\n",
    "device\n",
    "# velocity field model init\n",
    "vf = MLP(input_dim=2, time_dim=1, hidden_dim=hidden_dim).to(device)\n",
    "# data2noiseTransform=ttf_rqs_fix(dimension,Data_Splt).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "951d67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(data2noiseTransform.parameters())\n",
    "# data2noiseTransform(torch.randn(22,2).to(device))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bbbeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ix = Data_Splt[\"split\"][\"trn\"]\n",
    "val_ix = Data_Splt[\"split\"][\"val\"]\n",
    "tst_ix = Data_Splt[\"split\"][\"tst\"]\n",
    "\n",
    "mean = torch.tensor(Data_Splt['metadata'][\"mean\"])\n",
    "scale = torch.tensor(Data_Splt['metadata'][\"std\"])\n",
    "\n",
    "x_trn =ST_DATA[trn_ix] #(ST_DATA[trn_ix] - mean) / scale\n",
    "x_val =ST_DATA[val_ix] #(ST_DATA[val_ix] - mean) / scale\n",
    "x_tst =ST_DATA[tst_ix] #(ST_DATA[tst_ix] - mean) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6a753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35a5f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reverse_normalization(x):\n",
    "#     y=(x*scale)+mean\n",
    "#     return(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfd7ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_trn)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4c759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36d343dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| iter      2 | 3670.36 ms/step | loss 28061.549 \n",
      "| iter      3 | 31.02 ms/step | loss 13255.014 \n",
      "| iter      4 | 25.02 ms/step | loss 365664.406 \n",
      "| iter      5 | 26.01 ms/step | loss 14280.035 \n",
      "| iter      6 | 24.01 ms/step | loss 9272.260 \n",
      "| iter      7 | 26.01 ms/step | loss 92257.359 \n",
      "| iter      8 | 298.07 ms/step | loss 34866.219 \n",
      "| iter      9 | 25.01 ms/step | loss 408940.469 \n",
      "| iter     10 | 28.01 ms/step | loss 8436.281 \n",
      "| iter     11 | 25.01 ms/step | loss 5515.882 \n",
      "| iter     12 | 28.01 ms/step | loss 27923.156 \n",
      "| iter     13 | 26.01 ms/step | loss 20668138.000 \n",
      "| iter     14 | 30.99 ms/step | loss 16347.607 \n",
      "| iter     15 | 28.01 ms/step | loss 24963.125 \n",
      "| iter     16 | 25.01 ms/step | loss 6679.527 \n",
      "| iter     17 | 24.01 ms/step | loss 386260.625 \n",
      "| iter     18 | 24.01 ms/step | loss 34073.270 \n",
      "| iter     19 | 171.04 ms/step | loss 375107.656 \n",
      "| iter     20 | 25.01 ms/step | loss 7430.273 \n",
      "| iter     21 | 22.00 ms/step | loss 9719.587 \n",
      "| iter     22 | 27.01 ms/step | loss 388964.031 \n",
      "| iter     23 | 26.01 ms/step | loss 154688.156 \n",
      "| iter     24 | 22.00 ms/step | loss 1243680.375 \n",
      "| iter     25 | 26.01 ms/step | loss 8016.583 \n",
      "| iter     26 | 28.01 ms/step | loss 26936.920 \n",
      "| iter     27 | 24.01 ms/step | loss 113623.625 \n",
      "| iter     28 | 109.02 ms/step | loss 235362.531 \n",
      "| iter     29 | 25.01 ms/step | loss 1437186.125 \n",
      "| iter     30 | 27.01 ms/step | loss 14532.418 \n",
      "| iter     31 | 27.01 ms/step | loss 358844.594 \n",
      "| iter     32 | 24.01 ms/step | loss 26416.715 \n",
      "| iter     33 | 23.01 ms/step | loss 81663.688 \n",
      "| iter     34 | 27.01 ms/step | loss 13257.564 \n",
      "| iter     35 | 24.01 ms/step | loss 363421.250 \n",
      "| iter     36 | 26.00 ms/step | loss 661432.875 \n",
      "| iter     37 | 110.02 ms/step | loss 45199.301 \n",
      "| iter     38 | 28.01 ms/step | loss 12105.729 \n",
      "| iter     39 | 25.01 ms/step | loss 355443.969 \n",
      "| iter     40 | 25.01 ms/step | loss 24677.766 \n",
      "| iter     41 | 25.01 ms/step | loss 16416.320 \n",
      "| iter     42 | 28.01 ms/step | loss 345628.062 \n",
      "| iter     43 | 26.01 ms/step | loss 10571.619 \n",
      "| iter     44 | 27.01 ms/step | loss 6087.465 \n",
      "| iter     45 | 25.01 ms/step | loss 69251.953 \n",
      "| iter     46 | 29.01 ms/step | loss 367173.906 \n",
      "| iter     47 | 105.04 ms/step | loss 10227.499 \n",
      "| iter     48 | 23.01 ms/step | loss 119891.273 \n",
      "| iter     49 | 23.01 ms/step | loss 6263.260 \n",
      "| iter     50 | 28.01 ms/step | loss 36096.375 \n",
      "| iter     51 | 26.01 ms/step | loss 9021.832 \n",
      "| iter     52 | 24.01 ms/step | loss 986603.250 \n",
      "| iter     53 | 25.00 ms/step | loss 39071.723 \n",
      "| iter     54 | 23.01 ms/step | loss 12815.494 \n",
      "| iter     55 | 111.02 ms/step | loss 343759.688 \n",
      "| iter     56 | 25.01 ms/step | loss 120274.430 \n",
      "| iter     57 | 25.01 ms/step | loss 3052.261 \n",
      "| iter     58 | 23.01 ms/step | loss 33276.465 \n",
      "| iter     59 | 26.01 ms/step | loss 24419.398 \n",
      "| iter     60 | 26.00 ms/step | loss 475173.406 \n",
      "| iter     61 | 23.01 ms/step | loss 23643.295 \n",
      "| iter     62 | 24.01 ms/step | loss 371901.312 \n",
      "| iter     63 | 27.01 ms/step | loss 8222.736 \n",
      "| iter     64 | 24.01 ms/step | loss 5396.215 \n",
      "| iter     65 | 22.00 ms/step | loss 7504.884 \n",
      "| iter     66 | 103.02 ms/step | loss 506932.062 \n",
      "| iter     67 | 22.00 ms/step | loss 19512.791 \n",
      "| iter     68 | 24.01 ms/step | loss 11266.750 \n",
      "| iter     69 | 22.01 ms/step | loss 110367784.000 \n",
      "| iter     70 | 23.99 ms/step | loss 5007.717 \n",
      "| iter     71 | 24.99 ms/step | loss 80076.562 \n",
      "| iter     72 | 22.01 ms/step | loss 365591.969 \n",
      "| iter     73 | 23.99 ms/step | loss 34459.695 \n",
      "| iter     74 | 29.01 ms/step | loss 3822922.250 \n",
      "| iter     75 | 22.01 ms/step | loss 389216.281 \n",
      "| iter     76 | 105.02 ms/step | loss 11432.936 \n",
      "| iter     77 | 25.01 ms/step | loss 186290.641 \n",
      "| iter     78 | 28.01 ms/step | loss 95013.586 \n",
      "| iter     79 | 22.01 ms/step | loss 29798.383 \n",
      "| iter     80 | 26.01 ms/step | loss 100654.984 \n",
      "| iter     81 | 24.01 ms/step | loss 415510.250 \n",
      "| iter     82 | 25.01 ms/step | loss 404542.938 \n",
      "| iter     83 | 23.01 ms/step | loss 81042.648 \n",
      "| iter     84 | 105.03 ms/step | loss 378217.219 \n",
      "| iter     85 | 24.01 ms/step | loss 183779.484 \n",
      "| iter     86 | 26.01 ms/step | loss 8960.715 \n",
      "| iter     87 | 24.00 ms/step | loss 3891.518 \n",
      "| iter     88 | 24.01 ms/step | loss 103630.812 \n",
      "| iter     89 | 22.00 ms/step | loss 451563.750 \n",
      "| iter     90 | 29.01 ms/step | loss 234552.031 \n",
      "| iter     91 | 22.00 ms/step | loss 412990.250 \n",
      "| iter     92 | 22.01 ms/step | loss 10490.082 \n",
      "| iter     93 | 106.03 ms/step | loss 10968.599 \n",
      "| iter     94 | 26.02 ms/step | loss 514478.250 \n",
      "| iter     95 | 27.01 ms/step | loss 13620.213 \n",
      "| iter     96 | 25.01 ms/step | loss 30838.127 \n",
      "| iter     97 | 24.01 ms/step | loss 7922.008 \n",
      "| iter     98 | 23.01 ms/step | loss 12147.357 \n",
      "| iter     99 | 28.01 ms/step | loss 409652.281 \n",
      "| iter    100 | 26.02 ms/step | loss 4003.477 \n",
      "| iter    101 | 24.01 ms/step | loss 6098.948 \n",
      "| iter    102 | 29.00 ms/step | loss 72171.609 \n",
      "| iter    103 | 106.03 ms/step | loss 29431.584 \n",
      "| iter    104 | 25.01 ms/step | loss 52552.605 \n",
      "| iter    105 | 21.01 ms/step | loss 428995.125 \n",
      "| iter    106 | 23.99 ms/step | loss 10683.949 \n",
      "| iter    107 | 23.00 ms/step | loss 400856.406 \n",
      "| iter    108 | 22.01 ms/step | loss 16224.490 \n",
      "| iter    109 | 24.01 ms/step | loss 37499.457 \n",
      "| iter    110 | 27.02 ms/step | loss 9631.611 \n",
      "| iter    111 | 111.03 ms/step | loss 8703.771 \n",
      "| iter    112 | 28.01 ms/step | loss 434737.406 \n",
      "| iter    113 | 25.01 ms/step | loss 20025.479 \n",
      "| iter    114 | 25.01 ms/step | loss 18718.184 \n",
      "| iter    115 | 25.01 ms/step | loss 34440.371 \n",
      "| iter    116 | 23.01 ms/step | loss 374447.375 \n",
      "| iter    117 | 21.00 ms/step | loss 195700.375 \n",
      "| iter    118 | 27.00 ms/step | loss 416111.906 \n",
      "| iter    119 | 27.01 ms/step | loss 120603.250 \n",
      "| iter    120 | 111.01 ms/step | loss 8274.490 \n",
      "| iter    121 | 22.01 ms/step | loss 7633.355 \n",
      "| iter    122 | 23.01 ms/step | loss 3644763.000 \n",
      "| iter    123 | 23.01 ms/step | loss 15573.102 \n",
      "| iter    124 | 24.01 ms/step | loss 142965.656 \n",
      "| iter    125 | 23.02 ms/step | loss 439729.688 \n",
      "| iter    126 | 23.00 ms/step | loss 261441.469 \n",
      "| iter    127 | 25.01 ms/step | loss 399944.500 \n",
      "| iter    128 | 25.01 ms/step | loss 5937.258 \n",
      "| iter    129 | 23.01 ms/step | loss 7206.958 \n",
      "| iter    130 | 105.03 ms/step | loss 29339.635 \n",
      "| iter    131 | 25.01 ms/step | loss 2378527.500 \n",
      "| iter    132 | 29.01 ms/step | loss 16180.680 \n",
      "| iter    133 | 24.01 ms/step | loss 12416.071 \n",
      "| iter    134 | 26.01 ms/step | loss 8850.888 \n",
      "| iter    135 | 27.01 ms/step | loss 10830.339 \n",
      "| iter    136 | 24.01 ms/step | loss 368557.188 \n",
      "| iter    137 | 24.01 ms/step | loss 30792.396 \n",
      "| iter    138 | 26.01 ms/step | loss 31149.361 \n",
      "| iter    139 | 108.02 ms/step | loss 361203.719 \n",
      "| iter    140 | 23.01 ms/step | loss 17593.400 \n",
      "| iter    141 | 23.00 ms/step | loss 114393.023 \n",
      "| iter    142 | 27.01 ms/step | loss 72446.008 \n",
      "| iter    143 | 24.01 ms/step | loss 43782.594 \n",
      "| iter    144 | 23.01 ms/step | loss 17109.146 \n",
      "| iter    145 | 24.01 ms/step | loss 386185.719 \n",
      "| iter    146 | 26.01 ms/step | loss 64924.336 \n",
      "| iter    147 | 24.01 ms/step | loss 5079.842 \n",
      "| iter    148 | 28.01 ms/step | loss 373375.062 \n",
      "| iter    149 | 19.99 ms/step | loss 36394.430 \n",
      "| iter    150 | 105.01 ms/step | loss 157336.516 \n",
      "| iter    151 | 23.01 ms/step | loss 30745.293 \n",
      "| iter    152 | 26.01 ms/step | loss 7028.531 \n",
      "| iter    153 | 22.01 ms/step | loss 379073.469 \n",
      "| iter    154 | 24.00 ms/step | loss 29426.311 \n",
      "| iter    155 | 23.01 ms/step | loss 64622.754 \n",
      "| iter    156 | 25.01 ms/step | loss 341240.594 \n",
      "| iter    157 | 20.99 ms/step | loss 142367.828 \n",
      "| iter    158 | 27.01 ms/step | loss 356593.500 \n",
      "| iter    159 | 26.00 ms/step | loss 19850.691 \n",
      "| iter    160 | 104.01 ms/step | loss 45148.621 \n",
      "| iter    161 | 24.00 ms/step | loss 13705.021 \n",
      "| iter    162 | 22.01 ms/step | loss 17134.602 \n",
      "| iter    163 | 27.99 ms/step | loss 147163.203 \n",
      "| iter    164 | 26.99 ms/step | loss 385772.719 \n",
      "| iter    165 | 25.01 ms/step | loss 340610.094 \n",
      "| iter    166 | 27.01 ms/step | loss 11860.484 \n",
      "| iter    167 | 23.01 ms/step | loss 346466.469 \n",
      "| iter    168 | 106.02 ms/step | loss 11984.218 \n",
      "| iter    169 | 23.01 ms/step | loss 24468.965 \n",
      "| iter    170 | 26.01 ms/step | loss 25907.791 \n",
      "| iter    171 | 25.00 ms/step | loss 309926.219 \n",
      "| iter    172 | 25.01 ms/step | loss 1755942.375 \n",
      "| iter    173 | 23.01 ms/step | loss 58664.426 \n",
      "| iter    174 | 24.01 ms/step | loss 7209.457 \n",
      "| iter    175 | 25.01 ms/step | loss 37904.750 \n",
      "| iter    176 | 23.00 ms/step | loss 286864.844 \n",
      "| iter    177 | 110.03 ms/step | loss 11336.998 \n",
      "| iter    178 | 27.01 ms/step | loss 354307.281 \n",
      "| iter    179 | 26.01 ms/step | loss 25515.857 \n",
      "| iter    180 | 27.06 ms/step | loss 12754.857 \n",
      "| iter    181 | 22.01 ms/step | loss 4836.970 \n",
      "| iter    182 | 25.02 ms/step | loss 7504.790 \n",
      "| iter    183 | 22.01 ms/step | loss 347628.844 \n",
      "| iter    184 | 23.01 ms/step | loss 256726.594 \n",
      "| iter    185 | 24.01 ms/step | loss 9100.307 \n",
      "| iter    186 | 110.01 ms/step | loss 353823.438 \n",
      "| iter    187 | 24.00 ms/step | loss 4077.005 \n",
      "| iter    188 | 28.01 ms/step | loss 28533.262 \n",
      "| iter    189 | 22.01 ms/step | loss 25741926.000 \n",
      "| iter    190 | 26.00 ms/step | loss 366008.031 \n",
      "| iter    191 | 24.01 ms/step | loss 67541.836 \n",
      "| iter    192 | 27.00 ms/step | loss 14936.805 \n",
      "| iter    193 | 23.01 ms/step | loss 36371.777 \n",
      "| iter    194 | 26.01 ms/step | loss 96701.125 \n",
      "| iter    195 | 25.01 ms/step | loss 27947.643 \n",
      "| iter    196 | 104.02 ms/step | loss 16529.004 \n",
      "| iter    197 | 22.00 ms/step | loss 339957.750 \n",
      "| iter    198 | 25.01 ms/step | loss 319188.406 \n",
      "| iter    199 | 24.01 ms/step | loss 25425.621 \n",
      "| iter    200 | 22.01 ms/step | loss 27490.320 \n",
      "| iter    201 | 21.01 ms/step | loss 3497539.250 \n",
      "| iter    202 | 27.01 ms/step | loss 678816.375 \n",
      "| iter    203 | 21.99 ms/step | loss 9996.381 \n",
      "| iter    204 | 28.01 ms/step | loss 25272.434 \n",
      "| iter    205 | 22.00 ms/step | loss 316311.656 \n",
      "| iter    206 | 107.03 ms/step | loss 330773.031 \n",
      "| iter    207 | 25.01 ms/step | loss 7507.007 \n",
      "| iter    208 | 23.01 ms/step | loss 29009.609 \n",
      "| iter    209 | 21.00 ms/step | loss 10267.807 \n",
      "| iter    210 | 26.01 ms/step | loss 15118.701 \n",
      "| iter    211 | 25.00 ms/step | loss 7687.773 \n",
      "| iter    212 | 23.00 ms/step | loss 340200.438 \n",
      "| iter    213 | 22.00 ms/step | loss 12553.368 \n",
      "| iter    214 | 26.01 ms/step | loss 29271.111 \n",
      "| iter    215 | 107.04 ms/step | loss 9211.527 \n",
      "| iter    216 | 26.99 ms/step | loss 293851.125 \n",
      "| iter    217 | 23.00 ms/step | loss 24029.854 \n",
      "| iter    218 | 22.01 ms/step | loss 18800.699 \n",
      "| iter    219 | 24.01 ms/step | loss 302345.438 \n",
      "| iter    220 | 22.01 ms/step | loss 4274.374 \n",
      "| iter    221 | 25.01 ms/step | loss 39699.664 \n",
      "| iter    222 | 24.01 ms/step | loss 17275.242 \n",
      "| iter    223 | 23.00 ms/step | loss 272512.562 \n",
      "| iter    224 | 24.02 ms/step | loss 10682.482 \n",
      "| iter    225 | 20.01 ms/step | loss 10502.888 \n",
      "| iter    226 | 106.02 ms/step | loss 14630.526 \n",
      "| iter    227 | 27.01 ms/step | loss 8179.770 \n",
      "| iter    228 | 26.01 ms/step | loss 28509.414 \n",
      "| iter    229 | 23.01 ms/step | loss 229803.828 \n",
      "| iter    230 | 24.02 ms/step | loss 182811.547 \n",
      "| iter    231 | 24.07 ms/step | loss 150112.031 \n",
      "| iter    232 | 25.01 ms/step | loss 2403832.000 \n",
      "| iter    233 | 21.00 ms/step | loss 22294.555 \n",
      "| iter    234 | 26.01 ms/step | loss 364212.500 \n",
      "| iter    235 | 26.01 ms/step | loss 6293.383 \n",
      "| iter    236 | 104.02 ms/step | loss 8711.047 \n",
      "| iter    237 | 25.02 ms/step | loss 34589.723 \n",
      "| iter    238 | 38.99 ms/step | loss 5507588.500 \n",
      "| iter    239 | 34.01 ms/step | loss 10603.688 \n",
      "| iter    240 | 24.00 ms/step | loss 5818.197 \n",
      "| iter    241 | 25.01 ms/step | loss 24895.008 \n",
      "| iter    242 | 26.03 ms/step | loss 11709.149 \n",
      "| iter    243 | 27.01 ms/step | loss 384616.594 \n",
      "| iter    244 | 109.03 ms/step | loss 13516.808 \n",
      "| iter    245 | 25.01 ms/step | loss 9882.336 \n",
      "| iter    246 | 23.01 ms/step | loss 18643.027 \n",
      "| iter    247 | 25.01 ms/step | loss 6989.127 \n",
      "| iter    248 | 26.01 ms/step | loss 60615.422 \n",
      "| iter    249 | 24.01 ms/step | loss 317055.938 \n",
      "| iter    250 | 28.01 ms/step | loss 21923.801 \n",
      "| iter    251 | 25.02 ms/step | loss 381340.594 \n",
      "| iter    252 | 27.01 ms/step | loss 9770.432 \n",
      "| iter    253 | 106.02 ms/step | loss 11096.575 \n",
      "| iter    254 | 26.01 ms/step | loss 745925.250 \n",
      "| iter    255 | 28.01 ms/step | loss 11433.503 \n",
      "| iter    256 | 27.01 ms/step | loss 64573.859 \n",
      "| iter    257 | 23.01 ms/step | loss 29906.904 \n",
      "| iter    258 | 26.01 ms/step | loss 13487.597 \n",
      "| iter    259 | 26.01 ms/step | loss 467271.156 \n",
      "| iter    260 | 25.01 ms/step | loss 23173.082 \n",
      "| iter    261 | 20.01 ms/step | loss 46605.738 \n",
      "| iter    262 | 25.01 ms/step | loss 304720.500 \n",
      "| iter    263 | 106.02 ms/step | loss 7649.764 \n",
      "| iter    264 | 27.01 ms/step | loss 26582.291 \n",
      "| iter    265 | 23.01 ms/step | loss 5877.822 \n",
      "| iter    266 | 27.01 ms/step | loss 6902.276 \n",
      "| iter    267 | 24.01 ms/step | loss 330198.250 \n",
      "| iter    268 | 26.01 ms/step | loss 18994.748 \n",
      "| iter    269 | 23.01 ms/step | loss 6504.887 \n",
      "| iter    270 | 27.01 ms/step | loss 6676.644 \n",
      "| iter    271 | 108.02 ms/step | loss 10455.336 \n",
      "| iter    272 | 24.01 ms/step | loss 76277.648 \n",
      "| iter    273 | 21.96 ms/step | loss 364046.781 \n",
      "| iter    274 | 27.02 ms/step | loss 28260.020 \n",
      "| iter    275 | 25.01 ms/step | loss 321699.469 \n",
      "| iter    276 | 23.01 ms/step | loss 6997.390 \n",
      "| iter    277 | 23.01 ms/step | loss 34878.762 \n",
      "| iter    278 | 25.01 ms/step | loss 14420.500 \n",
      "| iter    279 | 23.02 ms/step | loss 328042.688 \n",
      "| iter    280 | 111.04 ms/step | loss 45680.055 \n",
      "| iter    281 | 23.01 ms/step | loss 27527.773 \n",
      "| iter    282 | 27.01 ms/step | loss 13810.392 \n",
      "| iter    283 | 22.00 ms/step | loss 307714.906 \n",
      "| iter    284 | 24.01 ms/step | loss 7718.420 \n",
      "| iter    285 | 25.01 ms/step | loss 25836.193 \n",
      "| iter    286 | 27.01 ms/step | loss 272835.531 \n",
      "| iter    287 | 25.00 ms/step | loss 12953.478 \n",
      "| iter    288 | 25.02 ms/step | loss 55225.875 \n",
      "| iter    289 | 25.08 ms/step | loss 30622.133 \n",
      "| iter    290 | 139.01 ms/step | loss 9711.322 \n",
      "| iter    291 | 27.01 ms/step | loss 265637.031 \n",
      "| iter    292 | 25.01 ms/step | loss 303214.781 \n",
      "| iter    293 | 20.01 ms/step | loss 6392.041 \n",
      "| iter    294 | 25.01 ms/step | loss 23566.182 \n",
      "| iter    295 | 23.01 ms/step | loss 14459.841 \n",
      "| iter    296 | 27.03 ms/step | loss 626262.750 \n",
      "| iter    297 | 24.01 ms/step | loss 355216.000 \n",
      "| iter    298 | 22.01 ms/step | loss 34378.617 \n",
      "| iter    299 | 108.02 ms/step | loss 296818.562 \n",
      "| iter    300 | 25.01 ms/step | loss 86083.234 \n",
      "| iter    301 | 23.02 ms/step | loss 14335.524 \n",
      "| iter    302 | 24.01 ms/step | loss 10876.205 \n",
      "| iter    303 | 23.01 ms/step | loss 32248.729 \n",
      "| iter    304 | 27.01 ms/step | loss 7613.021 \n",
      "| iter    305 | 23.01 ms/step | loss 228213.016 \n",
      "| iter    306 | 22.01 ms/step | loss 5181.587 \n",
      "| iter    307 | 26.01 ms/step | loss 213158.938 \n",
      "| iter    308 | 23.00 ms/step | loss 44194.355 \n",
      "| iter    309 | 24.01 ms/step | loss 21122.963 \n",
      "| iter    310 | 110.03 ms/step | loss 11933.292 \n",
      "| iter    311 | 22.00 ms/step | loss 179950.438 \n",
      "| iter    312 | 26.00 ms/step | loss 16602.342 \n",
      "| iter    313 | 23.01 ms/step | loss 286571.781 \n",
      "| iter    314 | 26.01 ms/step | loss 8949.376 \n",
      "| iter    315 | 25.01 ms/step | loss 299754.156 \n",
      "| iter    316 | 24.01 ms/step | loss 465362.844 \n",
      "| iter    317 | 23.01 ms/step | loss 23671.086 \n",
      "| iter    318 | 28.01 ms/step | loss 43664.680 \n",
      "| iter    319 | 27.01 ms/step | loss 13256.164 \n",
      "| iter    320 | 101.02 ms/step | loss 192350.594 \n",
      "| iter    321 | 24.01 ms/step | loss 187255.312 \n",
      "| iter    322 | 27.01 ms/step | loss 51944.945 \n",
      "| iter    323 | 27.01 ms/step | loss 11372.037 \n",
      "| iter    324 | 26.01 ms/step | loss 11906.459 \n",
      "| iter    325 | 24.02 ms/step | loss 431964.562 \n",
      "| iter    326 | 25.01 ms/step | loss 18161.287 \n",
      "| iter    327 | 21.01 ms/step | loss 13197.171 \n",
      "| iter    328 | 107.03 ms/step | loss 207947.984 \n",
      "| iter    329 | 25.01 ms/step | loss 5942.838 \n",
      "| iter    330 | 28.03 ms/step | loss 61698.594 \n",
      "| iter    331 | 23.00 ms/step | loss 6538.746 \n",
      "| iter    332 | 23.01 ms/step | loss 14525.254 \n",
      "| iter    333 | 20.00 ms/step | loss 333921.406 \n",
      "| iter    334 | 25.01 ms/step | loss 9343.965 \n",
      "| iter    335 | 25.01 ms/step | loss 13378.777 \n",
      "| iter    336 | 24.02 ms/step | loss 32422.156 \n",
      "| iter    337 | 106.03 ms/step | loss 209044.203 \n",
      "| iter    338 | 25.01 ms/step | loss 33638.906 \n",
      "| iter    339 | 25.01 ms/step | loss 27425.889 \n",
      "| iter    340 | 25.01 ms/step | loss 164884.562 \n",
      "| iter    341 | 24.01 ms/step | loss 271461.250 \n",
      "| iter    342 | 27.01 ms/step | loss 27589.029 \n",
      "| iter    343 | 23.01 ms/step | loss 5386.293 \n",
      "| iter    344 | 25.00 ms/step | loss 44144.508 \n",
      "| iter    345 | 25.01 ms/step | loss 206664.344 \n",
      "| iter    346 | 108.02 ms/step | loss 16406.205 \n",
      "| iter    347 | 26.01 ms/step | loss 452810.781 \n",
      "| iter    348 | 23.01 ms/step | loss 298706.656 \n",
      "| iter    349 | 25.01 ms/step | loss 4860.418 \n",
      "| iter    350 | 26.01 ms/step | loss 5608.591 \n",
      "| iter    351 | 25.01 ms/step | loss 2318713.500 \n",
      "| iter    352 | 25.01 ms/step | loss 9692.240 \n",
      "| iter    353 | 22.01 ms/step | loss 157420.906 \n",
      "| iter    354 | 28.01 ms/step | loss 302901.125 \n",
      "| iter    355 | 26.02 ms/step | loss 47591.598 \n",
      "| iter    356 | 108.03 ms/step | loss 6757.688 \n",
      "| iter    357 | 26.01 ms/step | loss 35793.629 \n",
      "| iter    358 | 25.00 ms/step | loss 5452672.000 \n",
      "| iter    359 | 28.01 ms/step | loss 270756.531 \n",
      "| iter    360 | 25.01 ms/step | loss 10306.433 \n",
      "| iter    361 | 25.01 ms/step | loss 7455.559 \n",
      "| iter    362 | 30.99 ms/step | loss 38717.562 \n",
      "| iter    363 | 28.16 ms/step | loss 6155.448 \n",
      "| iter    364 | 30.01 ms/step | loss 316277.438 \n",
      "| iter    365 | 28.01 ms/step | loss 26826922.000 \n",
      "| iter    366 | 115.03 ms/step | loss 49611.414 \n",
      "| iter    367 | 26.01 ms/step | loss 6743279.000 \n",
      "| iter    368 | 26.01 ms/step | loss 7779.985 \n",
      "| iter    369 | 31.01 ms/step | loss 308483.406 \n",
      "| iter    370 | 24.01 ms/step | loss 28187288.000 \n",
      "| iter    371 | 22.00 ms/step | loss 37183.520 \n",
      "| iter    372 | 25.01 ms/step | loss 27069.762 \n",
      "| iter    373 | 22.01 ms/step | loss 632104.750 \n",
      "| iter    374 | 24.01 ms/step | loss 36182.727 \n",
      "| iter    375 | 101.03 ms/step | loss 11040.778 \n",
      "| iter    376 | 25.01 ms/step | loss 30997.207 \n",
      "| iter    377 | 22.01 ms/step | loss 446287.125 \n",
      "| iter    378 | 27.00 ms/step | loss 29459.934 \n",
      "| iter    379 | 25.01 ms/step | loss 547842.000 \n",
      "| iter    380 | 27.02 ms/step | loss 24022.504 \n",
      "| iter    381 | 24.02 ms/step | loss 482776.906 \n",
      "| iter    382 | 26.01 ms/step | loss 10667.048 \n",
      "| iter    383 | 26.00 ms/step | loss 46709.016 \n",
      "| iter    384 | 25.01 ms/step | loss 19778.457 \n",
      "| iter    385 | 23.00 ms/step | loss 494690.469 \n",
      "| iter    386 | 113.03 ms/step | loss 525732.938 \n",
      "| iter    387 | 26.01 ms/step | loss 21178.902 \n",
      "| iter    388 | 27.99 ms/step | loss 82673.336 \n",
      "| iter    389 | 26.99 ms/step | loss 34324.211 \n",
      "| iter    390 | 29.00 ms/step | loss 19354.855 \n",
      "| iter    391 | 29.01 ms/step | loss 351402.375 \n",
      "| iter    392 | 31.02 ms/step | loss 90465.656 \n",
      "| iter    393 | 24.00 ms/step | loss 683982.750 \n",
      "| iter    394 | 29.00 ms/step | loss 108513.539 \n",
      "| iter    395 | 24.01 ms/step | loss 17655.930 \n",
      "| iter    396 | 103.04 ms/step | loss 451542.312 \n",
      "| iter    397 | 23.19 ms/step | loss 46685.723 \n",
      "| iter    398 | 26.01 ms/step | loss 23990.729 \n",
      "| iter    399 | 25.01 ms/step | loss 162999.281 \n",
      "| iter    400 | 25.01 ms/step | loss 24736.797 \n",
      "| iter    401 | 25.01 ms/step | loss 590710.688 \n",
      "| iter    402 | 25.01 ms/step | loss 50038.859 \n",
      "| iter    403 | 26.00 ms/step | loss 9880.403 \n",
      "| iter    404 | 28.01 ms/step | loss 41976.855 \n",
      "| iter    405 | 105.03 ms/step | loss 521678.750 \n",
      "| iter    406 | 24.02 ms/step | loss 31969.354 \n",
      "| iter    407 | 26.01 ms/step | loss 11908.308 \n",
      "| iter    408 | 25.01 ms/step | loss 471674.500 \n",
      "| iter    409 | 26.01 ms/step | loss 8791.207 \n",
      "| iter    410 | 27.01 ms/step | loss 19708.316 \n",
      "| iter    411 | 27.01 ms/step | loss 24263.748 \n",
      "| iter    412 | 24.01 ms/step | loss 374703.562 \n",
      "| iter    413 | 110.03 ms/step | loss 10420.770 \n",
      "| iter    414 | 24.00 ms/step | loss 350023.031 \n",
      "| iter    415 | 26.01 ms/step | loss 27052.854 \n",
      "| iter    416 | 24.01 ms/step | loss 31712.445 \n",
      "| iter    417 | 24.01 ms/step | loss 22004.676 \n",
      "| iter    418 | 25.02 ms/step | loss 52460.906 \n",
      "| iter    419 | 27.01 ms/step | loss 354351.750 \n",
      "| iter    420 | 23.01 ms/step | loss 8907.430 \n",
      "| iter    421 | 22.01 ms/step | loss 24171.648 \n",
      "| iter    422 | 105.03 ms/step | loss 382026.219 \n",
      "| iter    423 | 26.01 ms/step | loss 40648.047 \n",
      "| iter    424 | 24.01 ms/step | loss 9888.717 \n",
      "| iter    425 | 26.01 ms/step | loss 24194.168 \n",
      "| iter    426 | 28.00 ms/step | loss 257294.406 \n",
      "| iter    427 | 24.01 ms/step | loss 25432.949 \n",
      "| iter    428 | 26.01 ms/step | loss 130354.594 \n",
      "| iter    429 | 27.01 ms/step | loss 103635.008 \n",
      "| iter    430 | 31.01 ms/step | loss 11561.087 \n",
      "| iter    431 | 25.99 ms/step | loss 11208.350 \n",
      "| iter    432 | 122.02 ms/step | loss 247778.078 \n",
      "| iter    433 | 26.01 ms/step | loss 31794.211 \n",
      "| iter    434 | 25.01 ms/step | loss 572090.562 \n",
      "| iter    435 | 27.00 ms/step | loss 65310.227 \n",
      "| iter    436 | 27.01 ms/step | loss 10933.857 \n",
      "| iter    437 | 26.01 ms/step | loss 40089.145 \n",
      "| iter    438 | 29.01 ms/step | loss 238521.906 \n",
      "| iter    439 | 29.01 ms/step | loss 257014.703 \n",
      "| iter    440 | 27.99 ms/step | loss 143873.219 \n",
      "| iter    441 | 27.01 ms/step | loss 13390.858 \n",
      "| iter    442 | 116.04 ms/step | loss 34567.348 \n",
      "| iter    443 | 28.01 ms/step | loss 681125.625 \n",
      "| iter    444 | 30.01 ms/step | loss 214968.109 \n",
      "| iter    445 | 29.01 ms/step | loss 7975.136 \n",
      "| iter    446 | 30.01 ms/step | loss 37994.977 \n",
      "| iter    447 | 26.01 ms/step | loss 33873.398 \n",
      "| iter    448 | 30.01 ms/step | loss 226642.344 \n",
      "| iter    449 | 28.07 ms/step | loss 28281.695 \n",
      "| iter    450 | 56.01 ms/step | loss 93859.656 \n",
      "| iter    451 | 139.03 ms/step | loss 209062.094 \n",
      "| iter    452 | 30.01 ms/step | loss 1613722.875 \n",
      "| iter    453 | 24.00 ms/step | loss 13342.054 \n",
      "| iter    454 | 29.01 ms/step | loss 9431.383 \n",
      "| iter    455 | 29.01 ms/step | loss 68528.703 \n",
      "| iter    456 | 26.01 ms/step | loss 15698.043 \n",
      "| iter    457 | 30.01 ms/step | loss 383425.125 \n",
      "| iter    458 | 31.01 ms/step | loss 382837.969 \n",
      "| iter    459 | 28.01 ms/step | loss 25985.703 \n",
      "| iter    460 | 27.01 ms/step | loss 12248.504 \n",
      "| iter    461 | 26.01 ms/step | loss 35011.742 \n",
      "| iter    462 | 127.03 ms/step | loss 8877.148 \n",
      "| iter    463 | 26.90 ms/step | loss 21673.600 \n",
      "| iter    464 | 27.01 ms/step | loss 275835.281 \n",
      "| iter    465 | 28.00 ms/step | loss 535157.625 \n",
      "| iter    466 | 31.01 ms/step | loss 20096.445 \n",
      "| iter    467 | 27.01 ms/step | loss 17867.545 \n",
      "| iter    468 | 27.01 ms/step | loss 31034.469 \n",
      "| iter    469 | 24.00 ms/step | loss 262658.906 \n",
      "| iter    470 | 29.01 ms/step | loss 344641.562 \n",
      "| iter    471 | 28.01 ms/step | loss 19017.293 \n",
      "| iter    472 | 129.01 ms/step | loss 12931.224 \n",
      "| iter    473 | 28.01 ms/step | loss 6882.163 \n",
      "| iter    474 | 27.01 ms/step | loss 735384.750 \n",
      "| iter    475 | 24.00 ms/step | loss 12167.275 \n",
      "| iter    476 | 25.00 ms/step | loss 7661329.000 \n",
      "| iter    477 | 24.01 ms/step | loss 14048.812 \n",
      "| iter    478 | 27.01 ms/step | loss 95226.016 \n",
      "| iter    479 | 30.01 ms/step | loss 12198.481 \n",
      "| iter    480 | 26.99 ms/step | loss 47084.711 \n",
      "| iter    481 | 124.03 ms/step | loss 375452.094 \n",
      "| iter    482 | 29.01 ms/step | loss 37709.457 \n",
      "| iter    483 | 30.00 ms/step | loss 265781.969 \n",
      "| iter    484 | 30.01 ms/step | loss 26760.975 \n",
      "| iter    485 | 24.01 ms/step | loss 7712.481 \n",
      "| iter    486 | 27.01 ms/step | loss 201049.812 \n",
      "| iter    487 | 25.01 ms/step | loss 255843.703 \n",
      "| iter    488 | 26.01 ms/step | loss 34942.938 \n",
      "| iter    489 | 110.04 ms/step | loss 10298.775 \n",
      "| iter    490 | 26.01 ms/step | loss 41247.422 \n",
      "| iter    491 | 27.01 ms/step | loss 17941.176 \n",
      "| iter    492 | 27.01 ms/step | loss 335826.906 \n",
      "| iter    493 | 23.01 ms/step | loss 4071.943 \n",
      "| iter    494 | 29.01 ms/step | loss 7476.445 \n",
      "| iter    495 | 27.01 ms/step | loss 313006.500 \n",
      "| iter    496 | 24.01 ms/step | loss 35928.254 \n",
      "| iter    497 | 108.03 ms/step | loss 13547.267 \n",
      "| iter    498 | 25.01 ms/step | loss 31678.699 \n",
      "| iter    499 | 26.06 ms/step | loss 247256.812 \n",
      "| iter    500 | 26.00 ms/step | loss 13415.748 \n",
      "| iter    501 | 23.01 ms/step | loss 14106.425 \n",
      "| iter    502 | 27.01 ms/step | loss 264780.875 \n",
      "| iter    503 | 25.02 ms/step | loss 36031.621 \n",
      "| iter    504 | 24.01 ms/step | loss 7811.354 \n",
      "| iter    505 | 22.00 ms/step | loss 8916.152 \n",
      "| iter    506 | 109.03 ms/step | loss 282577184.000 \n",
      "| iter    507 | 24.01 ms/step | loss 76835.281 \n",
      "| iter    508 | 24.99 ms/step | loss 262973.156 \n",
      "| iter    509 | 22.01 ms/step | loss 313629.719 \n",
      "| iter    510 | 27.01 ms/step | loss 305385.812 \n",
      "| iter    511 | 25.01 ms/step | loss 26290.285 \n",
      "| iter    512 | 23.01 ms/step | loss 15986.485 \n",
      "| iter    513 | 21.99 ms/step | loss 39839.352 \n",
      "| iter    514 | 28.01 ms/step | loss 238972.438 \n",
      "| iter    515 | 25.01 ms/step | loss 41678.617 \n",
      "| iter    516 | 105.03 ms/step | loss 39987.660 \n",
      "| iter    517 | 22.01 ms/step | loss 293664.781 \n",
      "| iter    518 | 26.01 ms/step | loss 154635.219 \n",
      "| iter    519 | 25.01 ms/step | loss 11348.166 \n",
      "| iter    520 | 24.00 ms/step | loss 170070.328 \n",
      "| iter    521 | 22.01 ms/step | loss 28429.633 \n",
      "| iter    522 | 28.01 ms/step | loss 12143.049 \n",
      "| iter    523 | 24.01 ms/step | loss 15839.613 \n",
      "| iter    524 | 23.01 ms/step | loss 178749.672 \n",
      "| iter    525 | 26.01 ms/step | loss 29314.943 \n",
      "| iter    526 | 110.02 ms/step | loss 161469.453 \n",
      "| iter    527 | 26.00 ms/step | loss 18435.355 \n",
      "| iter    528 | 25.00 ms/step | loss 351022.062 \n",
      "| iter    529 | 25.01 ms/step | loss 33032.023 \n",
      "| iter    530 | 27.01 ms/step | loss 25632.406 \n",
      "| iter    531 | 24.01 ms/step | loss 204040.141 \n",
      "| iter    532 | 28.00 ms/step | loss 46488.086 \n",
      "| iter    533 | 22.01 ms/step | loss 1414600832.000 \n",
      "| iter    534 | 26.02 ms/step | loss 51475.473 \n",
      "| iter    535 | 109.02 ms/step | loss 21964.699 \n",
      "| iter    536 | 25.99 ms/step | loss 61355.602 \n",
      "| iter    537 | 26.01 ms/step | loss 422247.812 \n",
      "| iter    538 | 27.00 ms/step | loss 63321.934 \n",
      "| iter    539 | 27.00 ms/step | loss 955492288.000 \n",
      "| iter    540 | 23.99 ms/step | loss 54775.203 \n",
      "| iter    541 | 24.01 ms/step | loss 211534.844 \n",
      "| iter    542 | 25.02 ms/step | loss 177568.312 \n",
      "| iter    543 | 24.01 ms/step | loss 108268.734 \n",
      "| iter    544 | 23.01 ms/step | loss 491696.125 \n",
      "| iter    545 | 24.01 ms/step | loss 580308.750 \n",
      "| iter    546 | 115.03 ms/step | loss 650933.312 \n",
      "| iter    547 | 24.00 ms/step | loss 13563.033 \n",
      "| iter    548 | 23.01 ms/step | loss 25420.395 \n",
      "| iter    549 | 22.00 ms/step | loss 72812.266 \n",
      "| iter    550 | 28.01 ms/step | loss 34040.773 \n",
      "| iter    551 | 23.01 ms/step | loss 203086.188 \n",
      "| iter    552 | 27.00 ms/step | loss 532498.562 \n",
      "| iter    553 | 23.01 ms/step | loss 10520.045 \n",
      "| iter    554 | 27.99 ms/step | loss 12971.746 \n",
      "| iter    555 | 24.01 ms/step | loss 545916.125 \n",
      "| iter    556 | 105.01 ms/step | loss 15351.119 \n",
      "| iter    557 | 26.00 ms/step | loss 14633.153 \n",
      "| iter    558 | 25.02 ms/step | loss 9226.033 \n",
      "| iter    559 | 25.01 ms/step | loss 13643.220 \n",
      "| iter    560 | 24.01 ms/step | loss 56893.254 \n",
      "| iter    561 | 21.00 ms/step | loss 484786.719 \n",
      "| iter    562 | 23.01 ms/step | loss 695263.062 \n",
      "| iter    563 | 26.01 ms/step | loss 22747.938 \n",
      "| iter    564 | 103.03 ms/step | loss 16935.002 \n",
      "| iter    565 | 23.00 ms/step | loss 73499.953 \n",
      "| iter    566 | 24.01 ms/step | loss 14491.369 \n",
      "| iter    567 | 27.01 ms/step | loss 12965.683 \n",
      "| iter    568 | 23.00 ms/step | loss 538329.812 \n",
      "| iter    569 | 22.00 ms/step | loss 62568.629 \n",
      "| iter    570 | 23.01 ms/step | loss 7052.542 \n",
      "| iter    571 | 24.01 ms/step | loss 43668.996 \n",
      "| iter    572 | 25.00 ms/step | loss 408563.938 \n",
      "| iter    573 | 111.02 ms/step | loss 38368.410 \n",
      "| iter    574 | 25.01 ms/step | loss 8061.595 \n",
      "| iter    575 | 24.00 ms/step | loss 7797.754 \n",
      "| iter    576 | 28.01 ms/step | loss 159387.859 \n",
      "| iter    577 | 24.01 ms/step | loss 672006.062 \n",
      "| iter    578 | 26.01 ms/step | loss 462905.906 \n",
      "| iter    579 | 26.01 ms/step | loss 14433.275 \n",
      "| iter    580 | 25.01 ms/step | loss 16543.930 \n",
      "| iter    581 | 23.01 ms/step | loss 40783.602 \n",
      "| iter    582 | 113.03 ms/step | loss 545252.688 \n",
      "| iter    583 | 26.01 ms/step | loss 10291.502 \n",
      "| iter    584 | 25.01 ms/step | loss 58291.680 \n",
      "| iter    585 | 22.03 ms/step | loss 6207.821 \n",
      "| iter    586 | 28.01 ms/step | loss 14142.296 \n",
      "| iter    587 | 24.01 ms/step | loss 42210.797 \n",
      "| iter    588 | 26.00 ms/step | loss 750058.375 \n",
      "| iter    589 | 25.00 ms/step | loss 12292.286 \n",
      "| iter    590 | 27.01 ms/step | loss 18889.162 \n",
      "| iter    591 | 25.01 ms/step | loss 51555.891 \n",
      "| iter    592 | 107.01 ms/step | loss 729897.312 \n",
      "| iter    593 | 25.01 ms/step | loss 20091.975 \n",
      "| iter    594 | 23.00 ms/step | loss 766607.312 \n",
      "| iter    595 | 24.01 ms/step | loss 18623.330 \n",
      "| iter    596 | 23.01 ms/step | loss 23373.910 \n",
      "| iter    597 | 25.01 ms/step | loss 54441.793 \n",
      "| iter    598 | 25.00 ms/step | loss 34979.336 \n",
      "| iter    599 | 25.01 ms/step | loss 22340.678 \n",
      "| iter    600 | 23.01 ms/step | loss 9543.736 \n",
      "| iter    601 | 23.01 ms/step | loss 882463.375 \n",
      "| iter    602 | 104.04 ms/step | loss 592252.500 \n",
      "| iter    603 | 27.01 ms/step | loss 1139176.375 \n",
      "| iter    604 | 24.01 ms/step | loss 9649.211 \n",
      "| iter    605 | 25.02 ms/step | loss 5582.927 \n",
      "| iter    606 | 25.01 ms/step | loss 473654.438 \n",
      "| iter    607 | 26.00 ms/step | loss 278493.062 \n",
      "| iter    608 | 27.01 ms/step | loss 197719.062 \n",
      "| iter    609 | 22.01 ms/step | loss 45081.180 \n",
      "| iter    610 | 24.01 ms/step | loss 59923.508 \n",
      "| iter    611 | 23.01 ms/step | loss 2012904.625 \n",
      "| iter    612 | 112.01 ms/step | loss 10339.332 \n",
      "| iter    613 | 22.00 ms/step | loss 391713.156 \n",
      "| iter    614 | 25.01 ms/step | loss 18986.660 \n",
      "| iter    615 | 25.01 ms/step | loss 133176.359 \n",
      "| iter    616 | 30.01 ms/step | loss 79360.734 \n",
      "| iter    617 | 25.01 ms/step | loss 618454.750 \n",
      "| iter    618 | 27.01 ms/step | loss 58817.676 \n",
      "| iter    619 | 24.01 ms/step | loss 470850.469 \n",
      "| iter    620 | 28.01 ms/step | loss 2039075840.000 \n",
      "| iter    621 | 21.99 ms/step | loss 82029.914 \n",
      "| iter    622 | 103.01 ms/step | loss 787439.750 \n",
      "| iter    623 | 25.01 ms/step | loss 884181.188 \n",
      "| iter    624 | 23.01 ms/step | loss 26154.506 \n",
      "| iter    625 | 25.01 ms/step | loss 90688.328 \n",
      "| iter    626 | 30.01 ms/step | loss 25686.719 \n",
      "| iter    627 | 23.00 ms/step | loss 62413.520 \n",
      "| iter    628 | 27.01 ms/step | loss 37252.730 \n",
      "| iter    629 | 21.01 ms/step | loss 1239765.625 \n",
      "| iter    630 | 24.01 ms/step | loss 9301.469 \n",
      "| iter    631 | 22.99 ms/step | loss 11532.480 \n",
      "| iter    632 | 100.02 ms/step | loss 28200.629 \n",
      "| iter    633 | 26.01 ms/step | loss 1608907.500 \n",
      "| iter    634 | 24.99 ms/step | loss 798304.000 \n",
      "| iter    635 | 23.01 ms/step | loss 11990.222 \n",
      "| iter    636 | 24.01 ms/step | loss 59744.320 \n",
      "| iter    637 | 25.01 ms/step | loss 10221.278 \n",
      "| iter    638 | 29.01 ms/step | loss 1514837.125 \n",
      "| iter    639 | 24.01 ms/step | loss 162886.031 \n",
      "| iter    640 | 22.00 ms/step | loss 15684.566 \n",
      "| iter    641 | 100.02 ms/step | loss 26068.047 \n",
      "| iter    642 | 26.01 ms/step | loss 73641.328 \n",
      "| iter    643 | 25.01 ms/step | loss 9754.508 \n",
      "| iter    644 | 28.01 ms/step | loss 456171.250 \n",
      "| iter    645 | 26.01 ms/step | loss 30098.443 \n",
      "| iter    646 | 23.00 ms/step | loss 1539397.375 \n",
      "| iter    647 | 25.01 ms/step | loss 95445.078 \n",
      "| iter    648 | 24.01 ms/step | loss 11050.984 \n",
      "| iter    649 | 109.03 ms/step | loss 10404.237 \n",
      "| iter    650 | 23.01 ms/step | loss 62776.211 \n",
      "| iter    651 | 25.01 ms/step | loss 12638.830 \n",
      "| iter    652 | 23.01 ms/step | loss 41524.625 \n",
      "| iter    653 | 22.01 ms/step | loss 1129003.250 \n",
      "| iter    654 | 24.01 ms/step | loss 343395.219 \n",
      "| iter    655 | 23.01 ms/step | loss 1525621.750 \n",
      "| iter    656 | 26.01 ms/step | loss 632285.125 \n",
      "| iter    657 | 24.12 ms/step | loss 33178.414 \n",
      "| iter    658 | 105.02 ms/step | loss 1842628.750 \n",
      "| iter    659 | 26.01 ms/step | loss 14344.436 \n",
      "| iter    660 | 22.01 ms/step | loss 41447.484 \n",
      "| iter    661 | 20.00 ms/step | loss 15177.655 \n",
      "| iter    662 | 22.01 ms/step | loss 2541369.750 \n",
      "| iter    663 | 25.01 ms/step | loss 119981.391 \n",
      "| iter    664 | 23.01 ms/step | loss 48648.156 \n",
      "| iter    665 | 24.01 ms/step | loss 12355.202 \n",
      "| iter    666 | 26.01 ms/step | loss 1923982.625 \n",
      "| iter    667 | 22.01 ms/step | loss 28879.895 \n",
      "| iter    668 | 106.01 ms/step | loss 49465.227 \n",
      "| iter    669 | 27.01 ms/step | loss 736784.562 \n",
      "| iter    670 | 28.02 ms/step | loss 88203.023 \n",
      "| iter    671 | 23.01 ms/step | loss 22389.256 \n",
      "| iter    672 | 23.01 ms/step | loss 6610.226 \n",
      "| iter    673 | 25.01 ms/step | loss 876541.625 \n",
      "| iter    674 | 25.00 ms/step | loss 56001.281 \n",
      "| iter    675 | 24.99 ms/step | loss 142719.516 \n",
      "| iter    676 | 27.01 ms/step | loss 34672.758 \n",
      "| iter    677 | 23.01 ms/step | loss 2512795.750 \n",
      "| iter    678 | 101.02 ms/step | loss 1909492.875 \n",
      "| iter    679 | 22.01 ms/step | loss 46187.504 \n",
      "| iter    680 | 28.01 ms/step | loss 65169.586 \n",
      "| iter    681 | 24.01 ms/step | loss 22965.248 \n",
      "| iter    682 | 27.01 ms/step | loss 2900989.000 \n",
      "| iter    683 | 26.01 ms/step | loss 101401.562 \n",
      "| iter    684 | 23.01 ms/step | loss 250781.625 \n",
      "| iter    685 | 24.01 ms/step | loss 20294.393 \n",
      "| iter    686 | 27.01 ms/step | loss 9050.422 \n",
      "| iter    687 | 25.01 ms/step | loss 16643.082 \n",
      "| iter    688 | 102.04 ms/step | loss 679339.875 \n",
      "| iter    689 | 25.00 ms/step | loss 80571.672 \n",
      "| iter    690 | 27.01 ms/step | loss 21333.609 \n",
      "| iter    691 | 26.01 ms/step | loss 26759.293 \n",
      "| iter    692 | 28.00 ms/step | loss 9976.150 \n",
      "| iter    693 | 25.02 ms/step | loss 1579545.000 \n",
      "| iter    694 | 27.01 ms/step | loss 50686.895 \n",
      "| iter    695 | 24.01 ms/step | loss 48271.203 \n",
      "| iter    696 | 24.00 ms/step | loss 25315.219 \n",
      "| iter    697 | 25.01 ms/step | loss 2474051.750 \n",
      "| iter    698 | 117.00 ms/step | loss 12805.255 \n",
      "| iter    699 | 25.01 ms/step | loss 1391075.625 \n",
      "| iter    700 | 23.01 ms/step | loss 61181.648 \n",
      "| iter    701 | 23.00 ms/step | loss 10600.306 \n",
      "| iter    702 | 26.99 ms/step | loss 762060.438 \n",
      "| iter    703 | 22.00 ms/step | loss 7415.040 \n",
      "| iter    704 | 22.99 ms/step | loss 2022725.375 \n",
      "| iter    705 | 24.01 ms/step | loss 68384.812 \n",
      "| iter    706 | 23.01 ms/step | loss 25386.008 \n",
      "| iter    707 | 27.01 ms/step | loss 64521.586 \n",
      "| iter    708 | 105.02 ms/step | loss 1872171.375 \n",
      "| iter    709 | 24.01 ms/step | loss 19185.877 \n",
      "| iter    710 | 24.05 ms/step | loss 25467.721 \n",
      "| iter    711 | 23.01 ms/step | loss 7336.783 \n",
      "| iter    712 | 26.00 ms/step | loss 51675.320 \n",
      "| iter    713 | 21.00 ms/step | loss 660549.062 \n",
      "| iter    714 | 23.00 ms/step | loss 18946.914 \n",
      "| iter    715 | 26.00 ms/step | loss 16390.406 \n",
      "| iter    716 | 26.01 ms/step | loss 1113347.750 \n",
      "| iter    717 | 103.02 ms/step | loss 14225.768 \n",
      "| iter    718 | 24.01 ms/step | loss 63588.039 \n",
      "| iter    719 | 24.01 ms/step | loss 478911.531 \n",
      "| iter    720 | 27.01 ms/step | loss 21439.590 \n",
      "| iter    721 | 25.03 ms/step | loss 7252.917 \n",
      "| iter    722 | 26.01 ms/step | loss 41306.922 \n",
      "| iter    723 | 22.01 ms/step | loss 9389.075 \n",
      "| iter    724 | 25.01 ms/step | loss 12658.084 \n",
      "| iter    725 | 107.04 ms/step | loss 926717.812 \n",
      "| iter    726 | 28.01 ms/step | loss 46729.473 \n",
      "| iter    727 | 26.00 ms/step | loss 988884.000 \n",
      "| iter    728 | 25.01 ms/step | loss 12794.210 \n",
      "| iter    729 | 24.00 ms/step | loss 54896.820 \n",
      "| iter    730 | 24.00 ms/step | loss 69107.109 \n",
      "| iter    731 | 24.01 ms/step | loss 11425.672 \n",
      "| iter    732 | 25.01 ms/step | loss 712720.812 \n",
      "| iter    733 | 21.01 ms/step | loss 51046.078 \n",
      "| iter    734 | 98.02 ms/step | loss 34343.270 \n",
      "| iter    735 | 24.01 ms/step | loss 50638.402 \n",
      "| iter    736 | 27.01 ms/step | loss 124342.812 \n",
      "| iter    737 | 26.01 ms/step | loss 1278935.000 \n",
      "| iter    738 | 26.01 ms/step | loss 471376.938 \n",
      "| iter    739 | 24.01 ms/step | loss 567872.188 \n",
      "| iter    740 | 25.01 ms/step | loss 22284.695 \n",
      "| iter    741 | 22.01 ms/step | loss 53110.074 \n",
      "| iter    742 | 24.01 ms/step | loss 24419.775 \n",
      "| iter    743 | 24.01 ms/step | loss 1634324.500 \n",
      "| iter    744 | 103.02 ms/step | loss 68886.414 \n",
      "| iter    745 | 24.01 ms/step | loss 17212.951 \n",
      "| iter    746 | 25.01 ms/step | loss 79338.922 \n",
      "| iter    747 | 28.00 ms/step | loss 1341459.125 \n",
      "| iter    748 | 25.01 ms/step | loss 14617.323 \n",
      "| iter    749 | 26.01 ms/step | loss 18799.625 \n",
      "| iter    750 | 23.01 ms/step | loss 30346.203 \n",
      "| iter    751 | 24.01 ms/step | loss 22326.686 \n",
      "| iter    752 | 24.00 ms/step | loss 625836.938 \n",
      "| iter    753 | 24.01 ms/step | loss 61035.926 \n",
      "| iter    754 | 108.03 ms/step | loss 12953.918 \n",
      "| iter    755 | 26.01 ms/step | loss 26557.812 \n",
      "| iter    756 | 34.99 ms/step | loss 30512.957 \n",
      "| iter    757 | 30.01 ms/step | loss 1209174.500 \n",
      "| iter    758 | 27.01 ms/step | loss 12665.877 \n",
      "| iter    759 | 26.01 ms/step | loss 467871.875 \n",
      "| iter    760 | 28.01 ms/step | loss 17554018.000 \n",
      "| iter    761 | 23.01 ms/step | loss 20107.504 \n",
      "| iter    762 | 26.01 ms/step | loss 16460.703 \n",
      "| iter    763 | 106.02 ms/step | loss 63787.398 \n",
      "| iter    764 | 25.01 ms/step | loss 794534.375 \n",
      "| iter    765 | 27.00 ms/step | loss 44173.668 \n",
      "| iter    766 | 24.01 ms/step | loss 914382.875 \n",
      "| iter    767 | 23.00 ms/step | loss 41915.352 \n",
      "| iter    768 | 24.01 ms/step | loss 22142.502 \n",
      "| iter    769 | 23.02 ms/step | loss 94171.688 \n",
      "| iter    770 | 23.01 ms/step | loss 50636.773 \n",
      "| iter    771 | 28.02 ms/step | loss 678131.438 \n",
      "| iter    772 | 25.01 ms/step | loss 21293.252 \n",
      "| iter    773 | 24.01 ms/step | loss 35946.246 \n",
      "| iter    774 | 111.03 ms/step | loss 10075.953 \n",
      "| iter    775 | 26.01 ms/step | loss 135145.406 \n",
      "| iter    776 | 24.01 ms/step | loss 865750.688 \n",
      "| iter    777 | 24.01 ms/step | loss 10635.719 \n",
      "| iter    778 | 27.01 ms/step | loss 548391.188 \n",
      "| iter    779 | 22.01 ms/step | loss 72912.000 \n",
      "| iter    780 | 22.01 ms/step | loss 193656.047 \n",
      "| iter    781 | 22.01 ms/step | loss 3842745.750 \n",
      "| iter    782 | 26.01 ms/step | loss 642042.000 \n",
      "| iter    783 | 24.00 ms/step | loss 130633.312 \n",
      "| iter    784 | 106.02 ms/step | loss 11680.467 \n",
      "| iter    785 | 23.01 ms/step | loss 10903.010 \n",
      "| iter    786 | 25.01 ms/step | loss 6171.997 \n",
      "| iter    787 | 24.01 ms/step | loss 8282.676 \n",
      "| iter    788 | 25.01 ms/step | loss 740137.938 \n",
      "| iter    789 | 22.01 ms/step | loss 30614.424 \n",
      "| iter    790 | 25.01 ms/step | loss 477494.188 \n",
      "| iter    791 | 24.01 ms/step | loss 834418.938 \n",
      "| iter    792 | 104.04 ms/step | loss 922909.500 \n",
      "| iter    793 | 24.01 ms/step | loss 59456.457 \n",
      "| iter    794 | 27.01 ms/step | loss 10982.883 \n",
      "| iter    795 | 23.01 ms/step | loss 54674.902 \n",
      "| iter    796 | 26.01 ms/step | loss 8068.445 \n",
      "| iter    797 | 24.02 ms/step | loss 477448.375 \n",
      "| iter    798 | 28.01 ms/step | loss 89230.945 \n",
      "| iter    799 | 26.01 ms/step | loss 826224.312 \n",
      "| iter    800 | 22.01 ms/step | loss 13544.280 \n",
      "| iter    801 | 105.04 ms/step | loss 69678.125 \n",
      "| iter    802 | 30.01 ms/step | loss 7425170.500 \n",
      "| iter    803 | 24.01 ms/step | loss 27045.951 \n",
      "| iter    804 | 25.01 ms/step | loss 67247.078 \n",
      "| iter    805 | 21.00 ms/step | loss 864782.438 \n",
      "| iter    806 | 23.01 ms/step | loss 1028211.062 \n",
      "| iter    807 | 22.02 ms/step | loss 10528.997 \n",
      "| iter    808 | 22.02 ms/step | loss 47579.477 \n",
      "| iter    809 | 22.01 ms/step | loss 39509.777 \n",
      "| iter    810 | 108.02 ms/step | loss 53083.641 \n",
      "| iter    811 | 23.01 ms/step | loss 12287.516 \n",
      "| iter    812 | 24.01 ms/step | loss 15571.346 \n",
      "| iter    813 | 24.01 ms/step | loss 1331515.750 \n",
      "| iter    814 | 26.00 ms/step | loss 113440.297 \n",
      "| iter    815 | 25.01 ms/step | loss 10721.968 \n",
      "| iter    816 | 26.01 ms/step | loss 30319.910 \n",
      "| iter    817 | 26.01 ms/step | loss 481785.375 \n",
      "| iter    818 | 26.06 ms/step | loss 62841.469 \n",
      "| iter    819 | 25.01 ms/step | loss 524583.062 \n",
      "| iter    820 | 109.03 ms/step | loss 55605.062 \n",
      "| iter    821 | 25.00 ms/step | loss 136805.531 \n",
      "| iter    822 | 28.01 ms/step | loss 46347.352 \n",
      "| iter    823 | 27.01 ms/step | loss 402204.906 \n",
      "| iter    824 | 25.01 ms/step | loss 12327.501 \n",
      "| iter    825 | 21.01 ms/step | loss 18980.654 \n",
      "| iter    826 | 26.01 ms/step | loss 28351.064 \n",
      "| iter    827 | 26.01 ms/step | loss 28342.664 \n",
      "| iter    828 | 27.01 ms/step | loss 337256.719 \n",
      "| iter    829 | 25.01 ms/step | loss 1128994.875 \n",
      "| iter    830 | 110.11 ms/step | loss 51162.727 \n",
      "| iter    831 | 24.01 ms/step | loss 767145.750 \n",
      "| iter    832 | 25.01 ms/step | loss 22624.984 \n",
      "| iter    833 | 24.01 ms/step | loss 19235.992 \n",
      "| iter    834 | 23.01 ms/step | loss 195305.078 \n",
      "| iter    835 | 26.01 ms/step | loss 11313.837 \n",
      "| iter    836 | 27.01 ms/step | loss 891989.938 \n",
      "| iter    837 | 24.03 ms/step | loss 45254.223 \n",
      "| iter    838 | 25.01 ms/step | loss 28822.201 \n",
      "| iter    839 | 106.02 ms/step | loss 655089.000 \n",
      "| iter    840 | 26.01 ms/step | loss 44097.363 \n",
      "| iter    841 | 25.01 ms/step | loss 8709.091 \n",
      "| iter    842 | 26.01 ms/step | loss 65898.086 \n",
      "| iter    843 | 24.02 ms/step | loss 549985.875 \n",
      "| iter    844 | 24.01 ms/step | loss 33504.008 \n",
      "| iter    845 | 24.00 ms/step | loss 7974.784 \n",
      "| iter    846 | 23.01 ms/step | loss 17100.943 \n",
      "| iter    847 | 24.01 ms/step | loss 10971.327 \n",
      "| iter    848 | 28.99 ms/step | loss 828625.312 \n",
      "| iter    849 | 21.01 ms/step | loss 26727.955 \n",
      "| iter    850 | 108.02 ms/step | loss 565234.312 \n",
      "| iter    851 | 27.01 ms/step | loss 10148.217 \n",
      "| iter    852 | 24.99 ms/step | loss 14866.662 \n",
      "| iter    853 | 20.01 ms/step | loss 270066.438 \n",
      "| iter    854 | 23.01 ms/step | loss 39000.062 \n",
      "| iter    855 | 23.02 ms/step | loss 28477.689 \n",
      "| iter    856 | 26.01 ms/step | loss 523069.375 \n",
      "| iter    857 | 23.01 ms/step | loss 9363.607 \n",
      "| iter    858 | 24.01 ms/step | loss 424395.375 \n",
      "| iter    859 | 27.01 ms/step | loss 13530.452 \n",
      "| iter    860 | 109.04 ms/step | loss 17420.770 \n",
      "| iter    861 | 25.01 ms/step | loss 1034410.750 \n",
      "| iter    862 | 25.01 ms/step | loss 519616.406 \n",
      "| iter    863 | 28.01 ms/step | loss 2336850432.000 \n",
      "| iter    864 | 25.01 ms/step | loss 66806.719 \n",
      "| iter    865 | 23.01 ms/step | loss 385606.781 \n",
      "| iter    866 | 28.01 ms/step | loss 11532.980 \n",
      "| iter    867 | 22.01 ms/step | loss 96276.125 \n",
      "| iter    868 | 109.02 ms/step | loss 35442.121 \n",
      "| iter    869 | 25.01 ms/step | loss 2038341.250 \n",
      "| iter    870 | 24.01 ms/step | loss 457334.906 \n",
      "| iter    871 | 23.01 ms/step | loss 20483.250 \n",
      "| iter    872 | 27.01 ms/step | loss 48987.219 \n",
      "| iter    873 | 24.01 ms/step | loss 23314.641 \n",
      "| iter    874 | 24.01 ms/step | loss 2006468.500 \n",
      "| iter    875 | 25.01 ms/step | loss 108415.781 \n",
      "| iter    876 | 26.01 ms/step | loss 55337.164 \n",
      "| iter    877 | 105.02 ms/step | loss 12608.384 \n",
      "| iter    878 | 28.01 ms/step | loss 11682.121 \n",
      "| iter    879 | 23.01 ms/step | loss 3124555.500 \n",
      "| iter    880 | 25.01 ms/step | loss 45938.621 \n",
      "| iter    881 | 24.01 ms/step | loss 77310.516 \n",
      "| iter    882 | 27.02 ms/step | loss 34609.641 \n",
      "| iter    883 | 26.01 ms/step | loss 2372003.250 \n",
      "| iter    884 | 24.01 ms/step | loss 12422.380 \n",
      "| iter    885 | 26.03 ms/step | loss 61764.734 \n",
      "| iter    886 | 103.02 ms/step | loss 154122.312 \n",
      "| iter    887 | 26.00 ms/step | loss 73772.180 \n",
      "| iter    888 | 27.07 ms/step | loss 27073.182 \n",
      "| iter    889 | 24.01 ms/step | loss 1283006.375 \n",
      "| iter    890 | 24.01 ms/step | loss 24853.566 \n",
      "| iter    891 | 25.00 ms/step | loss 18273.434 \n",
      "| iter    892 | 26.00 ms/step | loss 3641145.750 \n",
      "| iter    893 | 24.99 ms/step | loss 11685.659 \n",
      "| iter    894 | 25.01 ms/step | loss 10590.986 \n",
      "| iter    895 | 24.01 ms/step | loss 939560.375 \n",
      "| iter    896 | 109.02 ms/step | loss 195091.875 \n",
      "| iter    897 | 23.01 ms/step | loss 21667.910 \n",
      "| iter    898 | 24.01 ms/step | loss 22759.869 \n",
      "| iter    899 | 25.01 ms/step | loss 32045.113 \n",
      "| iter    900 | 26.01 ms/step | loss 3723859.750 \n",
      "| iter    901 | 20.00 ms/step | loss 78164.930 \n",
      "| iter    902 | 25.01 ms/step | loss 14432.698 \n",
      "| iter    903 | 26.01 ms/step | loss 1973661.500 \n",
      "| iter    904 | 27.01 ms/step | loss 249218.328 \n",
      "| iter    905 | 25.00 ms/step | loss 48747.172 \n",
      "| iter    906 | 107.03 ms/step | loss 82413.375 \n",
      "| iter    907 | 25.03 ms/step | loss 18493.168 \n",
      "| iter    908 | 25.01 ms/step | loss 23093860.000 \n",
      "| iter    909 | 26.01 ms/step | loss 32098.008 \n",
      "| iter    910 | 26.01 ms/step | loss 3204824.750 \n",
      "| iter    911 | 26.01 ms/step | loss 231713.328 \n",
      "| iter    912 | 25.01 ms/step | loss 895763.812 \n",
      "| iter    913 | 24.01 ms/step | loss 234367.656 \n",
      "| iter    914 | 25.01 ms/step | loss 28566.158 \n",
      "| iter    915 | 105.03 ms/step | loss 2124281.500 \n",
      "| iter    916 | 26.01 ms/step | loss 626112.562 \n",
      "| iter    917 | 24.01 ms/step | loss 203468.172 \n",
      "| iter    918 | 26.08 ms/step | loss 35294.031 \n",
      "| iter    919 | 24.02 ms/step | loss 913982.625 \n",
      "| iter    920 | 26.01 ms/step | loss 17632.805 \n",
      "| iter    921 | 21.01 ms/step | loss 137683.703 \n",
      "| iter    922 | 25.01 ms/step | loss 83167.250 \n",
      "| iter    923 | 38.99 ms/step | loss 23145.650 \n",
      "| iter    924 | 24.01 ms/step | loss 15320.881 \n",
      "| iter    925 | 24.00 ms/step | loss 3730166.250 \n",
      "| iter    926 | 105.04 ms/step | loss 672398.312 \n",
      "| iter    927 | 25.06 ms/step | loss 71424.781 \n",
      "| iter    928 | 27.01 ms/step | loss 38918.211 \n",
      "| iter    929 | 21.01 ms/step | loss 12274.444 \n",
      "| iter    930 | 26.01 ms/step | loss 243662.562 \n",
      "| iter    931 | 23.00 ms/step | loss 14013.801 \n",
      "| iter    932 | 22.01 ms/step | loss 25793.270 \n",
      "| iter    933 | 26.01 ms/step | loss 3883057.000 \n",
      "| iter    934 | 23.01 ms/step | loss 16692.027 \n",
      "| iter    935 | 23.01 ms/step | loss 55331.859 \n",
      "| iter    936 | 107.02 ms/step | loss 18503.807 \n",
      "| iter    937 | 21.99 ms/step | loss 3659256.500 \n",
      "| iter    938 | 27.00 ms/step | loss 2912821.500 \n",
      "| iter    939 | 25.01 ms/step | loss 15742.078 \n",
      "| iter    940 | 28.01 ms/step | loss 14364.592 \n",
      "| iter    941 | 24.01 ms/step | loss 167865.203 \n",
      "| iter    942 | 24.01 ms/step | loss 1066673.375 \n",
      "| iter    943 | 27.02 ms/step | loss 24937.336 \n",
      "| iter    944 | 107.03 ms/step | loss 116894.273 \n",
      "| iter    945 | 23.01 ms/step | loss 139867.766 \n",
      "| iter    946 | 24.01 ms/step | loss 29791.812 \n",
      "| iter    947 | 28.05 ms/step | loss 138850.688 \n",
      "| iter    948 | 27.01 ms/step | loss 2520208.500 \n",
      "| iter    949 | 25.01 ms/step | loss 889300.250 \n",
      "| iter    950 | 26.01 ms/step | loss 110974.664 \n",
      "| iter    951 | 24.01 ms/step | loss 5122186.500 \n",
      "| iter    952 | 25.01 ms/step | loss 16043.260 \n",
      "| iter    953 | 107.02 ms/step | loss 78850.227 \n",
      "| iter    954 | 25.01 ms/step | loss 14076.209 \n",
      "| iter    955 | 23.01 ms/step | loss 182687.047 \n",
      "| iter    956 | 26.06 ms/step | loss 4176320.000 \n",
      "| iter    957 | 24.01 ms/step | loss 21854.939 \n",
      "| iter    958 | 25.01 ms/step | loss 16281.104 \n",
      "| iter    959 | 23.02 ms/step | loss 916637.938 \n",
      "| iter    960 | 27.01 ms/step | loss 129179.883 \n",
      "| iter    961 | 24.01 ms/step | loss 696454.438 \n",
      "| iter    962 | 25.01 ms/step | loss 1400394.250 \n",
      "| iter    963 | 108.04 ms/step | loss 1349458.375 \n",
      "| iter    964 | 28.01 ms/step | loss 43738.688 \n",
      "| iter    965 | 25.01 ms/step | loss 28884.229 \n",
      "| iter    966 | 26.02 ms/step | loss 9523.799 \n",
      "| iter    967 | 22.01 ms/step | loss 472268.938 \n",
      "| iter    968 | 26.01 ms/step | loss 4985799.500 \n",
      "| iter    969 | 24.01 ms/step | loss 142622.859 \n",
      "| iter    970 | 26.01 ms/step | loss 40835.875 \n",
      "| iter    971 | 110.03 ms/step | loss 10496.291 \n",
      "| iter    972 | 26.00 ms/step | loss 17931.125 \n",
      "| iter    973 | 23.00 ms/step | loss 4608997.500 \n",
      "| iter    974 | 26.01 ms/step | loss 16967.598 \n",
      "| iter    975 | 25.01 ms/step | loss 1472937.875 \n",
      "| iter    976 | 22.94 ms/step | loss 25536.445 \n",
      "| iter    977 | 21.01 ms/step | loss 20125.018 \n",
      "| iter    978 | 24.01 ms/step | loss 116694.602 \n",
      "| iter    979 | 27.01 ms/step | loss 14079.444 \n",
      "| iter    980 | 110.03 ms/step | loss 1573316.375 \n",
      "| iter    981 | 24.00 ms/step | loss 62518.984 \n",
      "| iter    982 | 27.01 ms/step | loss 127811.547 \n",
      "| iter    983 | 27.01 ms/step | loss 1257820.500 \n",
      "| iter    984 | 25.01 ms/step | loss 160749.688 \n",
      "| iter    985 | 23.02 ms/step | loss 16753.930 \n",
      "| iter    986 | 27.01 ms/step | loss 258193.812 \n",
      "| iter    987 | 26.01 ms/step | loss 31790.902 \n",
      "| iter    988 | 22.01 ms/step | loss 35885.289 \n",
      "| iter    989 | 108.02 ms/step | loss 434791.781 \n",
      "| iter    990 | 27.01 ms/step | loss 100273.383 \n",
      "| iter    991 | 21.01 ms/step | loss 38357.441 \n",
      "| iter    992 | 25.01 ms/step | loss 22819.432 \n",
      "| iter    993 | 22.01 ms/step | loss 4172947.500 \n",
      "| iter    994 | 26.01 ms/step | loss 19111.465 \n",
      "| iter    995 | 25.00 ms/step | loss 54990.918 \n",
      "| iter    996 | 26.01 ms/step | loss 105046.562 \n",
      "| iter    997 | 24.01 ms/step | loss 1928314.250 \n",
      "| iter    998 | 105.02 ms/step | loss 27951.531 \n",
      "| iter    999 | 23.01 ms/step | loss 28314.984 \n",
      "| iter   1000 | 27.00 ms/step | loss 972162.312 \n",
      "| iter   1001 | 26.01 ms/step | loss 15875.469 \n",
      "| iter   1002 | 26.00 ms/step | loss 678207.562 \n",
      "| iter   1003 | 26.01 ms/step | loss 64411.020 \n",
      "| iter   1004 | 23.01 ms/step | loss 191301.125 \n",
      "| iter   1005 | 24.01 ms/step | loss 10893.720 \n",
      "| iter   1006 | 27.06 ms/step | loss 18005.043 \n",
      "| iter   1007 | 103.02 ms/step | loss 3401140.000 \n",
      "| iter   1008 | 22.00 ms/step | loss 186520.344 \n",
      "| iter   1009 | 24.02 ms/step | loss 115229.922 \n",
      "| iter   1010 | 26.01 ms/step | loss 15088.932 \n",
      "| iter   1011 | 23.01 ms/step | loss 21022.178 \n",
      "| iter   1012 | 22.01 ms/step | loss 38720.457 \n",
      "| iter   1013 | 23.01 ms/step | loss 515868.781 \n",
      "| iter   1014 | 24.00 ms/step | loss 192461.984 \n",
      "| iter   1015 | 21.00 ms/step | loss 21624.936 \n",
      "| iter   1016 | 25.01 ms/step | loss 2889137.250 \n",
      "| iter   1017 | 23.01 ms/step | loss 18623.814 \n",
      "| iter   1018 | 101.02 ms/step | loss 16419.125 \n",
      "| iter   1019 | 26.01 ms/step | loss 96124.781 \n",
      "| iter   1020 | 23.01 ms/step | loss 1910424.625 \n",
      "| iter   1021 | 25.01 ms/step | loss 27728.975 \n",
      "| iter   1022 | 28.01 ms/step | loss 14999.927 \n",
      "| iter   1023 | 25.01 ms/step | loss 1711413.750 \n",
      "| iter   1024 | 25.01 ms/step | loss 903346.250 \n",
      "| iter   1025 | 26.01 ms/step | loss 98180.617 \n",
      "| iter   1026 | 28.01 ms/step | loss 112554.555 \n",
      "| iter   1027 | 101.02 ms/step | loss 17690.617 \n",
      "| iter   1028 | 25.01 ms/step | loss 2636511.750 \n",
      "| iter   1029 | 25.01 ms/step | loss 9389.836 \n",
      "| iter   1030 | 25.01 ms/step | loss 17157.105 \n",
      "| iter   1031 | 22.01 ms/step | loss 106446.211 \n",
      "| iter   1032 | 26.01 ms/step | loss 2569654.500 \n",
      "| iter   1033 | 21.00 ms/step | loss 34511.250 \n",
      "| iter   1034 | 27.01 ms/step | loss 809089.375 \n",
      "| iter   1035 | 26.02 ms/step | loss 16285.340 \n",
      "| iter   1036 | 102.08 ms/step | loss 8663.165 \n",
      "| iter   1037 | 21.02 ms/step | loss 23313.600 \n",
      "| iter   1038 | 25.01 ms/step | loss 14437.252 \n",
      "| iter   1039 | 26.01 ms/step | loss 363235.469 \n",
      "| iter   1040 | 24.01 ms/step | loss 1313814.875 \n",
      "| iter   1041 | 24.01 ms/step | loss 360422.656 \n",
      "| iter   1042 | 24.01 ms/step | loss 20316.672 \n",
      "| iter   1043 | 26.01 ms/step | loss 1464222.250 \n",
      "| iter   1044 | 27.01 ms/step | loss 183129.078 \n",
      "| iter   1045 | 103.03 ms/step | loss 70788.695 \n",
      "| iter   1046 | 26.01 ms/step | loss 19571.002 \n",
      "| iter   1047 | 23.01 ms/step | loss 1092982.875 \n",
      "| iter   1048 | 23.02 ms/step | loss 9070.646 \n",
      "| iter   1049 | 22.02 ms/step | loss 73275.508 \n",
      "| iter   1050 | 25.01 ms/step | loss 16859.428 \n",
      "| iter   1051 | 26.01 ms/step | loss 732943.500 \n",
      "| iter   1052 | 29.01 ms/step | loss 15575.264 \n",
      "| iter   1053 | 23.01 ms/step | loss 85525.312 \n",
      "| iter   1054 | 27.01 ms/step | loss 1137688.250 \n",
      "| iter   1055 | 99.02 ms/step | loss 593866.875 \n",
      "| iter   1056 | 26.01 ms/step | loss 13656.213 \n",
      "| iter   1057 | 22.06 ms/step | loss 11490.965 \n",
      "| iter   1058 | 25.99 ms/step | loss 1153747.250 \n",
      "| iter   1059 | 25.01 ms/step | loss 59063.172 \n",
      "| iter   1060 | 26.99 ms/step | loss 42585.430 \n",
      "| iter   1061 | 25.01 ms/step | loss 8937.867 \n",
      "| iter   1062 | 28.99 ms/step | loss 26453.201 \n",
      "| iter   1063 | 101.02 ms/step | loss 68076.328 \n",
      "| iter   1064 | 24.00 ms/step | loss 516717.781 \n",
      "| iter   1065 | 20.01 ms/step | loss 4343918.500 \n",
      "| iter   1066 | 27.01 ms/step | loss 57544.469 \n",
      "| iter   1067 | 23.01 ms/step | loss 747556.562 \n",
      "| iter   1068 | 25.02 ms/step | loss 20149.398 \n",
      "| iter   1069 | 23.01 ms/step | loss 7948.146 \n",
      "| iter   1070 | 28.01 ms/step | loss 17371.109 \n",
      "| iter   1071 | 24.01 ms/step | loss 1048645.250 \n",
      "| iter   1072 | 112.08 ms/step | loss 58768.578 \n",
      "| iter   1073 | 24.01 ms/step | loss 7483.694 \n",
      "| iter   1074 | 25.01 ms/step | loss 759649.188 \n",
      "| iter   1075 | 27.01 ms/step | loss 23844.057 \n",
      "| iter   1076 | 25.01 ms/step | loss 64609.039 \n",
      "| iter   1077 | 25.01 ms/step | loss 895068.750 \n",
      "| iter   1078 | 40.99 ms/step | loss 23795.012 \n",
      "| iter   1079 | 39.01 ms/step | loss 44565.492 \n",
      "| iter   1080 | 23.01 ms/step | loss 469629.250 \n",
      "| iter   1081 | 24.01 ms/step | loss 8294.199 \n",
      "| iter   1082 | 110.03 ms/step | loss 27536.625 \n",
      "| iter   1083 | 24.02 ms/step | loss 899338.000 \n",
      "| iter   1084 | 27.01 ms/step | loss 9827.688 \n",
      "| iter   1085 | 25.01 ms/step | loss 26170.090 \n",
      "| iter   1086 | 27.01 ms/step | loss 563093.750 \n",
      "| iter   1087 | 25.01 ms/step | loss 1314249984.000 \n",
      "| iter   1088 | 25.01 ms/step | loss 20971.678 \n",
      "| iter   1089 | 21.00 ms/step | loss 14884.770 \n",
      "| iter   1090 | 25.01 ms/step | loss 91775.398 \n",
      "| iter   1091 | 107.05 ms/step | loss 20693.635 \n",
      "| iter   1092 | 25.01 ms/step | loss 42633532.000 \n",
      "| iter   1093 | 25.01 ms/step | loss 15524.994 \n",
      "| iter   1094 | 24.01 ms/step | loss 1874759.750 \n",
      "| iter   1095 | 22.01 ms/step | loss 55895.473 \n",
      "| iter   1096 | 24.01 ms/step | loss 17393.705 \n",
      "| iter   1097 | 24.00 ms/step | loss 71808.328 \n",
      "| iter   1098 | 22.01 ms/step | loss 109812.195 \n",
      "| iter   1099 | 28.01 ms/step | loss 1254461.125 \n",
      "| iter   1100 | 27.01 ms/step | loss 190313.891 \n",
      "| iter   1101 | 25.01 ms/step | loss 162702.344 \n",
      "| iter   1102 | 107.02 ms/step | loss 148651.984 \n",
      "| iter   1103 | 24.01 ms/step | loss 1117426.250 \n",
      "| iter   1104 | 31.00 ms/step | loss 101140176.000 \n",
      "| iter   1105 | 21.99 ms/step | loss 60646.426 \n",
      "| iter   1106 | 25.01 ms/step | loss 1907574.000 \n",
      "| iter   1107 | 22.01 ms/step | loss 40220.484 \n",
      "| iter   1108 | 25.02 ms/step | loss 45398.727 \n",
      "| iter   1109 | 24.00 ms/step | loss 15111.602 \n",
      "| iter   1110 | 26.99 ms/step | loss 5721141.000 \n",
      "| iter   1111 | 24.01 ms/step | loss 234373.250 \n",
      "| iter   1112 | 104.04 ms/step | loss 155195.625 \n",
      "| iter   1113 | 26.01 ms/step | loss 413377312.000 \n",
      "| iter   1114 | 27.01 ms/step | loss 29893.754 \n",
      "| iter   1115 | 28.01 ms/step | loss 294108.375 \n",
      "| iter   1116 | 24.01 ms/step | loss 20539.477 \n",
      "| iter   1117 | 22.00 ms/step | loss 3941557.000 \n",
      "| iter   1118 | 28.01 ms/step | loss 113137.953 \n",
      "| iter   1119 | 27.01 ms/step | loss 9881697.000 \n",
      "| iter   1120 | 106.02 ms/step | loss 27842.887 \n",
      "| iter   1121 | 24.02 ms/step | loss 486508.750 \n",
      "| iter   1122 | 24.01 ms/step | loss 278542.438 \n",
      "| iter   1123 | 24.01 ms/step | loss 60865.234 \n",
      "| iter   1124 | 29.99 ms/step | loss 86803.406 \n",
      "| iter   1125 | 24.01 ms/step | loss 2923715.500 \n",
      "| iter   1126 | 26.01 ms/step | loss 145930.047 \n",
      "| iter   1127 | 22.01 ms/step | loss 174249.406 \n",
      "| iter   1128 | 25.02 ms/step | loss 72461.383 \n",
      "| iter   1129 | 103.02 ms/step | loss 3552764.750 \n",
      "| iter   1130 | 27.01 ms/step | loss 104328.562 \n",
      "| iter   1131 | 26.01 ms/step | loss 96985.070 \n",
      "| iter   1132 | 24.01 ms/step | loss 222510.422 \n",
      "| iter   1133 | 24.01 ms/step | loss 982726.438 \n",
      "| iter   1134 | 24.01 ms/step | loss 33186.805 \n",
      "| iter   1135 | 25.99 ms/step | loss 73632.125 \n",
      "| iter   1136 | 25.01 ms/step | loss 7303944.500 \n",
      "| iter   1137 | 21.00 ms/step | loss 34263.039 \n",
      "| iter   1138 | 103.04 ms/step | loss 3268712.250 \n",
      "| iter   1139 | 27.01 ms/step | loss 80317.828 \n",
      "| iter   1140 | 24.01 ms/step | loss 576763.375 \n",
      "| iter   1141 | 22.01 ms/step | loss 67789.266 \n",
      "| iter   1142 | 25.01 ms/step | loss 67579.852 \n",
      "| iter   1143 | 26.01 ms/step | loss 27585.812 \n",
      "| iter   1144 | 25.01 ms/step | loss 5131212.000 \n",
      "| iter   1145 | 25.01 ms/step | loss 224174.969 \n",
      "| iter   1146 | 24.01 ms/step | loss 52460.906 \n",
      "| iter   1147 | 24.00 ms/step | loss 7882644.000 \n",
      "| iter   1148 | 108.03 ms/step | loss 87585.281 \n",
      "| iter   1149 | 23.01 ms/step | loss 449197.500 \n",
      "| iter   1150 | 27.01 ms/step | loss 667324.750 \n",
      "| iter   1151 | 27.00 ms/step | loss 940981.062 \n",
      "| iter   1152 | 22.01 ms/step | loss 262111.859 \n",
      "| iter   1153 | 26.01 ms/step | loss 40736.766 \n",
      "| iter   1154 | 25.01 ms/step | loss 46725.281 \n",
      "| iter   1155 | 28.01 ms/step | loss 128646.234 \n",
      "| iter   1156 | 110.03 ms/step | loss 7384083.500 \n",
      "| iter   1157 | 25.01 ms/step | loss 202030.141 \n",
      "| iter   1158 | 25.00 ms/step | loss 56379.078 \n",
      "| iter   1159 | 22.01 ms/step | loss 15816.047 \n",
      "| iter   1160 | 25.01 ms/step | loss 6742559.000 \n",
      "| iter   1161 | 23.01 ms/step | loss 119170.359 \n",
      "| iter   1162 | 25.01 ms/step | loss 40596.164 \n",
      "| iter   1163 | 25.00 ms/step | loss 5036295.500 \n",
      "| iter   1164 | 23.00 ms/step | loss 55010.531 \n",
      "| iter   1165 | 22.07 ms/step | loss 15023.761 \n",
      "| iter   1166 | 101.02 ms/step | loss 43711.191 \n",
      "| iter   1167 | 26.01 ms/step | loss 179576.109 \n",
      "| iter   1168 | 24.01 ms/step | loss 74063.891 \n",
      "| iter   1169 | 25.01 ms/step | loss 955405.000 \n",
      "| iter   1170 | 26.01 ms/step | loss 328635.906 \n",
      "| iter   1171 | 24.02 ms/step | loss 27939.545 \n",
      "| iter   1172 | 26.01 ms/step | loss 874987.938 \n",
      "| iter   1173 | 23.01 ms/step | loss 16212.181 \n",
      "| iter   1174 | 23.01 ms/step | loss 5935407.000 \n",
      "| iter   1175 | 105.02 ms/step | loss 25560524.000 \n",
      "| iter   1176 | 26.01 ms/step | loss 68428.359 \n",
      "| iter   1177 | 26.01 ms/step | loss 71604.359 \n",
      "| iter   1178 | 29.01 ms/step | loss 138535.609 \n",
      "| iter   1179 | 25.01 ms/step | loss 14538.198 \n",
      "| iter   1180 | 24.01 ms/step | loss 5670943.000 \n",
      "| iter   1181 | 24.01 ms/step | loss 19510.918 \n",
      "| iter   1182 | 22.02 ms/step | loss 135655.781 \n",
      "| iter   1183 | 26.43 ms/step | loss 167154.828 \n",
      "| iter   1184 | 28.02 ms/step | loss 378225.875 \n",
      "| iter   1185 | 24.01 ms/step | loss 1772992.500 \n",
      "| iter   1186 | 108.03 ms/step | loss 88715.336 \n",
      "| iter   1187 | 25.02 ms/step | loss 26864.047 \n",
      "| iter   1188 | 26.01 ms/step | loss 1051645.875 \n",
      "| iter   1189 | 23.01 ms/step | loss 270884.812 \n",
      "| iter   1190 | 26.01 ms/step | loss 22478.742 \n",
      "| iter   1191 | 27.01 ms/step | loss 81175.312 \n",
      "| iter   1192 | 24.01 ms/step | loss 372772.094 \n",
      "| iter   1193 | 25.01 ms/step | loss 7148272.000 \n",
      "| iter   1194 | 26.01 ms/step | loss 15088.951 \n",
      "| iter   1195 | 26.01 ms/step | loss 37427.078 \n",
      "| iter   1196 | 106.03 ms/step | loss 1091464.500 \n",
      "| iter   1197 | 25.01 ms/step | loss 85428.992 \n",
      "| iter   1198 | 26.01 ms/step | loss 635590.500 \n",
      "| iter   1199 | 27.01 ms/step | loss 18919.164 \n",
      "| iter   1200 | 27.01 ms/step | loss 665309.250 \n",
      "| iter   1201 | 24.02 ms/step | loss 128003.742 \n",
      "| iter   1202 | 28.01 ms/step | loss 259570.719 \n",
      "| iter   1203 | 25.01 ms/step | loss 25178.047 \n",
      "| iter   1204 | 108.03 ms/step | loss 3459496.500 \n",
      "| iter   1205 | 24.01 ms/step | loss 3972432.500 \n",
      "| iter   1206 | 27.01 ms/step | loss 56951.930 \n",
      "| iter   1207 | 23.01 ms/step | loss 337303.125 \n",
      "| iter   1208 | 25.01 ms/step | loss 3388913.500 \n",
      "| iter   1209 | 24.01 ms/step | loss 6833449.000 \n",
      "| iter   1210 | 24.01 ms/step | loss 28406.062 \n",
      "| iter   1211 | 24.02 ms/step | loss 2945762.750 \n",
      "| iter   1212 | 24.01 ms/step | loss 41089.895 \n",
      "| iter   1213 | 108.02 ms/step | loss 5268642.000 \n",
      "| iter   1214 | 26.01 ms/step | loss 118381.102 \n",
      "| iter   1215 | 27.01 ms/step | loss 2414351.000 \n",
      "| iter   1216 | 23.01 ms/step | loss 92174.141 \n",
      "| iter   1217 | 22.01 ms/step | loss 25234.666 \n",
      "| iter   1218 | 26.00 ms/step | loss 2364087.000 \n",
      "| iter   1219 | 24.01 ms/step | loss 37366.492 \n",
      "| iter   1220 | 25.01 ms/step | loss 46385.688 \n",
      "| iter   1221 | 22.02 ms/step | loss 25581.395 \n",
      "| iter   1222 | 111.03 ms/step | loss 31817.129 \n",
      "| iter   1223 | 24.01 ms/step | loss 3661237.000 \n",
      "| iter   1224 | 26.01 ms/step | loss 66968.656 \n",
      "| iter   1225 | 23.00 ms/step | loss 91315.820 \n",
      "| iter   1226 | 27.01 ms/step | loss 19959.539 \n",
      "| iter   1227 | 25.01 ms/step | loss 96747.875 \n",
      "| iter   1228 | 22.01 ms/step | loss 4130352.250 \n",
      "| iter   1229 | 24.00 ms/step | loss 75301.266 \n",
      "| iter   1230 | 30.01 ms/step | loss 85163.094 \n",
      "| iter   1231 | 21.01 ms/step | loss 7050708.000 \n",
      "| iter   1232 | 104.02 ms/step | loss 458778.688 \n",
      "| iter   1233 | 25.01 ms/step | loss 21716.529 \n",
      "| iter   1234 | 26.01 ms/step | loss 416916.375 \n",
      "| iter   1235 | 25.01 ms/step | loss 192904.406 \n",
      "| iter   1236 | 26.01 ms/step | loss 4156874.250 \n",
      "| iter   1237 | 23.01 ms/step | loss 2331647.250 \n",
      "| iter   1238 | 26.01 ms/step | loss 12891.253 \n",
      "| iter   1239 | 26.00 ms/step | loss 185923.125 \n",
      "| iter   1240 | 24.00 ms/step | loss 73505.516 \n",
      "| iter   1241 | 23.00 ms/step | loss 1147245.875 \n",
      "| iter   1242 | 106.04 ms/step | loss 94801.398 \n",
      "| iter   1243 | 25.00 ms/step | loss 2091340.750 \n",
      "| iter   1244 | 25.01 ms/step | loss 1235481.875 \n",
      "| iter   1245 | 23.01 ms/step | loss 187147.719 \n",
      "| iter   1246 | 26.01 ms/step | loss 17257.273 \n",
      "| iter   1247 | 23.01 ms/step | loss 945681.875 \n",
      "| iter   1248 | 24.01 ms/step | loss 3729844.750 \n",
      "| iter   1249 | 24.01 ms/step | loss 74596.453 \n",
      "| iter   1250 | 25.01 ms/step | loss 42633.273 \n",
      "| iter   1251 | 109.02 ms/step | loss 75693.211 \n",
      "| iter   1252 | 26.01 ms/step | loss 2064924.000 \n",
      "| iter   1253 | 25.01 ms/step | loss 46501.320 \n",
      "| iter   1254 | 24.01 ms/step | loss 1943079.750 \n",
      "| iter   1255 | 26.01 ms/step | loss 22885.340 \n",
      "| iter   1256 | 23.01 ms/step | loss 151041.234 \n",
      "| iter   1257 | 24.00 ms/step | loss 92260.094 \n",
      "| iter   1258 | 25.01 ms/step | loss 2955270.500 \n",
      "| iter   1259 | 25.01 ms/step | loss 30816.609 \n",
      "| iter   1260 | 23.01 ms/step | loss 15887.337 \n",
      "| iter   1261 | 24.04 ms/step | loss 27988.670 \n",
      "| iter   1262 | 108.02 ms/step | loss 18210802.000 \n",
      "| iter   1263 | 29.98 ms/step | loss 34458.227 \n",
      "| iter   1264 | 22.01 ms/step | loss 1494516.250 \n",
      "| iter   1265 | 20.99 ms/step | loss 18162.221 \n",
      "| iter   1266 | 25.01 ms/step | loss 94526.352 \n",
      "| iter   1267 | 26.01 ms/step | loss 87539.680 \n",
      "| iter   1268 | 26.01 ms/step | loss 17465.324 \n",
      "| iter   1269 | 24.01 ms/step | loss 2831066.750 \n",
      "| iter   1270 | 29.01 ms/step | loss 52999.371 \n",
      "| iter   1271 | 26.01 ms/step | loss 18467.516 \n",
      "| iter   1272 | 102.09 ms/step | loss 1138327.500 \n",
      "| iter   1273 | 24.01 ms/step | loss 27945.348 \n",
      "| iter   1274 | 22.00 ms/step | loss 15964.608 \n",
      "| iter   1275 | 24.01 ms/step | loss 551880.188 \n",
      "| iter   1276 | 24.01 ms/step | loss 17055.906 \n",
      "| iter   1277 | 25.01 ms/step | loss 160356.844 \n",
      "| iter   1278 | 26.01 ms/step | loss 102525.531 \n",
      "| iter   1279 | 24.01 ms/step | loss 11868.445 \n",
      "| iter   1280 | 111.02 ms/step | loss 970835.250 \n",
      "| iter   1281 | 26.00 ms/step | loss 80480.258 \n",
      "| iter   1282 | 24.01 ms/step | loss 38177.262 \n",
      "| iter   1283 | 27.01 ms/step | loss 9739.375 \n",
      "| iter   1284 | 26.01 ms/step | loss 11335.898 \n",
      "| iter   1285 | 25.01 ms/step | loss 2154987.250 \n",
      "| iter   1286 | 25.01 ms/step | loss 87864.055 \n",
      "| iter   1287 | 27.01 ms/step | loss 20524.570 \n",
      "| iter   1288 | 26.00 ms/step | loss 2792291.500 \n",
      "| iter   1289 | 108.02 ms/step | loss 93093.852 \n",
      "| iter   1290 | 25.01 ms/step | loss 171832.188 \n",
      "| iter   1291 | 24.00 ms/step | loss 2723780.500 \n",
      "| iter   1292 | 24.01 ms/step | loss 70604.102 \n",
      "| iter   1293 | 23.01 ms/step | loss 31274.594 \n",
      "| iter   1294 | 25.01 ms/step | loss 402849.531 \n",
      "| iter   1295 | 26.01 ms/step | loss 51392.562 \n",
      "| iter   1296 | 24.01 ms/step | loss 100572.500 \n",
      "| iter   1297 | 22.01 ms/step | loss 66327.664 \n",
      "| iter   1298 | 107.02 ms/step | loss 114067880.000 \n",
      "| iter   1299 | 26.01 ms/step | loss 998808.938 \n",
      "| iter   1300 | 27.01 ms/step | loss 52029.312 \n",
      "| iter   1301 | 20.00 ms/step | loss 15292.472 \n",
      "| iter   1302 | 23.00 ms/step | loss 46464.969 \n",
      "| iter   1303 | 25.01 ms/step | loss 757626.500 \n",
      "| iter   1304 | 27.01 ms/step | loss 58675.676 \n",
      "| iter   1305 | 27.01 ms/step | loss 16519.777 \n",
      "| iter   1306 | 25.01 ms/step | loss 47416.152 \n",
      "| iter   1307 | 23.01 ms/step | loss 844436.812 \n",
      "| iter   1308 | 115.02 ms/step | loss 32628.623 \n",
      "| iter   1309 | 24.01 ms/step | loss 17200.615 \n",
      "| iter   1310 | 28.01 ms/step | loss 805419.062 \n",
      "| iter   1311 | 25.00 ms/step | loss 8099.932 \n",
      "| iter   1312 | 27.00 ms/step | loss 969841.000 \n",
      "| iter   1313 | 22.00 ms/step | loss 13233.793 \n",
      "| iter   1314 | 25.01 ms/step | loss 23094.010 \n",
      "| iter   1315 | 27.01 ms/step | loss 14331.830 \n",
      "| iter   1316 | 29.01 ms/step | loss 1111108.000 \n",
      "| iter   1317 | 24.00 ms/step | loss 38709.906 \n",
      "| iter   1318 | 115.02 ms/step | loss 279206.531 \n",
      "| iter   1319 | 25.02 ms/step | loss 20722.602 \n",
      "| iter   1320 | 24.01 ms/step | loss 999920.875 \n",
      "| iter   1321 | 24.01 ms/step | loss 112132.523 \n",
      "| iter   1322 | 28.00 ms/step | loss 100496.750 \n",
      "| iter   1323 | 26.01 ms/step | loss 17484.928 \n",
      "| iter   1324 | 25.00 ms/step | loss 1706797.250 \n",
      "| iter   1325 | 28.00 ms/step | loss 30708.684 \n",
      "| iter   1326 | 30.01 ms/step | loss 98089.445 \n",
      "| iter   1327 | 127.03 ms/step | loss 52305.977 \n",
      "| iter   1328 | 27.01 ms/step | loss 774585.812 \n",
      "| iter   1329 | 25.01 ms/step | loss 8960.548 \n",
      "| iter   1330 | 23.00 ms/step | loss 1056661.250 \n",
      "| iter   1331 | 26.01 ms/step | loss 13044.342 \n",
      "| iter   1332 | 26.01 ms/step | loss 110902.430 \n",
      "| iter   1333 | 26.01 ms/step | loss 638005.125 \n",
      "| iter   1334 | 28.01 ms/step | loss 14993.826 \n",
      "| iter   1335 | 24.00 ms/step | loss 67232.070 \n",
      "| iter   1336 | 24.01 ms/step | loss 21288.062 \n",
      "| iter   1337 | 23.00 ms/step | loss 450793.438 \n",
      "| iter   1338 | 161.04 ms/step | loss 23611.467 \n",
      "| iter   1339 | 26.01 ms/step | loss 73221.875 \n",
      "| iter   1340 | 27.01 ms/step | loss 594545.250 \n",
      "| iter   1341 | 25.01 ms/step | loss 165646.812 \n",
      "| iter   1342 | 30.01 ms/step | loss 9025.807 \n",
      "| iter   1343 | 28.01 ms/step | loss 22737.729 \n",
      "| iter   1344 | 28.01 ms/step | loss 71236.188 \n",
      "| iter   1345 | 27.01 ms/step | loss 490388.969 \n",
      "| iter   1346 | 27.00 ms/step | loss 28935.949 \n",
      "| iter   1347 | 25.01 ms/step | loss 480916.469 \n",
      "| iter   1348 | 120.03 ms/step | loss 30440.338 \n",
      "| iter   1349 | 26.01 ms/step | loss 18967.307 \n",
      "| iter   1350 | 25.01 ms/step | loss 619052.000 \n",
      "| iter   1351 | 24.01 ms/step | loss 220508.875 \n",
      "| iter   1352 | 27.01 ms/step | loss 545383.062 \n",
      "| iter   1353 | 24.01 ms/step | loss 51437.777 \n",
      "| iter   1354 | 28.01 ms/step | loss 91430.719 \n",
      "| iter   1355 | 28.01 ms/step | loss 576205.750 \n",
      "| iter   1356 | 25.01 ms/step | loss 151406.797 \n",
      "| iter   1357 | 104.10 ms/step | loss 41715.086 \n",
      "| iter   1358 | 29.01 ms/step | loss 15695.987 \n",
      "| iter   1359 | 25.02 ms/step | loss 10217.898 \n",
      "| iter   1360 | 24.01 ms/step | loss 36944.105 \n",
      "| iter   1361 | 23.01 ms/step | loss 1197566.250 \n",
      "| iter   1362 | 26.01 ms/step | loss 16371665.000 \n",
      "| iter   1363 | 24.01 ms/step | loss 721871.625 \n",
      "| iter   1364 | 23.01 ms/step | loss 11183.008 \n",
      "| iter   1365 | 110.02 ms/step | loss 490957.438 \n",
      "| iter   1366 | 30.00 ms/step | loss 15407.198 \n",
      "| iter   1367 | 26.01 ms/step | loss 29748.441 \n",
      "| iter   1368 | 24.99 ms/step | loss 571999.875 \n",
      "| iter   1369 | 25.00 ms/step | loss 59552.629 \n",
      "| iter   1370 | 26.99 ms/step | loss 60739.758 \n",
      "| iter   1371 | 26.01 ms/step | loss 37208.125 \n",
      "| iter   1372 | 26.02 ms/step | loss 34418.492 \n",
      "| iter   1373 | 25.01 ms/step | loss 602875.562 \n",
      "| iter   1374 | 114.03 ms/step | loss 13386.391 \n",
      "| iter   1375 | 25.00 ms/step | loss 81124.992 \n",
      "| iter   1376 | 25.01 ms/step | loss 619448.125 \n",
      "| iter   1377 | 24.01 ms/step | loss 10202.295 \n",
      "| iter   1378 | 27.07 ms/step | loss 687507.125 \n",
      "| iter   1379 | 26.01 ms/step | loss 12102.112 \n",
      "| iter   1380 | 24.00 ms/step | loss 14663.500 \n",
      "| iter   1381 | 23.00 ms/step | loss 85874.109 \n",
      "| iter   1382 | 25.99 ms/step | loss 13933.834 \n",
      "| iter   1383 | 24.00 ms/step | loss 66669.852 \n",
      "| iter   1384 | 112.02 ms/step | loss 71749.875 \n",
      "| iter   1385 | 23.01 ms/step | loss 583784.812 \n",
      "| iter   1386 | 28.01 ms/step | loss 488751.688 \n",
      "| iter   1387 | 26.99 ms/step | loss 9644.957 \n",
      "| iter   1388 | 25.01 ms/step | loss 47648.168 \n",
      "| iter   1389 | 25.99 ms/step | loss 11548.382 \n",
      "| iter   1390 | 27.01 ms/step | loss 531283.438 \n",
      "| iter   1391 | 23.01 ms/step | loss 10127.129 \n",
      "| iter   1392 | 25.01 ms/step | loss 46175.164 \n",
      "| iter   1393 | 23.01 ms/step | loss 760116.812 \n",
      "| iter   1394 | 109.04 ms/step | loss 503890.312 \n",
      "| iter   1395 | 25.01 ms/step | loss 266065.000 \n",
      "| iter   1396 | 25.01 ms/step | loss 18515.270 \n",
      "| iter   1397 | 24.01 ms/step | loss 326938.000 \n",
      "| iter   1398 | 26.01 ms/step | loss 31631.602 \n",
      "| iter   1399 | 24.01 ms/step | loss 556017.938 \n",
      "| iter   1400 | 25.01 ms/step | loss 66469.148 \n",
      "| iter   1401 | 23.01 ms/step | loss 10160.110 \n",
      "| iter   1402 | 37.00 ms/step | loss 9903.936 \n",
      "| iter   1403 | 23.01 ms/step | loss 405255.594 \n",
      "| iter   1404 | 101.02 ms/step | loss 23049.875 \n",
      "| iter   1405 | 25.01 ms/step | loss 64015.527 \n",
      "| iter   1406 | 28.01 ms/step | loss 21439.785 \n",
      "| iter   1407 | 24.00 ms/step | loss 504519.469 \n",
      "| iter   1408 | 24.99 ms/step | loss 9119.572 \n",
      "| iter   1409 | 21.00 ms/step | loss 404752.094 \n",
      "| iter   1410 | 27.99 ms/step | loss 231484.469 \n",
      "| iter   1411 | 25.01 ms/step | loss 10061.630 \n",
      "| iter   1412 | 24.00 ms/step | loss 32298.336 \n",
      "| iter   1413 | 24.01 ms/step | loss 451908.562 \n",
      "| iter   1414 | 113.03 ms/step | loss 16930.191 \n",
      "| iter   1415 | 22.99 ms/step | loss 15530.833 \n",
      "| iter   1416 | 22.00 ms/step | loss 131403.984 \n",
      "| iter   1417 | 23.00 ms/step | loss 546552.062 \n",
      "| iter   1418 | 25.01 ms/step | loss 589279.312 \n",
      "| iter   1419 | 23.99 ms/step | loss 11617.786 \n",
      "| iter   1420 | 21.99 ms/step | loss 95725.523 \n",
      "| iter   1421 | 23.01 ms/step | loss 25469.758 \n",
      "| iter   1422 | 28.01 ms/step | loss 15114.939 \n",
      "| iter   1423 | 25.01 ms/step | loss 457480.188 \n",
      "| iter   1424 | 105.02 ms/step | loss 40880.188 \n",
      "| iter   1425 | 22.01 ms/step | loss 31489.174 \n",
      "| iter   1426 | 26.99 ms/step | loss 13559.002 \n",
      "| iter   1427 | 26.01 ms/step | loss 293352.750 \n",
      "| iter   1428 | 26.01 ms/step | loss 26342426.000 \n",
      "| iter   1429 | 24.01 ms/step | loss 475086.375 \n",
      "| iter   1430 | 25.00 ms/step | loss 370491.344 \n",
      "| iter   1431 | 23.99 ms/step | loss 9661.252 \n",
      "| iter   1432 | 108.03 ms/step | loss 87986.578 \n",
      "| iter   1433 | 24.00 ms/step | loss 14352.182 \n",
      "| iter   1434 | 23.01 ms/step | loss 32124.988 \n",
      "| iter   1435 | 24.02 ms/step | loss 14200.880 \n",
      "| iter   1436 | 25.01 ms/step | loss 364019.969 \n",
      "| iter   1437 | 22.01 ms/step | loss 130125.562 \n",
      "| iter   1438 | 27.01 ms/step | loss 38253.211 \n",
      "| iter   1439 | 25.01 ms/step | loss 964810.000 \n",
      "| iter   1440 | 25.01 ms/step | loss 339227.312 \n",
      "| iter   1441 | 106.02 ms/step | loss 854166.125 \n",
      "| iter   1442 | 29.01 ms/step | loss 428324.406 \n",
      "| iter   1443 | 28.01 ms/step | loss 13379.641 \n",
      "| iter   1444 | 23.01 ms/step | loss 4495705.500 \n",
      "| iter   1445 | 26.00 ms/step | loss 22332.693 \n",
      "| iter   1446 | 27.01 ms/step | loss 283826.625 \n",
      "| iter   1447 | 23.01 ms/step | loss 115961.719 \n",
      "| iter   1448 | 22.00 ms/step | loss 14363.148 \n",
      "| iter   1449 | 24.01 ms/step | loss 16120.916 \n",
      "| iter   1450 | 111.02 ms/step | loss 7872.199 \n",
      "| iter   1451 | 28.01 ms/step | loss 366551.281 \n",
      "| iter   1452 | 27.01 ms/step | loss 92942.688 \n",
      "| iter   1453 | 26.01 ms/step | loss 36583.742 \n",
      "| iter   1454 | 26.01 ms/step | loss 70020.695 \n",
      "| iter   1455 | 25.01 ms/step | loss 32373.092 \n",
      "| iter   1456 | 26.01 ms/step | loss 274656.781 \n",
      "| iter   1457 | 24.01 ms/step | loss 93603352.000 \n",
      "| iter   1458 | 28.01 ms/step | loss 44885.586 \n",
      "| iter   1459 | 26.01 ms/step | loss 10431.352 \n",
      "| iter   1460 | 109.02 ms/step | loss 338076.125 \n",
      "| iter   1461 | 25.03 ms/step | loss 20464.254 \n",
      "| iter   1462 | 26.01 ms/step | loss 375767.156 \n",
      "| iter   1463 | 26.01 ms/step | loss 44825.102 \n",
      "| iter   1464 | 26.01 ms/step | loss 10280.342 \n",
      "| iter   1465 | 24.01 ms/step | loss 20956.146 \n",
      "| iter   1466 | 28.01 ms/step | loss 42706.172 \n",
      "| iter   1467 | 23.01 ms/step | loss 10617.168 \n",
      "| iter   1468 | 25.01 ms/step | loss 8500419.000 \n",
      "| iter   1469 | 22.02 ms/step | loss 28002.996 \n",
      "| iter   1470 | 113.01 ms/step | loss 49497.578 \n",
      "| iter   1471 | 27.00 ms/step | loss 326114.719 \n",
      "| iter   1472 | 27.01 ms/step | loss 28312.473 \n",
      "| iter   1473 | 22.99 ms/step | loss 26819.311 \n",
      "| iter   1474 | 24.00 ms/step | loss 234753.125 \n",
      "| iter   1475 | 25.00 ms/step | loss 83566.195 \n",
      "| iter   1476 | 24.01 ms/step | loss 8861.230 \n",
      "| iter   1477 | 24.01 ms/step | loss 433886.969 \n",
      "| iter   1478 | 27.01 ms/step | loss 367427.062 \n",
      "| iter   1479 | 112.03 ms/step | loss 43484.246 \n",
      "| iter   1480 | 23.00 ms/step | loss 24758.734 \n",
      "| iter   1481 | 21.01 ms/step | loss 7761.797 \n",
      "| iter   1482 | 24.02 ms/step | loss 386513.656 \n",
      "| iter   1483 | 24.99 ms/step | loss 36153.906 \n",
      "| iter   1484 | 23.01 ms/step | loss 20191.891 \n",
      "| iter   1485 | 22.99 ms/step | loss 30833.863 \n",
      "| iter   1486 | 27.01 ms/step | loss 341252.344 \n",
      "| iter   1487 | 26.01 ms/step | loss 27679.240 \n",
      "| iter   1488 | 23.01 ms/step | loss 688116.312 \n",
      "| iter   1489 | 22.01 ms/step | loss 265028.125 \n",
      "| iter   1490 | 103.02 ms/step | loss 354698.781 \n",
      "| iter   1491 | 23.01 ms/step | loss 27459.318 \n",
      "| iter   1492 | 27.01 ms/step | loss 21016.041 \n",
      "| iter   1493 | 23.00 ms/step | loss 22330.332 \n",
      "| iter   1494 | 25.01 ms/step | loss 30600.898 \n",
      "| iter   1495 | 23.01 ms/step | loss 108503.922 \n",
      "| iter   1496 | 26.01 ms/step | loss 322111.406 \n",
      "| iter   1497 | 25.02 ms/step | loss 8276.634 \n",
      "| iter   1498 | 26.01 ms/step | loss 55418.328 \n",
      "| iter   1499 | 24.01 ms/step | loss 36145.211 \n",
      "| iter   1500 | 110.04 ms/step | loss 338951.250 \n",
      "| iter   1501 | 27.01 ms/step | loss 10876.198 \n",
      "| iter   1502 | 27.01 ms/step | loss 339592.625 \n",
      "| iter   1503 | 26.07 ms/step | loss 529087.812 \n",
      "| iter   1504 | 26.02 ms/step | loss 12846.733 \n",
      "| iter   1505 | 20.01 ms/step | loss 498475.438 \n",
      "| iter   1506 | 25.02 ms/step | loss 370946.000 \n",
      "| iter   1507 | 26.00 ms/step | loss 15973.310 \n",
      "| iter   1508 | 107.03 ms/step | loss 13517.146 \n",
      "| iter   1509 | 21.01 ms/step | loss 24479.943 \n",
      "| iter   1510 | 25.99 ms/step | loss 39666.875 \n",
      "| iter   1511 | 27.01 ms/step | loss 20469.281 \n",
      "| iter   1512 | 26.01 ms/step | loss 7091.171 \n",
      "| iter   1513 | 24.01 ms/step | loss 338432.688 \n",
      "| iter   1514 | 23.01 ms/step | loss 372861.812 \n",
      "| iter   1515 | 25.00 ms/step | loss 64544.492 \n",
      "| iter   1516 | 26.01 ms/step | loss 17429.832 \n",
      "| iter   1517 | 103.02 ms/step | loss 63949.102 \n",
      "| iter   1518 | 27.00 ms/step | loss 320768.812 \n",
      "| iter   1519 | 26.01 ms/step | loss 33914.594 \n",
      "| iter   1520 | 27.01 ms/step | loss 200769.719 \n",
      "| iter   1521 | 23.01 ms/step | loss 35539.332 \n",
      "| iter   1522 | 23.01 ms/step | loss 292887.438 \n",
      "| iter   1523 | 26.00 ms/step | loss 13975.500 \n",
      "| iter   1524 | 24.01 ms/step | loss 61206.094 \n",
      "| iter   1525 | 21.01 ms/step | loss 37141.570 \n",
      "| iter   1526 | 23.02 ms/step | loss 54591.906 \n",
      "| iter   1527 | 105.01 ms/step | loss 369033.719 \n",
      "| iter   1528 | 29.01 ms/step | loss 29047.492 \n",
      "| iter   1529 | 22.00 ms/step | loss 5471.453 \n",
      "| iter   1530 | 23.99 ms/step | loss 29881.266 \n",
      "| iter   1531 | 26.99 ms/step | loss 17998.268 \n",
      "| iter   1532 | 25.02 ms/step | loss 361037.344 \n",
      "| iter   1533 | 22.00 ms/step | loss 16547.193 \n",
      "| iter   1534 | 26.01 ms/step | loss 31688.635 \n",
      "| iter   1535 | 102.02 ms/step | loss 362635.062 \n",
      "| iter   1536 | 27.01 ms/step | loss 17968.676 \n",
      "| iter   1537 | 27.01 ms/step | loss 12448.602 \n",
      "| iter   1538 | 26.01 ms/step | loss 411502.406 \n",
      "| iter   1539 | 23.01 ms/step | loss 812810.188 \n",
      "| iter   1540 | 24.00 ms/step | loss 7850.992 \n",
      "| iter   1541 | 25.01 ms/step | loss 51785.824 \n",
      "| iter   1542 | 25.01 ms/step | loss 164255.281 \n",
      "| iter   1543 | 29.01 ms/step | loss 361106.969 \n",
      "| iter   1544 | 111.02 ms/step | loss 15882.225 \n",
      "| iter   1545 | 22.01 ms/step | loss 47259.984 \n",
      "| iter   1546 | 24.02 ms/step | loss 40703.422 \n",
      "| iter   1547 | 22.01 ms/step | loss 25409.400 \n",
      "| iter   1548 | 25.01 ms/step | loss 5857652.500 \n",
      "| iter   1549 | 25.01 ms/step | loss 4283667.500 \n",
      "| iter   1550 | 23.00 ms/step | loss 8323.273 \n",
      "| iter   1551 | 26.99 ms/step | loss 335641.312 \n",
      "| iter   1552 | 22.99 ms/step | loss 30857.797 \n",
      "| iter   1553 | 97.04 ms/step | loss 25765.861 \n",
      "| iter   1554 | 26.01 ms/step | loss 362536.594 \n",
      "| iter   1555 | 26.00 ms/step | loss 8679.955 \n",
      "| iter   1556 | 25.01 ms/step | loss 48397.203 \n",
      "| iter   1557 | 24.00 ms/step | loss 16930.961 \n",
      "| iter   1558 | 26.99 ms/step | loss 510763.188 \n",
      "| iter   1559 | 27.01 ms/step | loss 28556.910 \n",
      "| iter   1560 | 27.01 ms/step | loss 6026.810 \n",
      "| iter   1561 | 27.01 ms/step | loss 41681.828 \n",
      "| iter   1562 | 110.08 ms/step | loss 12268.393 \n",
      "| iter   1563 | 28.01 ms/step | loss 32223.709 \n",
      "| iter   1564 | 25.01 ms/step | loss 405456.969 \n",
      "| iter   1565 | 24.01 ms/step | loss 29509.875 \n",
      "| iter   1566 | 25.01 ms/step | loss 4922.384 \n",
      "| iter   1567 | 23.00 ms/step | loss 23823.064 \n",
      "| iter   1568 | 24.01 ms/step | loss 23863.658 \n",
      "| iter   1569 | 24.02 ms/step | loss 451630.500 \n",
      "| iter   1570 | 29.01 ms/step | loss 14268.769 \n",
      "| iter   1571 | 102.02 ms/step | loss 42154.125 \n",
      "| iter   1572 | 24.01 ms/step | loss 9637.473 \n",
      "| iter   1573 | 24.01 ms/step | loss 5507074.500 \n",
      "| iter   1574 | 27.02 ms/step | loss 2958431.750 \n",
      "| iter   1575 | 27.01 ms/step | loss 38592.383 \n",
      "| iter   1576 | 22.01 ms/step | loss 7466.461 \n",
      "| iter   1577 | 25.01 ms/step | loss 16224.719 \n",
      "| iter   1578 | 34.01 ms/step | loss 7852.883 \n",
      "| iter   1579 | 25.01 ms/step | loss 14237.727 \n",
      "| iter   1580 | 26.01 ms/step | loss 32196.123 \n",
      "| iter   1581 | 22.02 ms/step | loss 612066.062 \n",
      "| iter   1582 | 120.01 ms/step | loss 10733.644 \n",
      "| iter   1583 | 27.01 ms/step | loss 33037.266 \n",
      "| iter   1584 | 27.02 ms/step | loss 53417.914 \n",
      "| iter   1585 | 25.99 ms/step | loss 592720.250 \n",
      "| iter   1586 | 27.01 ms/step | loss 22119.213 \n",
      "| iter   1587 | 29.01 ms/step | loss 6724.552 \n",
      "| iter   1588 | 28.01 ms/step | loss 462556.375 \n",
      "| iter   1589 | 23.00 ms/step | loss 358694.438 \n",
      "| iter   1590 | 24.01 ms/step | loss 531540.125 \n",
      "| iter   1591 | 109.04 ms/step | loss 62481.766 \n",
      "| iter   1592 | 27.01 ms/step | loss 17824.402 \n",
      "| iter   1593 | 23.01 ms/step | loss 108586.969 \n",
      "| iter   1594 | 25.01 ms/step | loss 397782.750 \n",
      "| iter   1595 | 25.01 ms/step | loss 58541.461 \n",
      "| iter   1596 | 25.01 ms/step | loss 7491.896 \n",
      "| iter   1597 | 24.01 ms/step | loss 48916.801 \n",
      "| iter   1598 | 24.01 ms/step | loss 543407.500 \n",
      "| iter   1599 | 32.99 ms/step | loss 274310.188 \n",
      "| iter   1600 | 117.03 ms/step | loss 12047.725 \n",
      "| iter   1601 | 26.01 ms/step | loss 12950.999 \n",
      "| iter   1602 | 27.01 ms/step | loss 421530.625 \n",
      "| iter   1603 | 24.00 ms/step | loss 20693.262 \n",
      "| iter   1604 | 26.01 ms/step | loss 52105.953 \n",
      "| iter   1605 | 23.01 ms/step | loss 34174.375 \n",
      "| iter   1606 | 28.00 ms/step | loss 13685.641 \n",
      "| iter   1607 | 27.01 ms/step | loss 25672.527 \n",
      "| iter   1608 | 24.01 ms/step | loss 624315.312 \n",
      "| iter   1609 | 103.02 ms/step | loss 46580.891 \n",
      "| iter   1610 | 28.01 ms/step | loss 51184.219 \n",
      "| iter   1611 | 25.01 ms/step | loss 107687.516 \n",
      "| iter   1612 | 24.01 ms/step | loss 633773.688 \n",
      "| iter   1613 | 23.99 ms/step | loss 7985.194 \n",
      "| iter   1614 | 25.01 ms/step | loss 8521.327 \n",
      "| iter   1615 | 25.01 ms/step | loss 49358.020 \n",
      "| iter   1616 | 25.01 ms/step | loss 447707.688 \n",
      "| iter   1617 | 21.99 ms/step | loss 19443.135 \n",
      "| iter   1618 | 110.03 ms/step | loss 416097.031 \n",
      "| iter   1619 | 34.01 ms/step | loss 22902.668 \n",
      "| iter   1620 | 25.01 ms/step | loss 44546.410 \n",
      "| iter   1621 | 23.01 ms/step | loss 25827.822 \n",
      "| iter   1622 | 25.00 ms/step | loss 8987.227 \n",
      "| iter   1623 | 22.01 ms/step | loss 5911.477 \n",
      "| iter   1624 | 26.01 ms/step | loss 421871.906 \n",
      "| iter   1625 | 21.01 ms/step | loss 61985.629 \n",
      "| iter   1626 | 25.01 ms/step | loss 44531.480 \n",
      "| iter   1627 | 28.01 ms/step | loss 401688.906 \n",
      "| iter   1628 | 104.04 ms/step | loss 13378.784 \n",
      "| iter   1629 | 21.01 ms/step | loss 14501.164 \n",
      "| iter   1630 | 26.01 ms/step | loss 8400.535 \n",
      "| iter   1631 | 28.00 ms/step | loss 438623.938 \n",
      "| iter   1632 | 24.00 ms/step | loss 8537.686 \n",
      "| iter   1633 | 21.01 ms/step | loss 26365.250 \n",
      "| iter   1634 | 23.01 ms/step | loss 43049.996 \n",
      "| iter   1635 | 24.02 ms/step | loss 383044.531 \n",
      "| iter   1636 | 26.01 ms/step | loss 26193.891 \n",
      "| iter   1637 | 24.01 ms/step | loss 8962.171 \n",
      "| iter   1638 | 106.03 ms/step | loss 13764.432 \n",
      "| iter   1639 | 23.99 ms/step | loss 23761.078 \n",
      "| iter   1640 | 25.03 ms/step | loss 493738.000 \n",
      "| iter   1641 | 24.01 ms/step | loss 10029.038 \n",
      "| iter   1642 | 32.01 ms/step | loss 100476.953 \n",
      "| iter   1643 | 21.02 ms/step | loss 17482.807 \n",
      "| iter   1644 | 22.01 ms/step | loss 1065659.750 \n",
      "| iter   1645 | 26.00 ms/step | loss 35010.992 \n",
      "| iter   1646 | 25.01 ms/step | loss 492964.094 \n",
      "| iter   1647 | 115.03 ms/step | loss 39639.383 \n",
      "| iter   1648 | 46.01 ms/step | loss 11942.168 \n",
      "| iter   1649 | 30.01 ms/step | loss 26285.145 \n",
      "| iter   1650 | 25.01 ms/step | loss 34542.004 \n",
      "| iter   1651 | 26.02 ms/step | loss 8647.394 \n",
      "| iter   1652 | 24.02 ms/step | loss 477624.469 \n",
      "| iter   1653 | 25.02 ms/step | loss 40379.684 \n",
      "| iter   1654 | 26.00 ms/step | loss 13006.403 \n",
      "| iter   1655 | 25.01 ms/step | loss 54717.289 \n",
      "| iter   1656 | 24.02 ms/step | loss 11047.747 \n",
      "| iter   1657 | 23.00 ms/step | loss 482160.688 \n",
      "| iter   1658 | 114.04 ms/step | loss 439258.031 \n",
      "| iter   1659 | 27.01 ms/step | loss 13733.775 \n",
      "| iter   1660 | 27.03 ms/step | loss 6134.450 \n",
      "| iter   1661 | 25.00 ms/step | loss 70824.492 \n",
      "| iter   1662 | 26.02 ms/step | loss 413794.125 \n",
      "| iter   1663 | 27.01 ms/step | loss 8159.563 \n",
      "| iter   1664 | 26.01 ms/step | loss 51880.219 \n",
      "| iter   1665 | 24.00 ms/step | loss 10376.211 \n",
      "| iter   1666 | 23.01 ms/step | loss 44114.953 \n",
      "| iter   1667 | 23.00 ms/step | loss 15353.683 \n",
      "| iter   1668 | 110.05 ms/step | loss 394042.125 \n",
      "| iter   1669 | 23.01 ms/step | loss 12405.684 \n",
      "| iter   1670 | 24.98 ms/step | loss 430513.688 \n",
      "| iter   1671 | 25.01 ms/step | loss 103764.328 \n",
      "| iter   1672 | 25.01 ms/step | loss 8320.047 \n",
      "| iter   1673 | 23.01 ms/step | loss 39626.453 \n",
      "| iter   1674 | 28.01 ms/step | loss 14578.463 \n",
      "| iter   1675 | 24.01 ms/step | loss 26008.910 \n",
      "| iter   1676 | 27.01 ms/step | loss 380252.969 \n",
      "| iter   1677 | 106.11 ms/step | loss 56403.324 \n",
      "| iter   1678 | 27.01 ms/step | loss 9974.876 \n",
      "| iter   1679 | 24.01 ms/step | loss 45780.609 \n",
      "| iter   1680 | 28.01 ms/step | loss 368728.438 \n",
      "| iter   1681 | 25.01 ms/step | loss 25245.018 \n",
      "| iter   1682 | 27.01 ms/step | loss 25027.848 \n",
      "| iter   1683 | 28.00 ms/step | loss 37338.754 \n",
      "| iter   1684 | 25.01 ms/step | loss 382501.656 \n",
      "| iter   1685 | 110.02 ms/step | loss 26362.955 \n",
      "| iter   1686 | 27.01 ms/step | loss 62557.656 \n",
      "| iter   1687 | 23.01 ms/step | loss 371858.438 \n",
      "| iter   1688 | 27.01 ms/step | loss 12618.436 \n",
      "| iter   1689 | 25.01 ms/step | loss 101293.680 \n",
      "| iter   1690 | 27.01 ms/step | loss 43685.508 \n",
      "| iter   1691 | 24.01 ms/step | loss 27019.027 \n",
      "| iter   1692 | 25.01 ms/step | loss 15235.090 \n",
      "| iter   1693 | 23.01 ms/step | loss 393966.375 \n",
      "| iter   1694 | 104.03 ms/step | loss 37498.383 \n",
      "| iter   1695 | 23.21 ms/step | loss 15595.883 \n",
      "| iter   1696 | 24.99 ms/step | loss 369463.500 \n",
      "| iter   1697 | 22.00 ms/step | loss 6662.512 \n",
      "| iter   1698 | 22.01 ms/step | loss 32670.918 \n",
      "| iter   1699 | 26.01 ms/step | loss 17273.023 \n",
      "| iter   1700 | 22.01 ms/step | loss 328240.062 \n",
      "| iter   1701 | 22.00 ms/step | loss 10571.487 \n",
      "| iter   1702 | 24.00 ms/step | loss 22736.771 \n",
      "| iter   1703 | 25.01 ms/step | loss 16113.793 \n",
      "| iter   1704 | 102.01 ms/step | loss 34319.367 \n",
      "| iter   1705 | 22.00 ms/step | loss 374732.312 \n",
      "| iter   1706 | 27.01 ms/step | loss 16484.127 \n",
      "| iter   1707 | 23.00 ms/step | loss 27662.445 \n",
      "| iter   1708 | 27.01 ms/step | loss 313688.812 \n",
      "| iter   1709 | 26.01 ms/step | loss 12205.043 \n",
      "| iter   1710 | 26.01 ms/step | loss 15865.227 \n",
      "| iter   1711 | 26.99 ms/step | loss 32728.545 \n",
      "| iter   1712 | 26.01 ms/step | loss 326051.375 \n",
      "| iter   1713 | 24.00 ms/step | loss 82656.539 \n",
      "| iter   1714 | 123.03 ms/step | loss 59865.762 \n",
      "| iter   1715 | 29.01 ms/step | loss 10888.087 \n",
      "| iter   1716 | 26.01 ms/step | loss 32386.857 \n",
      "| iter   1717 | 24.01 ms/step | loss 351316.281 \n",
      "| iter   1718 | 25.01 ms/step | loss 15905.156 \n",
      "| iter   1719 | 26.00 ms/step | loss 24199.398 \n",
      "| iter   1720 | 24.01 ms/step | loss 493893.875 \n",
      "| iter   1721 | 24.01 ms/step | loss 53710.402 \n",
      "| iter   1722 | 28.01 ms/step | loss 31131.789 \n",
      "| iter   1723 | 27.00 ms/step | loss 33762.312 \n",
      "| iter   1724 | 115.04 ms/step | loss 30161.213 \n",
      "| iter   1725 | 22.00 ms/step | loss 329616.281 \n",
      "| iter   1726 | 28.01 ms/step | loss 19434.268 \n",
      "| iter   1727 | 25.99 ms/step | loss 10086.309 \n",
      "| iter   1728 | 25.02 ms/step | loss 39211.332 \n",
      "| iter   1729 | 27.01 ms/step | loss 314863.938 \n",
      "| iter   1730 | 27.01 ms/step | loss 38694.016 \n",
      "| iter   1731 | 25.01 ms/step | loss 354655.812 \n",
      "| iter   1732 | 28.00 ms/step | loss 13587.240 \n",
      "| iter   1733 | 27.01 ms/step | loss 163907.234 \n",
      "| iter   1734 | 111.98 ms/step | loss 340897.188 \n",
      "| iter   1735 | 23.00 ms/step | loss 34629.363 \n",
      "| iter   1736 | 28.01 ms/step | loss 61812.328 \n",
      "| iter   1737 | 23.99 ms/step | loss 28340.463 \n",
      "| iter   1738 | 24.01 ms/step | loss 34681.773 \n",
      "| iter   1739 | 24.00 ms/step | loss 32025672.000 \n",
      "| iter   1740 | 27.00 ms/step | loss 34921.812 \n",
      "| iter   1741 | 22.01 ms/step | loss 21966.900 \n",
      "| iter   1742 | 26.99 ms/step | loss 374181.750 \n",
      "| iter   1743 | 27.02 ms/step | loss 51583.918 \n",
      "| iter   1744 | 107.01 ms/step | loss 14309.599 \n",
      "| iter   1745 | 24.00 ms/step | loss 14334.316 \n",
      "| iter   1746 | 27.02 ms/step | loss 382872.000 \n",
      "| iter   1747 | 25.01 ms/step | loss 57769.297 \n",
      "| iter   1748 | 27.99 ms/step | loss 30284.824 \n",
      "| iter   1749 | 21.99 ms/step | loss 8491.528 \n",
      "| iter   1750 | 25.01 ms/step | loss 16179.991 \n",
      "| iter   1751 | 23.01 ms/step | loss 26952.445 \n",
      "| iter   1752 | 109.03 ms/step | loss 71361.812 \n",
      "| iter   1753 | 27.01 ms/step | loss 593388.250 \n",
      "| iter   1754 | 27.00 ms/step | loss 16093.822 \n",
      "| iter   1755 | 27.02 ms/step | loss 678955.562 \n",
      "| iter   1756 | 22.01 ms/step | loss 57634.359 \n",
      "| iter   1757 | 23.01 ms/step | loss 25784.125 \n",
      "| iter   1758 | 30.01 ms/step | loss 429870.844 \n",
      "| iter   1759 | 26.01 ms/step | loss 25852.910 \n",
      "| iter   1760 | 26.01 ms/step | loss 63661.273 \n",
      "| iter   1761 | 114.03 ms/step | loss 12136.263 \n",
      "| iter   1762 | 26.01 ms/step | loss 179684.750 \n",
      "| iter   1763 | 23.02 ms/step | loss 25889.674 \n",
      "| iter   1764 | 26.01 ms/step | loss 441416.906 \n",
      "| iter   1765 | 27.99 ms/step | loss 2732292.250 \n",
      "| iter   1766 | 38.01 ms/step | loss 16303.396 \n",
      "| iter   1767 | 30.00 ms/step | loss 405083.406 \n",
      "| iter   1768 | 33.01 ms/step | loss 78685.055 \n",
      "| iter   1769 | 38.01 ms/step | loss 5949.480 \n",
      "| iter   1770 | 110.03 ms/step | loss 39748.922 \n",
      "| iter   1771 | 24.01 ms/step | loss 31548.039 \n",
      "| iter   1772 | 25.01 ms/step | loss 433225.000 \n",
      "| iter   1773 | 26.01 ms/step | loss 9381.873 \n",
      "| iter   1774 | 25.01 ms/step | loss 14092.922 \n",
      "| iter   1775 | 25.03 ms/step | loss 197535.766 \n",
      "| iter   1776 | 25.00 ms/step | loss 393557.656 \n",
      "| iter   1777 | 23.00 ms/step | loss 5990.220 \n",
      "| iter   1778 | 26.01 ms/step | loss 779051.875 \n",
      "| iter   1779 | 25.01 ms/step | loss 14228.094 \n",
      "| iter   1780 | 104.02 ms/step | loss 11766.707 \n",
      "| iter   1781 | 21.00 ms/step | loss 203999.062 \n",
      "| iter   1782 | 23.01 ms/step | loss 34382.879 \n",
      "| iter   1783 | 22.01 ms/step | loss 428659.438 \n",
      "| iter   1784 | 20.01 ms/step | loss 52552.785 \n",
      "| iter   1785 | 24.01 ms/step | loss 16348.268 \n",
      "| iter   1786 | 27.01 ms/step | loss 63383.238 \n",
      "| iter   1787 | 27.01 ms/step | loss 9660.089 \n",
      "| iter   1788 | 29.99 ms/step | loss 992427.188 \n",
      "| iter   1789 | 23.01 ms/step | loss 14893.247 \n",
      "| iter   1790 | 106.03 ms/step | loss 968241.500 \n",
      "| iter   1791 | 23.00 ms/step | loss 18975.281 \n",
      "| iter   1792 | 23.01 ms/step | loss 53139.008 \n",
      "| iter   1793 | 21.04 ms/step | loss 8014.923 \n",
      "| iter   1794 | 22.00 ms/step | loss 26925.785 \n",
      "| iter   1795 | 30.01 ms/step | loss 19888.416 \n",
      "| iter   1796 | 24.00 ms/step | loss 45284.914 \n",
      "| iter   1797 | 23.01 ms/step | loss 602037.125 \n",
      "| iter   1798 | 26.01 ms/step | loss 9379.598 \n",
      "| iter   1799 | 118.03 ms/step | loss 435069.250 \n",
      "| iter   1800 | 23.01 ms/step | loss 55890.098 \n",
      "| iter   1801 | 21.01 ms/step | loss 5832.914 \n",
      "| iter   1802 | 24.02 ms/step | loss 108096.469 \n",
      "| iter   1803 | 27.01 ms/step | loss 470462.031 \n",
      "| iter   1804 | 25.01 ms/step | loss 62502.734 \n",
      "| iter   1805 | 31.01 ms/step | loss 35297.594 \n",
      "| iter   1806 | 27.01 ms/step | loss 177970.031 \n",
      "| iter   1807 | 28.01 ms/step | loss 478823.750 \n",
      "| iter   1808 | 28.01 ms/step | loss 9719.545 \n",
      "| iter   1809 | 25.01 ms/step | loss 87188.328 \n",
      "| iter   1810 | 104.02 ms/step | loss 31870.982 \n",
      "| iter   1811 | 24.01 ms/step | loss 55077.312 \n",
      "| iter   1812 | 23.02 ms/step | loss 10043.568 \n",
      "| iter   1813 | 23.01 ms/step | loss 637338.938 \n",
      "| iter   1814 | 25.01 ms/step | loss 14677.197 \n",
      "| iter   1815 | 27.01 ms/step | loss 739097.875 \n",
      "| iter   1816 | 26.01 ms/step | loss 5489.212 \n",
      "| iter   1817 | 25.01 ms/step | loss 10842.271 \n",
      "| iter   1818 | 26.01 ms/step | loss 15096.557 \n",
      "| iter   1819 | 25.99 ms/step | loss 16693.754 \n",
      "| iter   1820 | 103.03 ms/step | loss 484002.312 \n",
      "| iter   1821 | 26.01 ms/step | loss 72641.781 \n",
      "| iter   1822 | 28.01 ms/step | loss 440422.219 \n",
      "| iter   1823 | 25.01 ms/step | loss 24531.188 \n",
      "| iter   1824 | 27.01 ms/step | loss 19554.973 \n",
      "| iter   1825 | 25.01 ms/step | loss 51977.320 \n",
      "| iter   1826 | 27.02 ms/step | loss 1287553.875 \n",
      "| iter   1827 | 25.01 ms/step | loss 62267.047 \n",
      "| iter   1828 | 112.01 ms/step | loss 648354.750 \n",
      "| iter   1829 | 22.00 ms/step | loss 28193.320 \n",
      "| iter   1830 | 27.00 ms/step | loss 8792.508 \n",
      "| iter   1831 | 24.01 ms/step | loss 510008.156 \n",
      "| iter   1832 | 24.01 ms/step | loss 132083.078 \n",
      "| iter   1833 | 23.01 ms/step | loss 26627.221 \n",
      "| iter   1834 | 26.01 ms/step | loss 187512.078 \n",
      "| iter   1835 | 26.01 ms/step | loss 380779.375 \n",
      "| iter   1836 | 23.01 ms/step | loss 16337.719 \n",
      "| iter   1837 | 111.01 ms/step | loss 6495.177 \n",
      "| iter   1838 | 26.01 ms/step | loss 66980.125 \n",
      "| iter   1839 | 25.01 ms/step | loss 121423.734 \n",
      "| iter   1840 | 24.01 ms/step | loss 54089.156 \n",
      "| iter   1841 | 23.99 ms/step | loss 683273.062 \n",
      "| iter   1842 | 27.01 ms/step | loss 397388.000 \n",
      "| iter   1843 | 22.01 ms/step | loss 17585.344 \n",
      "| iter   1844 | 24.01 ms/step | loss 44725.930 \n",
      "| iter   1845 | 27.01 ms/step | loss 1041195.188 \n",
      "| iter   1846 | 26.01 ms/step | loss 44954.031 \n",
      "| iter   1847 | 103.01 ms/step | loss 19129.994 \n",
      "| iter   1848 | 23.00 ms/step | loss 501029.844 \n",
      "| iter   1849 | 24.00 ms/step | loss 31100.273 \n",
      "| iter   1850 | 24.01 ms/step | loss 26142.336 \n",
      "| iter   1851 | 25.01 ms/step | loss 444714.938 \n",
      "| iter   1852 | 23.01 ms/step | loss 7578.123 \n",
      "| iter   1853 | 24.01 ms/step | loss 10448.663 \n",
      "| iter   1854 | 25.02 ms/step | loss 13045.625 \n",
      "| iter   1855 | 106.02 ms/step | loss 80360.719 \n",
      "| iter   1856 | 28.98 ms/step | loss 5258193.000 \n",
      "| iter   1857 | 24.00 ms/step | loss 412547.375 \n",
      "| iter   1858 | 25.00 ms/step | loss 97198.438 \n",
      "| iter   1859 | 22.01 ms/step | loss 419249.188 \n",
      "| iter   1860 | 24.02 ms/step | loss 11883.320 \n",
      "| iter   1861 | 25.00 ms/step | loss 9765.402 \n",
      "| iter   1862 | 25.00 ms/step | loss 412828.156 \n",
      "| iter   1863 | 24.01 ms/step | loss 37264.562 \n",
      "| iter   1864 | 111.01 ms/step | loss 429171.531 \n",
      "| iter   1865 | 29.01 ms/step | loss 60826.719 \n",
      "| iter   1866 | 25.00 ms/step | loss 4230991.500 \n",
      "| iter   1867 | 22.01 ms/step | loss 66615.750 \n",
      "| iter   1868 | 24.01 ms/step | loss 386893.094 \n",
      "| iter   1869 | 22.01 ms/step | loss 113051.062 \n",
      "| iter   1870 | 27.01 ms/step | loss 404801.875 \n",
      "| iter   1871 | 28.01 ms/step | loss 54960.734 \n",
      "| iter   1872 | 26.01 ms/step | loss 22825.045 \n",
      "| iter   1873 | 107.04 ms/step | loss 15374.095 \n",
      "| iter   1874 | 26.01 ms/step | loss 11928340.000 \n",
      "| iter   1875 | 23.00 ms/step | loss 17781.592 \n",
      "| iter   1876 | 28.01 ms/step | loss 12336.117 \n",
      "| iter   1877 | 22.00 ms/step | loss 24711.832 \n",
      "| iter   1878 | 24.01 ms/step | loss 12259.280 \n",
      "| iter   1879 | 26.01 ms/step | loss 513264.875 \n",
      "| iter   1880 | 25.01 ms/step | loss 61998.902 \n",
      "| iter   1881 | 23.00 ms/step | loss 43568.668 \n",
      "| iter   1882 | 28.01 ms/step | loss 412134.656 \n",
      "| iter   1883 | 117.07 ms/step | loss 15404.475 \n",
      "| iter   1884 | 22.01 ms/step | loss 14170.077 \n",
      "| iter   1885 | 23.00 ms/step | loss 85876.164 \n",
      "| iter   1886 | 26.01 ms/step | loss 24296.508 \n",
      "| iter   1887 | 26.01 ms/step | loss 62935.574 \n",
      "| iter   1888 | 27.01 ms/step | loss 541145.562 \n",
      "| iter   1889 | 23.01 ms/step | loss 4919.663 \n",
      "| iter   1890 | 26.01 ms/step | loss 23030.725 \n",
      "| iter   1891 | 112.03 ms/step | loss 14197.529 \n",
      "| iter   1892 | 24.99 ms/step | loss 43540.070 \n",
      "| iter   1893 | 23.01 ms/step | loss 465763.656 \n",
      "| iter   1894 | 24.02 ms/step | loss 12387.520 \n",
      "| iter   1895 | 22.01 ms/step | loss 53492.254 \n",
      "| iter   1896 | 22.01 ms/step | loss 448118.156 \n",
      "| iter   1897 | 27.01 ms/step | loss 69508.773 \n",
      "| iter   1898 | 25.00 ms/step | loss 19634.004 \n",
      "| iter   1899 | 24.00 ms/step | loss 13948.110 \n",
      "| iter   1900 | 26.01 ms/step | loss 429562.000 \n",
      "| iter   1901 | 25.01 ms/step | loss 25006.312 \n",
      "| iter   1902 | 114.03 ms/step | loss 381760.312 \n",
      "| iter   1903 | 26.01 ms/step | loss 1871839.625 \n",
      "| iter   1904 | 27.01 ms/step | loss 8180.462 \n",
      "| iter   1905 | 27.01 ms/step | loss 43673.785 \n",
      "| iter   1906 | 25.02 ms/step | loss 7102.369 \n",
      "| iter   1907 | 27.01 ms/step | loss 17142.607 \n",
      "| iter   1908 | 26.99 ms/step | loss 18837.354 \n",
      "| iter   1909 | 22.01 ms/step | loss 529929.188 \n",
      "| iter   1910 | 27.01 ms/step | loss 406182.281 \n",
      "| iter   1911 | 27.01 ms/step | loss 39697.883 \n",
      "| iter   1912 | 106.04 ms/step | loss 10521.350 \n",
      "| iter   1913 | 24.00 ms/step | loss 9001.956 \n",
      "| iter   1914 | 28.01 ms/step | loss 59852.078 \n",
      "| iter   1915 | 23.01 ms/step | loss 482240.125 \n",
      "| iter   1916 | 25.00 ms/step | loss 16262.857 \n",
      "| iter   1917 | 24.01 ms/step | loss 18469.965 \n",
      "| iter   1918 | 26.01 ms/step | loss 453496.906 \n",
      "| iter   1919 | 27.01 ms/step | loss 39181.500 \n",
      "| iter   1920 | 141.41 ms/step | loss 30502.916 \n",
      "| iter   1921 | 24.02 ms/step | loss 23133.203 \n",
      "| iter   1922 | 27.00 ms/step | loss 11355.795 \n",
      "| iter   1923 | 23.00 ms/step | loss 18802.498 \n",
      "| iter   1924 | 26.01 ms/step | loss 472735.656 \n",
      "| iter   1925 | 23.01 ms/step | loss 118449.008 \n",
      "| iter   1926 | 26.01 ms/step | loss 27016.223 \n",
      "| iter   1927 | 26.00 ms/step | loss 411577.719 \n",
      "| iter   1928 | 25.01 ms/step | loss 10096.519 \n",
      "| iter   1929 | 111.02 ms/step | loss 54179.219 \n",
      "| iter   1930 | 26.01 ms/step | loss 6563.422 \n",
      "| iter   1931 | 27.01 ms/step | loss 25910.895 \n",
      "| iter   1932 | 25.01 ms/step | loss 68889.719 \n",
      "| iter   1933 | 24.74 ms/step | loss 409678.781 \n",
      "| iter   1934 | 25.01 ms/step | loss 17715.074 \n",
      "| iter   1935 | 25.00 ms/step | loss 16693.602 \n",
      "| iter   1936 | 26.01 ms/step | loss 383083.000 \n",
      "| iter   1937 | 24.01 ms/step | loss 209410.234 \n",
      "| iter   1938 | 109.04 ms/step | loss 433474.062 \n",
      "| iter   1939 | 26.02 ms/step | loss 53277.848 \n",
      "| iter   1940 | 24.99 ms/step | loss 13945.834 \n",
      "| iter   1941 | 20.01 ms/step | loss 55269.344 \n",
      "| iter   1942 | 25.00 ms/step | loss 386963.344 \n",
      "| iter   1943 | 23.18 ms/step | loss 8173.977 \n",
      "| iter   1944 | 24.02 ms/step | loss 31818.719 \n",
      "| iter   1945 | 26.01 ms/step | loss 76160.883 \n",
      "| iter   1946 | 26.99 ms/step | loss 32844.188 \n",
      "| iter   1947 | 24.01 ms/step | loss 119082.141 \n",
      "| iter   1948 | 108.03 ms/step | loss 50016.777 \n",
      "| iter   1949 | 21.01 ms/step | loss 428632.625 \n",
      "| iter   1950 | 33.01 ms/step | loss 39669.426 \n",
      "| iter   1951 | 27.04 ms/step | loss 2452842.000 \n",
      "| iter   1952 | 22.00 ms/step | loss 387969.750 \n",
      "| iter   1953 | 21.01 ms/step | loss 9544.146 \n",
      "| iter   1954 | 27.01 ms/step | loss 11599.527 \n",
      "| iter   1955 | 25.01 ms/step | loss 380508.438 \n",
      "| iter   1956 | 110.03 ms/step | loss 78313.078 \n",
      "| iter   1957 | 25.02 ms/step | loss 504161.312 \n",
      "| iter   1958 | 26.00 ms/step | loss 14187.570 \n",
      "| iter   1959 | 26.01 ms/step | loss 65953.688 \n",
      "| iter   1960 | 27.01 ms/step | loss 403997.688 \n",
      "| iter   1961 | 22.01 ms/step | loss 6643.041 \n",
      "| iter   1962 | 25.02 ms/step | loss 38543.363 \n",
      "| iter   1963 | 24.01 ms/step | loss 88519.281 \n",
      "| iter   1964 | 24.01 ms/step | loss 507159.875 \n",
      "| iter   1965 | 106.03 ms/step | loss 471486.875 \n",
      "| iter   1966 | 27.01 ms/step | loss 17700.361 \n",
      "| iter   1967 | 25.06 ms/step | loss 378728.781 \n",
      "| iter   1968 | 27.99 ms/step | loss 12575.177 \n",
      "| iter   1969 | 25.01 ms/step | loss 41095.809 \n",
      "| iter   1970 | 23.01 ms/step | loss 152092.125 \n",
      "| iter   1971 | 26.01 ms/step | loss 815830.125 \n",
      "| iter   1972 | 25.01 ms/step | loss 62812.117 \n",
      "| iter   1973 | 25.01 ms/step | loss 17174.008 \n",
      "| iter   1974 | 107.04 ms/step | loss 387660.281 \n",
      "| iter   1975 | 26.99 ms/step | loss 18478.018 \n",
      "| iter   1976 | 25.01 ms/step | loss 20664.656 \n",
      "| iter   1977 | 21.00 ms/step | loss 9201.097 \n",
      "| iter   1978 | 30.09 ms/step | loss 50863.750 \n",
      "| iter   1979 | 32.01 ms/step | loss 25318.855 \n",
      "| iter   1980 | 37.00 ms/step | loss 7119.501 \n",
      "| iter   1981 | 32.01 ms/step | loss 404162.031 \n",
      "| iter   1982 | 26.02 ms/step | loss 8970.471 \n",
      "| iter   1983 | 104.03 ms/step | loss 366830.469 \n",
      "| iter   1984 | 26.01 ms/step | loss 48242.098 \n",
      "| iter   1985 | 25.01 ms/step | loss 20592.436 \n",
      "| iter   1986 | 27.00 ms/step | loss 14230.410 \n",
      "| iter   1987 | 23.01 ms/step | loss 327548.438 \n",
      "| iter   1988 | 28.01 ms/step | loss 7862.130 \n",
      "| iter   1989 | 23.01 ms/step | loss 57220.508 \n",
      "| iter   1990 | 26.01 ms/step | loss 14344.823 \n",
      "| iter   1991 | 26.02 ms/step | loss 56126.562 \n",
      "| iter   1992 | 25.01 ms/step | loss 77411.125 \n",
      "| iter   1993 | 20.00 ms/step | loss 360233.094 \n",
      "| iter   1994 | 105.02 ms/step | loss 12685.889 \n",
      "| iter   1995 | 26.06 ms/step | loss 381896.438 \n",
      "| iter   1996 | 25.00 ms/step | loss 23596.000 \n",
      "| iter   1997 | 25.01 ms/step | loss 5288.191 \n",
      "| iter   1998 | 25.01 ms/step | loss 313836.750 \n",
      "| iter   1999 | 25.01 ms/step | loss 10167.792 \n",
      "| iter   2000 | 26.01 ms/step | loss 86981.797 \n",
      "| iter   2001 | 24.01 ms/step | loss 37703.258 \n",
      "| iter   2002 | 24.02 ms/step | loss 371486.688 \n",
      "| iter   2003 | 104.02 ms/step | loss 15362.423 \n",
      "| iter   2004 | 23.01 ms/step | loss 51450.461 \n",
      "| iter   2005 | 23.00 ms/step | loss 8843.229 \n",
      "| iter   2006 | 27.01 ms/step | loss 596025.000 \n",
      "| iter   2007 | 27.01 ms/step | loss 23671.506 \n",
      "| iter   2008 | 24.01 ms/step | loss 50251.938 \n",
      "| iter   2009 | 25.01 ms/step | loss 14076.936 \n",
      "| iter   2010 | 25.01 ms/step | loss 10775.286 \n",
      "| iter   2011 | 27.04 ms/step | loss 45868.352 \n",
      "| iter   2012 | 110.03 ms/step | loss 1922877.125 \n",
      "| iter   2013 | 27.01 ms/step | loss 408629.562 \n",
      "| iter   2014 | 27.01 ms/step | loss 18961.371 \n",
      "| iter   2015 | 24.99 ms/step | loss 250219.531 \n",
      "| iter   2016 | 25.01 ms/step | loss 48368.520 \n",
      "| iter   2017 | 23.01 ms/step | loss 40266.676 \n",
      "| iter   2018 | 27.01 ms/step | loss 13964.428 \n",
      "| iter   2019 | 23.07 ms/step | loss 236875.500 \n",
      "| iter   2020 | 25.01 ms/step | loss 31095.795 \n",
      "| iter   2021 | 105.02 ms/step | loss 37634.375 \n",
      "| iter   2022 | 30.01 ms/step | loss 36099.344 \n",
      "| iter   2023 | 25.01 ms/step | loss 11297.893 \n",
      "| iter   2024 | 22.01 ms/step | loss 9370.133 \n",
      "| iter   2025 | 21.00 ms/step | loss 302529.969 \n",
      "| iter   2026 | 27.02 ms/step | loss 33937.555 \n",
      "| iter   2027 | 23.01 ms/step | loss 14763.202 \n",
      "| iter   2028 | 23.01 ms/step | loss 327679.594 \n",
      "| iter   2029 | 26.01 ms/step | loss 22344.518 \n",
      "| iter   2030 | 109.02 ms/step | loss 11213.330 \n",
      "| iter   2031 | 24.00 ms/step | loss 20260.963 \n",
      "| iter   2032 | 25.01 ms/step | loss 231551.312 \n",
      "| iter   2033 | 23.02 ms/step | loss 49605.191 \n",
      "| iter   2034 | 29.01 ms/step | loss 15098.544 \n",
      "| iter   2035 | 22.00 ms/step | loss 34608.762 \n",
      "| iter   2036 | 25.01 ms/step | loss 260033.219 \n",
      "| iter   2037 | 25.01 ms/step | loss 24796.992 \n",
      "| iter   2038 | 23.01 ms/step | loss 13101.956 \n",
      "| iter   2039 | 25.01 ms/step | loss 256415.375 \n",
      "| iter   2040 | 104.02 ms/step | loss 2614974.500 \n",
      "| iter   2041 | 24.01 ms/step | loss 21518.816 \n",
      "| iter   2042 | 28.01 ms/step | loss 232044.438 \n",
      "| iter   2043 | 24.01 ms/step | loss 437204.594 \n",
      "| iter   2044 | 26.01 ms/step | loss 14616.272 \n",
      "| iter   2045 | 28.99 ms/step | loss 203772.016 \n",
      "| iter   2046 | 24.01 ms/step | loss 13130.547 \n",
      "| iter   2047 | 21.00 ms/step | loss 8544.152 \n",
      "| iter   2048 | 25.01 ms/step | loss 262233.781 \n",
      "| iter   2049 | 22.01 ms/step | loss 52207.848 \n",
      "| iter   2050 | 104.03 ms/step | loss 320040.562 \n",
      "| iter   2051 | 27.01 ms/step | loss 48535.000 \n",
      "| iter   2052 | 26.01 ms/step | loss 271394.375 \n",
      "| iter   2053 | 23.05 ms/step | loss 8373.821 \n",
      "| iter   2054 | 25.01 ms/step | loss 194304.984 \n",
      "| iter   2055 | 26.01 ms/step | loss 2364713.000 \n",
      "| iter   2056 | 28.01 ms/step | loss 7585.954 \n",
      "| iter   2057 | 25.01 ms/step | loss 4798.522 \n",
      "| iter   2058 | 25.00 ms/step | loss 28609.143 \n",
      "| iter   2059 | 103.02 ms/step | loss 169964.875 \n",
      "| iter   2060 | 27.01 ms/step | loss 37338.656 \n",
      "| iter   2061 | 23.01 ms/step | loss 18097.650 \n",
      "| iter   2062 | 27.01 ms/step | loss 62108.398 \n",
      "| iter   2063 | 23.02 ms/step | loss 21052.035 \n",
      "| iter   2064 | 27.01 ms/step | loss 244310.094 \n",
      "| iter   2065 | 21.02 ms/step | loss 8705.567 \n",
      "| iter   2066 | 23.01 ms/step | loss 6435646.000 \n",
      "| iter   2067 | 26.01 ms/step | loss 14413.845 \n",
      "| iter   2068 | 27.01 ms/step | loss 29315.334 \n",
      "| iter   2069 | 24.01 ms/step | loss 69443.695 \n",
      "| iter   2070 | 103.02 ms/step | loss 32867.324 \n",
      "| iter   2071 | 24.01 ms/step | loss 9859.840 \n",
      "| iter   2072 | 24.01 ms/step | loss 11060.836 \n",
      "| iter   2073 | 24.01 ms/step | loss 361034.344 \n",
      "| iter   2074 | 26.01 ms/step | loss 46449.816 \n",
      "| iter   2075 | 25.01 ms/step | loss 8455.180 \n",
      "| iter   2076 | 25.06 ms/step | loss 32369.301 \n",
      "| iter   2077 | 25.01 ms/step | loss 253118.344 \n",
      "| iter   2078 | 28.00 ms/step | loss 31779.779 \n",
      "| iter   2079 | 25.01 ms/step | loss 500308.188 \n",
      "| iter   2080 | 108.03 ms/step | loss 206826.656 \n",
      "| iter   2081 | 25.01 ms/step | loss 40556.918 \n",
      "| iter   2082 | 26.01 ms/step | loss 72734.438 \n",
      "| iter   2083 | 25.62 ms/step | loss 82783.445 \n",
      "| iter   2084 | 31.01 ms/step | loss 30796.016 \n",
      "| iter   2085 | 22.00 ms/step | loss 42585.457 \n",
      "| iter   2086 | 25.01 ms/step | loss 18337.830 \n",
      "| iter   2087 | 27.01 ms/step | loss 267800.844 \n",
      "| iter   2088 | 111.05 ms/step | loss 339906.844 \n",
      "| iter   2089 | 25.01 ms/step | loss 29551.820 \n",
      "| iter   2090 | 26.75 ms/step | loss 22990.785 \n",
      "| iter   2091 | 27.01 ms/step | loss 16761.207 \n",
      "| iter   2092 | 25.01 ms/step | loss 38120.117 \n",
      "| iter   2093 | 24.01 ms/step | loss 5907761.000 \n",
      "| iter   2094 | 27.01 ms/step | loss 54775.680 \n",
      "| iter   2095 | 23.01 ms/step | loss 25971.938 \n",
      "| iter   2096 | 25.01 ms/step | loss 362036.469 \n",
      "| iter   2097 | 104.09 ms/step | loss 37033.820 \n",
      "| iter   2098 | 27.01 ms/step | loss 1675926.375 \n",
      "| iter   2099 | 23.01 ms/step | loss 13541.744 \n",
      "| iter   2100 | 24.01 ms/step | loss 162804.750 \n",
      "| iter   2101 | 25.01 ms/step | loss 35435.039 \n",
      "| iter   2102 | 29.99 ms/step | loss 24997.059 \n",
      "| iter   2103 | 28.00 ms/step | loss 33069.852 \n",
      "| iter   2104 | 27.01 ms/step | loss 8620.308 \n",
      "| iter   2105 | 23.99 ms/step | loss 157846.766 \n",
      "| iter   2106 | 24.01 ms/step | loss 292660.656 \n",
      "| iter   2107 | 97.02 ms/step | loss 12317.291 \n",
      "| iter   2108 | 26.00 ms/step | loss 89657.680 \n",
      "| iter   2109 | 24.01 ms/step | loss 88424.859 \n",
      "| iter   2110 | 25.00 ms/step | loss 17318.447 \n",
      "| iter   2111 | 26.01 ms/step | loss 17158.982 \n",
      "| iter   2112 | 21.01 ms/step | loss 144095.562 \n",
      "| iter   2113 | 24.03 ms/step | loss 56300.559 \n",
      "| iter   2114 | 25.02 ms/step | loss 42441.227 \n",
      "| iter   2115 | 107.03 ms/step | loss 78588.297 \n",
      "| iter   2116 | 26.01 ms/step | loss 688498.562 \n",
      "| iter   2117 | 25.01 ms/step | loss 16467.387 \n",
      "| iter   2118 | 26.01 ms/step | loss 43093.945 \n",
      "| iter   2119 | 23.02 ms/step | loss 11360.133 \n",
      "| iter   2120 | 25.00 ms/step | loss 208893.344 \n",
      "| iter   2121 | 26.01 ms/step | loss 6876.083 \n",
      "| iter   2122 | 28.01 ms/step | loss 133284.562 \n",
      "| iter   2123 | 23.00 ms/step | loss 1054611.625 \n",
      "| iter   2124 | 109.03 ms/step | loss 62064.684 \n",
      "| iter   2125 | 24.00 ms/step | loss 92802.812 \n",
      "| iter   2126 | 28.03 ms/step | loss 1343637.125 \n",
      "| iter   2127 | 25.01 ms/step | loss 167125.359 \n",
      "| iter   2128 | 25.00 ms/step | loss 54581.762 \n",
      "| iter   2129 | 24.01 ms/step | loss 37576.125 \n",
      "| iter   2130 | 26.01 ms/step | loss 22420.385 \n",
      "| iter   2131 | 26.01 ms/step | loss 40147.195 \n",
      "| iter   2132 | 26.01 ms/step | loss 15112.256 \n",
      "| iter   2133 | 100.02 ms/step | loss 133626.562 \n",
      "| iter   2134 | 27.01 ms/step | loss 120770.375 \n",
      "| iter   2135 | 27.01 ms/step | loss 292442.125 \n",
      "| iter   2136 | 27.98 ms/step | loss 23468.934 \n",
      "| iter   2137 | 24.00 ms/step | loss 6132.548 \n",
      "| iter   2138 | 23.01 ms/step | loss 13139.736 \n",
      "| iter   2139 | 22.01 ms/step | loss 306127.938 \n",
      "| iter   2140 | 24.02 ms/step | loss 138976.906 \n",
      "| iter   2141 | 25.01 ms/step | loss 71073.008 \n",
      "| iter   2142 | 27.01 ms/step | loss 281182.625 \n",
      "| iter   2143 | 106.02 ms/step | loss 870928.438 \n",
      "| iter   2144 | 24.01 ms/step | loss 280526.812 \n",
      "| iter   2145 | 23.01 ms/step | loss 402686.062 \n",
      "| iter   2146 | 27.01 ms/step | loss 154002.797 \n",
      "| iter   2147 | 21.00 ms/step | loss 12578.442 \n",
      "| iter   2148 | 26.01 ms/step | loss 118881.633 \n",
      "| iter   2149 | 25.01 ms/step | loss 134872.109 \n",
      "| iter   2150 | 27.01 ms/step | loss 11217.576 \n",
      "| iter   2151 | 112.01 ms/step | loss 27446.145 \n",
      "| iter   2152 | 26.00 ms/step | loss 95961.984 \n",
      "| iter   2153 | 21.00 ms/step | loss 45046.031 \n",
      "| iter   2154 | 22.01 ms/step | loss 10236.020 \n",
      "| iter   2155 | 25.02 ms/step | loss 36120.320 \n",
      "| iter   2156 | 22.01 ms/step | loss 41873.180 \n",
      "| iter   2157 | 24.01 ms/step | loss 39029.008 \n",
      "| iter   2158 | 26.01 ms/step | loss 13964.753 \n",
      "| iter   2159 | 24.01 ms/step | loss 387384.938 \n",
      "| iter   2160 | 26.01 ms/step | loss 14509.486 \n",
      "| iter   2161 | 24.00 ms/step | loss 426481.938 \n",
      "| iter   2162 | 114.03 ms/step | loss 9719.752 \n",
      "| iter   2163 | 27.00 ms/step | loss 107228.312 \n",
      "| iter   2164 | 30.01 ms/step | loss 14932776.000 \n",
      "| iter   2165 | 22.00 ms/step | loss 14204.914 \n",
      "| iter   2166 | 23.01 ms/step | loss 358295072.000 \n",
      "| iter   2167 | 24.01 ms/step | loss 44638.320 \n",
      "| iter   2168 | 25.00 ms/step | loss 41876.938 \n",
      "| iter   2169 | 22.01 ms/step | loss 23164.309 \n",
      "| iter   2170 | 23.00 ms/step | loss 41792.559 \n",
      "| iter   2171 | 23.99 ms/step | loss 282572.656 \n",
      "| iter   2172 | 105.03 ms/step | loss 15480.212 \n",
      "| iter   2173 | 21.01 ms/step | loss 3200540.750 \n",
      "| iter   2174 | 24.01 ms/step | loss 9349.155 \n",
      "| iter   2175 | 21.02 ms/step | loss 16274.723 \n",
      "| iter   2176 | 25.01 ms/step | loss 1612955.625 \n",
      "| iter   2177 | 22.00 ms/step | loss 280736.688 \n",
      "| iter   2178 | 27.01 ms/step | loss 51547.617 \n",
      "| iter   2179 | 24.01 ms/step | loss 20770.598 \n",
      "| iter   2180 | 25.00 ms/step | loss 279451.750 \n",
      "| iter   2181 | 100.02 ms/step | loss 47937.512 \n",
      "| iter   2182 | 23.01 ms/step | loss 918934.188 \n",
      "| iter   2183 | 24.03 ms/step | loss 40162.184 \n",
      "| iter   2184 | 25.02 ms/step | loss 12309.527 \n",
      "| iter   2185 | 22.00 ms/step | loss 920881.500 \n",
      "| iter   2186 | 23.00 ms/step | loss 2084584.875 \n",
      "| iter   2187 | 21.01 ms/step | loss 597494.625 \n",
      "| iter   2188 | 23.01 ms/step | loss 238994.281 \n",
      "| iter   2189 | 100.02 ms/step | loss 1275451.250 \n",
      "| iter   2190 | 27.00 ms/step | loss 258219.281 \n",
      "| iter   2191 | 22.00 ms/step | loss 375927.031 \n",
      "| iter   2192 | 25.02 ms/step | loss 23860.043 \n",
      "| iter   2193 | 24.01 ms/step | loss 29700.270 \n",
      "| iter   2194 | 27.01 ms/step | loss 3590284.250 \n",
      "| iter   2195 | 28.01 ms/step | loss 328314.875 \n",
      "| iter   2196 | 27.01 ms/step | loss 12383.769 \n",
      "| iter   2197 | 22.01 ms/step | loss 37505.137 \n",
      "| iter   2198 | 107.03 ms/step | loss 338439.500 \n",
      "| iter   2199 | 23.01 ms/step | loss 30787.479 \n",
      "| iter   2200 | 65.01 ms/step | loss 26732.525 \n",
      "| iter   2201 | 26.00 ms/step | loss 267424.875 \n",
      "| iter   2202 | 23.01 ms/step | loss 359855.781 \n",
      "| iter   2203 | 23.02 ms/step | loss 44671.141 \n",
      "| iter   2204 | 27.99 ms/step | loss 117929.250 \n",
      "| iter   2205 | 21.01 ms/step | loss 24684.453 \n",
      "| iter   2206 | 23.01 ms/step | loss 3100915.250 \n",
      "| iter   2207 | 21.01 ms/step | loss 113175.281 \n",
      "| iter   2208 | 105.03 ms/step | loss 24104.965 \n",
      "| iter   2209 | 22.01 ms/step | loss 576488.500 \n",
      "| iter   2210 | 27.01 ms/step | loss 54066.031 \n",
      "| iter   2211 | 24.01 ms/step | loss 349996.594 \n",
      "| iter   2212 | 21.00 ms/step | loss 62553.031 \n",
      "| iter   2213 | 22.01 ms/step | loss 217963.484 \n",
      "| iter   2214 | 24.01 ms/step | loss 726209.000 \n",
      "| iter   2215 | 26.01 ms/step | loss 743490.938 \n",
      "| iter   2216 | 26.01 ms/step | loss 99292.266 \n",
      "| iter   2217 | 24.01 ms/step | loss 37368.207 \n",
      "| iter   2218 | 108.02 ms/step | loss 60828.164 \n",
      "| iter   2219 | 29.99 ms/step | loss 28960.250 \n",
      "| iter   2220 | 31.01 ms/step | loss 509509.688 \n",
      "| iter   2221 | 21.00 ms/step | loss 27838.900 \n",
      "| iter   2222 | 24.01 ms/step | loss 101540.141 \n",
      "| iter   2223 | 21.02 ms/step | loss 47862.922 \n",
      "| iter   2224 | 25.03 ms/step | loss 381720.219 \n",
      "| iter   2225 | 25.07 ms/step | loss 14609.282 \n",
      "| iter   2226 | 28.02 ms/step | loss 121594.266 \n",
      "| iter   2227 | 102.01 ms/step | loss 318012.250 \n",
      "| iter   2228 | 27.00 ms/step | loss 76412.320 \n",
      "| iter   2229 | 22.99 ms/step | loss 35459.184 \n",
      "| iter   2230 | 25.01 ms/step | loss 275314.625 \n",
      "| iter   2231 | 26.15 ms/step | loss 293102.094 \n",
      "| iter   2232 | 27.03 ms/step | loss 68957.547 \n",
      "| iter   2233 | 22.01 ms/step | loss 82787.992 \n",
      "| iter   2234 | 23.99 ms/step | loss 25167.381 \n",
      "| iter   2235 | 24.01 ms/step | loss 2038298.250 \n",
      "| iter   2236 | 21.00 ms/step | loss 46034.184 \n",
      "| iter   2237 | 22.01 ms/step | loss 312395.375 \n",
      "| iter   2238 | 105.04 ms/step | loss 50338.430 \n",
      "| iter   2239 | 26.01 ms/step | loss 25626.693 \n",
      "| iter   2240 | 25.01 ms/step | loss 56537.742 \n",
      "| iter   2241 | 26.02 ms/step | loss 299752.656 \n",
      "| iter   2242 | 25.01 ms/step | loss 34901.281 \n",
      "| iter   2243 | 23.01 ms/step | loss 22298.750 \n",
      "| iter   2244 | 23.01 ms/step | loss 23463.520 \n",
      "| iter   2245 | 20.00 ms/step | loss 322838.938 \n",
      "| iter   2246 | 23.01 ms/step | loss 48263.586 \n",
      "| iter   2247 | 27.01 ms/step | loss 75891.820 \n",
      "| iter   2248 | 101.02 ms/step | loss 666173.812 \n",
      "| iter   2249 | 25.01 ms/step | loss 1154646.750 \n",
      "| iter   2250 | 27.08 ms/step | loss 2970778.250 \n",
      "| iter   2251 | 23.01 ms/step | loss 204398.438 \n",
      "| iter   2252 | 22.00 ms/step | loss 254678.906 \n",
      "| iter   2253 | 23.00 ms/step | loss 63145.707 \n",
      "| iter   2254 | 26.01 ms/step | loss 47001.574 \n",
      "| iter   2255 | 22.00 ms/step | loss 37359.703 \n",
      "| iter   2256 | 26.01 ms/step | loss 42974.168 \n",
      "| iter   2257 | 98.02 ms/step | loss 238409.719 \n",
      "| iter   2258 | 27.01 ms/step | loss 516099.875 \n",
      "| iter   2259 | 24.01 ms/step | loss 115030.586 \n",
      "| iter   2260 | 25.01 ms/step | loss 34810.855 \n",
      "| iter   2261 | 22.00 ms/step | loss 109613.578 \n",
      "| iter   2262 | 26.06 ms/step | loss 152882608.000 \n",
      "| iter   2263 | 22.01 ms/step | loss 13158733.000 \n",
      "| iter   2264 | 23.01 ms/step | loss 55808.574 \n",
      "| iter   2265 | 102.02 ms/step | loss 156130.891 \n",
      "| iter   2266 | 25.00 ms/step | loss 132710.125 \n",
      "| iter   2267 | 25.01 ms/step | loss 102937.969 \n",
      "| iter   2268 | 23.01 ms/step | loss 78460.539 \n",
      "| iter   2269 | 20.00 ms/step | loss 31599.029 \n",
      "| iter   2270 | 23.01 ms/step | loss 606675.250 \n",
      "| iter   2271 | 26.01 ms/step | loss 28630.594 \n",
      "| iter   2272 | 25.01 ms/step | loss 46722.395 \n",
      "| iter   2273 | 24.01 ms/step | loss 21840.246 \n",
      "| iter   2274 | 104.02 ms/step | loss 12033.376 \n",
      "| iter   2275 | 25.01 ms/step | loss 23071.002 \n",
      "| iter   2276 | 27.00 ms/step | loss 120945.023 \n",
      "| iter   2277 | 27.01 ms/step | loss 6880.239 \n",
      "| iter   2278 | 25.01 ms/step | loss 8829.879 \n",
      "| iter   2279 | 25.01 ms/step | loss 224162.156 \n",
      "| iter   2280 | 24.01 ms/step | loss 29184.428 \n",
      "| iter   2281 | 24.01 ms/step | loss 37367.703 \n",
      "| iter   2282 | 25.01 ms/step | loss 231273.328 \n",
      "| iter   2283 | 25.01 ms/step | loss 17610.035 \n",
      "| iter   2284 | 111.02 ms/step | loss 27003.350 \n",
      "| iter   2285 | 22.01 ms/step | loss 27236.672 \n",
      "| iter   2286 | 26.00 ms/step | loss 103909.797 \n",
      "| iter   2287 | 25.01 ms/step | loss 22183.652 \n",
      "| iter   2288 | 25.01 ms/step | loss 7125.056 \n",
      "| iter   2289 | 23.02 ms/step | loss 124762.789 \n",
      "| iter   2290 | 23.01 ms/step | loss 69702.797 \n",
      "| iter   2291 | 25.01 ms/step | loss 77451.367 \n",
      "| iter   2292 | 23.06 ms/step | loss 12452.684 \n",
      "| iter   2293 | 22.01 ms/step | loss 18122.549 \n",
      "| iter   2294 | 110.04 ms/step | loss 106168.742 \n",
      "| iter   2295 | 26.01 ms/step | loss 33988.223 \n",
      "| iter   2296 | 25.01 ms/step | loss 25188.211 \n",
      "| iter   2297 | 26.01 ms/step | loss 78299.203 \n",
      "| iter   2298 | 22.00 ms/step | loss 282329.062 \n",
      "| iter   2299 | 24.02 ms/step | loss 154550.312 \n",
      "| iter   2300 | 27.01 ms/step | loss 2029778.875 \n",
      "| iter   2301 | 25.01 ms/step | loss 333374.562 \n",
      "| iter   2302 | 24.01 ms/step | loss 301888.500 \n",
      "| iter   2303 | 106.02 ms/step | loss 12716.406 \n",
      "| iter   2304 | 24.01 ms/step | loss 78430.062 \n",
      "| iter   2305 | 24.01 ms/step | loss 165925.984 \n",
      "| iter   2306 | 23.01 ms/step | loss 12609.142 \n",
      "| iter   2307 | 23.01 ms/step | loss 231119.859 \n",
      "| iter   2308 | 22.03 ms/step | loss 11408.051 \n",
      "| iter   2309 | 23.01 ms/step | loss 27346.223 \n",
      "| iter   2310 | 26.03 ms/step | loss 38955.609 \n",
      "| iter   2311 | 26.01 ms/step | loss 54474.340 \n",
      "| iter   2312 | 35.01 ms/step | loss 29099.270 \n",
      "| iter   2313 | 22.01 ms/step | loss 9667.015 \n",
      "| iter   2314 | 106.02 ms/step | loss 32159.113 \n",
      "| iter   2315 | 26.01 ms/step | loss 486792.250 \n",
      "| iter   2316 | 26.05 ms/step | loss 75506.438 \n",
      "| iter   2317 | 24.01 ms/step | loss 38742.906 \n",
      "| iter   2318 | 26.01 ms/step | loss 17736.418 \n",
      "| iter   2319 | 24.01 ms/step | loss 28898.303 \n",
      "| iter   2320 | 22.01 ms/step | loss 63150.609 \n",
      "| iter   2321 | 23.01 ms/step | loss 149988.156 \n",
      "| iter   2322 | 28.01 ms/step | loss 12926.305 \n",
      "| iter   2323 | 25.01 ms/step | loss 41535.098 \n",
      "| iter   2324 | 96.02 ms/step | loss 514797.938 \n",
      "| iter   2325 | 23.02 ms/step | loss 26134.121 \n",
      "| iter   2326 | 26.01 ms/step | loss 12077.191 \n",
      "| iter   2327 | 25.01 ms/step | loss 25610.211 \n",
      "| iter   2328 | 27.99 ms/step | loss 117689.008 \n",
      "| iter   2329 | 22.00 ms/step | loss 155531.156 \n",
      "| iter   2330 | 30.01 ms/step | loss 13313.879 \n",
      "| iter   2331 | 25.01 ms/step | loss 211926.859 \n",
      "| iter   2332 | 100.04 ms/step | loss 79394.656 \n",
      "| iter   2333 | 26.01 ms/step | loss 29135.793 \n",
      "| iter   2334 | 26.01 ms/step | loss 302074.031 \n",
      "| iter   2335 | 25.01 ms/step | loss 81868.180 \n",
      "| iter   2336 | 25.01 ms/step | loss 11883.764 \n",
      "| iter   2337 | 22.01 ms/step | loss 15644.062 \n",
      "| iter   2338 | 27.02 ms/step | loss 32248.297 \n",
      "| iter   2339 | 27.01 ms/step | loss 79453.953 \n",
      "| iter   2340 | 24.01 ms/step | loss 851371.438 \n",
      "| iter   2341 | 105.02 ms/step | loss 151819.578 \n",
      "| iter   2342 | 26.00 ms/step | loss 18052.934 \n",
      "| iter   2343 | 26.01 ms/step | loss 84475.648 \n",
      "| iter   2344 | 26.01 ms/step | loss 63894.066 \n",
      "| iter   2345 | 22.02 ms/step | loss 98242.195 \n",
      "| iter   2346 | 26.02 ms/step | loss 22871.686 \n",
      "| iter   2347 | 22.07 ms/step | loss 49989.648 \n",
      "| iter   2348 | 25.00 ms/step | loss 209112.734 \n",
      "| iter   2349 | 23.01 ms/step | loss 27392.686 \n",
      "| iter   2350 | 103.02 ms/step | loss 25058.754 \n",
      "| iter   2351 | 24.00 ms/step | loss 20351.715 \n",
      "| iter   2352 | 22.01 ms/step | loss 89807.594 \n",
      "| iter   2353 | 21.00 ms/step | loss 47808.012 \n",
      "| iter   2354 | 27.01 ms/step | loss 12775.713 \n",
      "| iter   2355 | 26.01 ms/step | loss 154232.750 \n",
      "| iter   2356 | 24.01 ms/step | loss 40502.547 \n",
      "| iter   2357 | 21.00 ms/step | loss 15040.689 \n",
      "| iter   2358 | 22.00 ms/step | loss 41180.426 \n",
      "| iter   2359 | 23.01 ms/step | loss 190322.000 \n",
      "| iter   2360 | 104.02 ms/step | loss 16631719.000 \n",
      "| iter   2361 | 22.02 ms/step | loss 23617.416 \n",
      "| iter   2362 | 27.02 ms/step | loss 67406.680 \n",
      "| iter   2363 | 27.01 ms/step | loss 56370.898 \n",
      "| iter   2364 | 26.01 ms/step | loss 11168.900 \n",
      "| iter   2365 | 22.02 ms/step | loss 189103.266 \n",
      "| iter   2366 | 26.01 ms/step | loss 27055.740 \n",
      "| iter   2367 | 26.01 ms/step | loss 163805.484 \n",
      "| iter   2368 | 26.01 ms/step | loss 31566.828 \n",
      "| iter   2369 | 21.01 ms/step | loss 14580.297 \n",
      "| iter   2370 | 112.03 ms/step | loss 28563.287 \n",
      "| iter   2371 | 23.01 ms/step | loss 68418.766 \n",
      "| iter   2372 | 24.01 ms/step | loss 8571.834 \n",
      "| iter   2373 | 21.01 ms/step | loss 46814.777 \n",
      "| iter   2374 | 26.01 ms/step | loss 34911.289 \n",
      "| iter   2375 | 24.01 ms/step | loss 33350.484 \n",
      "| iter   2376 | 26.01 ms/step | loss 33560.391 \n",
      "| iter   2377 | 26.01 ms/step | loss 15858.771 \n",
      "| iter   2378 | 27.01 ms/step | loss 38753.980 \n",
      "| iter   2379 | 104.03 ms/step | loss 11979.478 \n",
      "| iter   2380 | 22.00 ms/step | loss 259225.609 \n",
      "| iter   2381 | 26.00 ms/step | loss 449301.875 \n",
      "| iter   2382 | 28.01 ms/step | loss 24682.533 \n",
      "| iter   2383 | 24.01 ms/step | loss 361698.562 \n",
      "| iter   2384 | 24.01 ms/step | loss 30323.627 \n",
      "| iter   2385 | 23.01 ms/step | loss 7639.729 \n",
      "| iter   2386 | 30.01 ms/step | loss 138180.375 \n",
      "| iter   2387 | 28.01 ms/step | loss 18048.393 \n",
      "| iter   2388 | 25.00 ms/step | loss 11874.528 \n",
      "| iter   2389 | 26.01 ms/step | loss 14768.196 \n",
      "| iter   2390 | 108.03 ms/step | loss 43206.141 \n",
      "| iter   2391 | 27.01 ms/step | loss 201793.594 \n",
      "| iter   2392 | 30.00 ms/step | loss 429187.375 \n",
      "| iter   2393 | 22.01 ms/step | loss 21143.863 \n",
      "| iter   2394 | 23.01 ms/step | loss 1164693.875 \n",
      "| iter   2395 | 22.01 ms/step | loss 52981.242 \n",
      "| iter   2396 | 22.02 ms/step | loss 56535.598 \n",
      "| iter   2397 | 22.01 ms/step | loss 91780.430 \n",
      "| iter   2398 | 27.00 ms/step | loss 36830.359 \n",
      "| iter   2399 | 26.01 ms/step | loss 13549.529 \n",
      "| iter   2400 | 100.03 ms/step | loss 346730.750 \n",
      "| iter   2401 | 24.01 ms/step | loss 25665.072 \n",
      "| iter   2402 | 27.00 ms/step | loss 163165.047 \n",
      "| iter   2403 | 23.01 ms/step | loss 21172.871 \n",
      "| iter   2404 | 26.01 ms/step | loss 30667.469 \n",
      "| iter   2405 | 23.01 ms/step | loss 40459.816 \n",
      "| iter   2406 | 24.02 ms/step | loss 287754.062 \n",
      "| iter   2407 | 26.00 ms/step | loss 22981.406 \n",
      "| iter   2408 | 108.02 ms/step | loss 47774.109 \n",
      "| iter   2409 | 24.01 ms/step | loss 11389.217 \n",
      "| iter   2410 | 23.01 ms/step | loss 124594.094 \n",
      "| iter   2411 | 23.37 ms/step | loss 18596.309 \n",
      "| iter   2412 | 26.01 ms/step | loss 31029.311 \n",
      "| iter   2413 | 24.01 ms/step | loss 390440.719 \n",
      "| iter   2414 | 28.01 ms/step | loss 101172.648 \n",
      "| iter   2415 | 28.02 ms/step | loss 17911.781 \n",
      "| iter   2416 | 26.01 ms/step | loss 16061.292 \n",
      "| iter   2417 | 108.04 ms/step | loss 165294.281 \n",
      "| iter   2418 | 28.01 ms/step | loss 57511.316 \n",
      "| iter   2419 | 24.00 ms/step | loss 32537.189 \n",
      "| iter   2420 | 26.01 ms/step | loss 70509.234 \n",
      "| iter   2421 | 23.01 ms/step | loss 78384.781 \n",
      "| iter   2422 | 28.01 ms/step | loss 51396408.000 \n",
      "| iter   2423 | 25.01 ms/step | loss 26992.910 \n",
      "| iter   2424 | 24.01 ms/step | loss 229841.422 \n",
      "| iter   2425 | 25.02 ms/step | loss 259440.484 \n",
      "| iter   2426 | 110.10 ms/step | loss 187773.953 \n",
      "| iter   2427 | 25.99 ms/step | loss 22367.014 \n",
      "| iter   2428 | 27.01 ms/step | loss 49089.574 \n",
      "| iter   2429 | 22.00 ms/step | loss 28322.680 \n",
      "| iter   2430 | 28.01 ms/step | loss 17304.512 \n",
      "| iter   2431 | 26.01 ms/step | loss 58881.340 \n",
      "| iter   2432 | 24.01 ms/step | loss 662905.938 \n",
      "| iter   2433 | 22.02 ms/step | loss 21164.010 \n",
      "| iter   2434 | 23.00 ms/step | loss 10744.912 \n",
      "| iter   2435 | 22.01 ms/step | loss 6853.607 \n",
      "| iter   2436 | 99.02 ms/step | loss 116154.438 \n",
      "| iter   2437 | 22.01 ms/step | loss 2446853.750 \n",
      "| iter   2438 | 27.01 ms/step | loss 23798.543 \n",
      "| iter   2439 | 26.01 ms/step | loss 45028.090 \n",
      "| iter   2440 | 24.01 ms/step | loss 585064.062 \n",
      "| iter   2441 | 20.07 ms/step | loss 30766.354 \n",
      "| iter   2442 | 24.01 ms/step | loss 13297.865 \n",
      "| iter   2443 | 26.02 ms/step | loss 42408.664 \n",
      "| iter   2444 | 26.01 ms/step | loss 10634.658 \n",
      "| iter   2445 | 22.01 ms/step | loss 536796.188 \n",
      "| iter   2446 | 107.02 ms/step | loss 245184.984 \n",
      "| iter   2447 | 25.02 ms/step | loss 48305.535 \n",
      "| iter   2448 | 26.01 ms/step | loss 9189.639 \n",
      "| iter   2449 | 25.01 ms/step | loss 577808.312 \n",
      "| iter   2450 | 25.01 ms/step | loss 560419.500 \n",
      "| iter   2451 | 26.01 ms/step | loss 15690.941 \n",
      "| iter   2452 | 23.01 ms/step | loss 32957.137 \n",
      "| iter   2453 | 23.01 ms/step | loss 735919.375 \n",
      "| iter   2454 | 22.02 ms/step | loss 31965.873 \n",
      "| iter   2455 | 108.02 ms/step | loss 617344.000 \n",
      "| iter   2456 | 27.01 ms/step | loss 12763.995 \n",
      "| iter   2457 | 24.01 ms/step | loss 58864.859 \n",
      "| iter   2458 | 26.01 ms/step | loss 7412.085 \n",
      "| iter   2459 | 23.01 ms/step | loss 630860.562 \n",
      "| iter   2460 | 27.01 ms/step | loss 161235.781 \n",
      "| iter   2461 | 25.01 ms/step | loss 8752.213 \n",
      "| iter   2462 | 29.01 ms/step | loss 13501.157 \n",
      "| iter   2463 | 24.01 ms/step | loss 38041.316 \n",
      "| iter   2464 | 21.02 ms/step | loss 2497363.000 \n",
      "| iter   2465 | 23.01 ms/step | loss 10579.727 \n",
      "| iter   2466 | 102.03 ms/step | loss 17282.727 \n",
      "| iter   2467 | 27.99 ms/step | loss 92688.461 \n",
      "| iter   2468 | 22.01 ms/step | loss 15308.627 \n",
      "| iter   2469 | 20.01 ms/step | loss 602585.625 \n",
      "| iter   2470 | 23.00 ms/step | loss 33701.121 \n",
      "| iter   2471 | 22.01 ms/step | loss 534771.688 \n",
      "| iter   2472 | 20.99 ms/step | loss 8270.965 \n",
      "| iter   2473 | 21.00 ms/step | loss 321213.625 \n",
      "| iter   2474 | 28.01 ms/step | loss 31771.578 \n",
      "| iter   2475 | 22.01 ms/step | loss 520001.531 \n",
      "| iter   2476 | 107.02 ms/step | loss 25899470.000 \n",
      "| iter   2477 | 25.02 ms/step | loss 42261.156 \n",
      "| iter   2478 | 25.01 ms/step | loss 546578.562 \n",
      "| iter   2479 | 25.01 ms/step | loss 77754.180 \n",
      "| iter   2480 | 26.01 ms/step | loss 28032.031 \n",
      "| iter   2481 | 22.01 ms/step | loss 19223.971 \n",
      "| iter   2482 | 26.01 ms/step | loss 18153.828 \n",
      "| iter   2483 | 26.01 ms/step | loss 50180.117 \n",
      "| iter   2484 | 105.03 ms/step | loss 488642.000 \n",
      "| iter   2485 | 22.01 ms/step | loss 18367.750 \n",
      "| iter   2486 | 26.99 ms/step | loss 469334.938 \n",
      "| iter   2487 | 23.01 ms/step | loss 15789.533 \n",
      "| iter   2488 | 26.01 ms/step | loss 49716.281 \n",
      "| iter   2489 | 22.00 ms/step | loss 38182.602 \n",
      "| iter   2490 | 24.01 ms/step | loss 458148.312 \n",
      "| iter   2491 | 24.02 ms/step | loss 9563.150 \n",
      "| iter   2492 | 25.01 ms/step | loss 487381.969 \n",
      "| iter   2493 | 107.02 ms/step | loss 44176.770 \n",
      "| iter   2494 | 28.01 ms/step | loss 12893.696 \n",
      "| iter   2495 | 26.01 ms/step | loss 458063.188 \n",
      "| iter   2496 | 24.01 ms/step | loss 28712.918 \n",
      "| iter   2497 | 24.01 ms/step | loss 51773.453 \n",
      "| iter   2498 | 26.01 ms/step | loss 418331.094 \n",
      "| iter   2499 | 26.01 ms/step | loss 15234.850 \n",
      "| iter   2500 | 24.01 ms/step | loss 12525.795 \n",
      "| iter   2501 | 26.02 ms/step | loss 53539.469 \n",
      "| iter   2502 | 25.01 ms/step | loss 42815.820 \n",
      "| iter   2503 | 103.02 ms/step | loss 42663.613 \n",
      "| iter   2504 | 26.01 ms/step | loss 396248.000 \n",
      "| iter   2505 | 22.01 ms/step | loss 172173.281 \n",
      "| iter   2506 | 24.02 ms/step | loss 40366.539 \n",
      "| iter   2507 | 38.99 ms/step | loss 380764.375 \n",
      "| iter   2508 | 23.00 ms/step | loss 55200.773 \n",
      "| iter   2509 | 25.01 ms/step | loss 11516.898 \n",
      "| iter   2510 | 26.00 ms/step | loss 275406.062 \n",
      "| iter   2511 | 107.04 ms/step | loss 45717.199 \n",
      "| iter   2512 | 26.01 ms/step | loss 353843.094 \n",
      "| iter   2513 | 22.02 ms/step | loss 50670.441 \n",
      "| iter   2514 | 26.01 ms/step | loss 14662.319 \n",
      "| iter   2515 | 26.01 ms/step | loss 33524.586 \n",
      "| iter   2516 | 23.01 ms/step | loss 337889.062 \n",
      "| iter   2517 | 25.01 ms/step | loss 296636.812 \n",
      "| iter   2518 | 27.01 ms/step | loss 41297.172 \n",
      "| iter   2519 | 26.01 ms/step | loss 17138.916 \n",
      "| iter   2520 | 109.08 ms/step | loss 19160.383 \n",
      "| iter   2521 | 21.01 ms/step | loss 391655.250 \n",
      "| iter   2522 | 25.02 ms/step | loss 94504.703 \n",
      "| iter   2523 | 23.01 ms/step | loss 353675.344 \n",
      "| iter   2524 | 25.01 ms/step | loss 14253.800 \n",
      "| iter   2525 | 22.01 ms/step | loss 11160.161 \n",
      "| iter   2526 | 26.01 ms/step | loss 361887.000 \n",
      "| iter   2527 | 24.01 ms/step | loss 82598.000 \n",
      "| iter   2528 | 25.01 ms/step | loss 284521.062 \n",
      "| iter   2529 | 103.08 ms/step | loss 14744.972 \n",
      "| iter   2530 | 27.01 ms/step | loss 23882.521 \n",
      "| iter   2531 | 25.01 ms/step | loss 10884.676 \n",
      "| iter   2532 | 26.01 ms/step | loss 29255.230 \n",
      "| iter   2533 | 23.04 ms/step | loss 388173.594 \n",
      "| iter   2534 | 25.00 ms/step | loss 19230.895 \n",
      "| iter   2535 | 26.01 ms/step | loss 404081.469 \n",
      "| iter   2536 | 26.00 ms/step | loss 14529.012 \n",
      "| iter   2537 | 22.01 ms/step | loss 9840.809 \n",
      "| iter   2538 | 109.29 ms/step | loss 11390.555 \n",
      "| iter   2539 | 24.01 ms/step | loss 25760.719 \n",
      "| iter   2540 | 23.01 ms/step | loss 54383.020 \n",
      "| iter   2541 | 25.01 ms/step | loss 383910.312 \n",
      "| iter   2542 | 27.02 ms/step | loss 11443.421 \n",
      "| iter   2543 | 23.00 ms/step | loss 26390.314 \n",
      "| iter   2544 | 22.01 ms/step | loss 537012.375 \n",
      "| iter   2545 | 21.00 ms/step | loss 14565.121 \n",
      "| iter   2546 | 25.01 ms/step | loss 263672.188 \n",
      "| iter   2547 | 111.01 ms/step | loss 54850.090 \n",
      "| iter   2548 | 21.00 ms/step | loss 6803.708 \n",
      "| iter   2549 | 23.01 ms/step | loss 7419.325 \n",
      "| iter   2550 | 27.01 ms/step | loss 5213.921 \n",
      "| iter   2551 | 23.01 ms/step | loss 313838.750 \n",
      "| iter   2552 | 25.00 ms/step | loss 50892.727 \n",
      "| iter   2553 | 24.01 ms/step | loss 42477.496 \n",
      "| iter   2554 | 24.01 ms/step | loss 24096.014 \n",
      "| iter   2555 | 26.01 ms/step | loss 4420.193 \n",
      "| iter   2556 | 26.01 ms/step | loss 2574558.500 \n",
      "| iter   2557 | 24.00 ms/step | loss 373379.594 \n",
      "| iter   2558 | 101.08 ms/step | loss 84819.492 \n",
      "| iter   2559 | 26.01 ms/step | loss 294913.625 \n",
      "| iter   2560 | 25.01 ms/step | loss 18585.822 \n",
      "| iter   2561 | 24.00 ms/step | loss 26673.496 \n",
      "| iter   2562 | 24.01 ms/step | loss 37332.621 \n",
      "| iter   2563 | 24.08 ms/step | loss 16572.684 \n",
      "| iter   2564 | 26.01 ms/step | loss 46398.844 \n",
      "| iter   2565 | 24.01 ms/step | loss 310685.750 \n",
      "| iter   2566 | 24.01 ms/step | loss 23214.809 \n",
      "| iter   2567 | 107.02 ms/step | loss 41565.996 \n",
      "| iter   2568 | 26.01 ms/step | loss 254297.641 \n",
      "| iter   2569 | 22.01 ms/step | loss 32586.854 \n",
      "| iter   2570 | 24.02 ms/step | loss 273448.938 \n",
      "| iter   2571 | 22.02 ms/step | loss 14463.170 \n",
      "| iter   2572 | 23.01 ms/step | loss 19828.391 \n",
      "| iter   2573 | 24.01 ms/step | loss 363149.938 \n",
      "| iter   2574 | 27.01 ms/step | loss 437214.500 \n",
      "| iter   2575 | 25.01 ms/step | loss 11416.261 \n",
      "| iter   2576 | 106.03 ms/step | loss 251093.328 \n",
      "| iter   2577 | 25.01 ms/step | loss 15108.309 \n",
      "| iter   2578 | 24.01 ms/step | loss 8899.809 \n",
      "| iter   2579 | 22.01 ms/step | loss 272732.781 \n",
      "| iter   2580 | 25.01 ms/step | loss 287378.094 \n",
      "| iter   2581 | 21.02 ms/step | loss 59114.809 \n",
      "| iter   2582 | 25.01 ms/step | loss 208315.828 \n",
      "| iter   2583 | 24.01 ms/step | loss 3168898.000 \n",
      "| iter   2584 | 25.00 ms/step | loss 27032.902 \n",
      "| iter   2585 | 107.02 ms/step | loss 172552.781 \n",
      "| iter   2586 | 25.01 ms/step | loss 22037.625 \n",
      "| iter   2587 | 25.01 ms/step | loss 12078.067 \n",
      "| iter   2588 | 27.02 ms/step | loss 47226.035 \n",
      "| iter   2589 | 22.01 ms/step | loss 223808.000 \n",
      "| iter   2590 | 25.01 ms/step | loss 12344.657 \n",
      "| iter   2591 | 26.01 ms/step | loss 20729.982 \n",
      "| iter   2592 | 25.01 ms/step | loss 254404.672 \n",
      "| iter   2593 | 22.02 ms/step | loss 39754.246 \n",
      "| iter   2594 | 104.02 ms/step | loss 10504.520 \n",
      "| iter   2595 | 24.99 ms/step | loss 13773.939 \n",
      "| iter   2596 | 25.01 ms/step | loss 327151.938 \n",
      "| iter   2597 | 24.01 ms/step | loss 19627.525 \n",
      "| iter   2598 | 23.01 ms/step | loss 74132.203 \n",
      "| iter   2599 | 22.00 ms/step | loss 271322.250 \n",
      "| iter   2600 | 27.01 ms/step | loss 9009.924 \n",
      "| iter   2601 | 25.00 ms/step | loss 11059.338 \n",
      "| iter   2602 | 25.01 ms/step | loss 17591.535 \n",
      "| iter   2603 | 26.01 ms/step | loss 267842.531 \n",
      "| iter   2604 | 106.03 ms/step | loss 46384.914 \n",
      "| iter   2605 | 23.00 ms/step | loss 28732.582 \n",
      "| iter   2606 | 25.01 ms/step | loss 343209.906 \n",
      "| iter   2607 | 24.01 ms/step | loss 10187.429 \n",
      "| iter   2608 | 24.01 ms/step | loss 16729.885 \n",
      "| iter   2609 | 21.00 ms/step | loss 340330.406 \n",
      "| iter   2610 | 23.01 ms/step | loss 9097.939 \n",
      "| iter   2611 | 24.01 ms/step | loss 2623100.000 \n",
      "| iter   2612 | 107.02 ms/step | loss 351408.656 \n",
      "| iter   2613 | 24.01 ms/step | loss 44809.840 \n",
      "| iter   2614 | 26.01 ms/step | loss 348788.500 \n",
      "| iter   2615 | 25.99 ms/step | loss 2057650.250 \n",
      "| iter   2616 | 23.01 ms/step | loss 23324.930 \n",
      "| iter   2617 | 24.01 ms/step | loss 13238.610 \n",
      "| iter   2618 | 26.01 ms/step | loss 210914.094 \n",
      "| iter   2619 | 24.02 ms/step | loss 56394.633 \n",
      "| iter   2620 | 25.01 ms/step | loss 23527.070 \n",
      "| iter   2621 | 23.01 ms/step | loss 174215.672 \n",
      "| iter   2622 | 111.03 ms/step | loss 202221.656 \n",
      "| iter   2623 | 25.00 ms/step | loss 15460.252 \n",
      "| iter   2624 | 25.00 ms/step | loss 7516.404 \n",
      "| iter   2625 | 21.01 ms/step | loss 57742.422 \n",
      "| iter   2626 | 27.01 ms/step | loss 361308.906 \n",
      "| iter   2627 | 25.01 ms/step | loss 6286.039 \n",
      "| iter   2628 | 25.01 ms/step | loss 11572.086 \n",
      "| iter   2629 | 27.00 ms/step | loss 10490.327 \n",
      "| iter   2630 | 26.01 ms/step | loss 19666.805 \n",
      "| iter   2631 | 108.04 ms/step | loss 2034357.000 \n",
      "| iter   2632 | 25.01 ms/step | loss 10470.789 \n",
      "| iter   2633 | 25.01 ms/step | loss 215702.281 \n",
      "| iter   2634 | 26.01 ms/step | loss 435318.594 \n",
      "| iter   2635 | 23.01 ms/step | loss 56033.555 \n",
      "| iter   2636 | 25.02 ms/step | loss 7259.185 \n",
      "| iter   2637 | 23.00 ms/step | loss 5729.745 \n",
      "| iter   2638 | 25.01 ms/step | loss 232738.672 \n",
      "| iter   2639 | 25.01 ms/step | loss 281036.188 \n",
      "| iter   2640 | 23.01 ms/step | loss 10247.971 \n",
      "| iter   2641 | 24.00 ms/step | loss 4868.637 \n",
      "| iter   2642 | 111.03 ms/step | loss 48662.320 \n",
      "| iter   2643 | 25.00 ms/step | loss 15611.959 \n",
      "| iter   2644 | 25.00 ms/step | loss 826877.500 \n",
      "| iter   2645 | 25.01 ms/step | loss 8305.283 \n",
      "| iter   2646 | 28.06 ms/step | loss 3608261.000 \n",
      "| iter   2647 | 26.01 ms/step | loss 14844.711 \n",
      "| iter   2648 | 25.01 ms/step | loss 59536.148 \n",
      "| iter   2649 | 25.00 ms/step | loss 12493.097 \n",
      "| iter   2650 | 23.01 ms/step | loss 10726.816 \n",
      "| iter   2651 | 26.01 ms/step | loss 616927.438 \n",
      "| iter   2652 | 102.01 ms/step | loss 69965.531 \n",
      "| iter   2653 | 22.00 ms/step | loss 223959.719 \n",
      "| iter   2654 | 23.01 ms/step | loss 32352.410 \n",
      "| iter   2655 | 22.01 ms/step | loss 220479.625 \n",
      "| iter   2656 | 25.02 ms/step | loss 17294.684 \n",
      "| iter   2657 | 22.01 ms/step | loss 24045.836 \n",
      "| iter   2658 | 27.01 ms/step | loss 31733.154 \n",
      "| iter   2659 | 27.01 ms/step | loss 162138.062 \n",
      "| iter   2660 | 27.00 ms/step | loss 10728.482 \n",
      "| iter   2661 | 99.02 ms/step | loss 36475.559 \n",
      "| iter   2662 | 26.01 ms/step | loss 14729.968 \n",
      "| iter   2663 | 24.01 ms/step | loss 147459.188 \n",
      "| iter   2664 | 26.01 ms/step | loss 21779.051 \n",
      "| iter   2665 | 25.02 ms/step | loss 9979.201 \n",
      "| iter   2666 | 24.01 ms/step | loss 154194.734 \n",
      "| iter   2667 | 26.01 ms/step | loss 8983.638 \n",
      "| iter   2668 | 24.01 ms/step | loss 118031.195 \n",
      "| iter   2669 | 104.04 ms/step | loss 20555.770 \n",
      "| iter   2670 | 27.01 ms/step | loss 19015.789 \n",
      "| iter   2671 | 25.01 ms/step | loss 17164.209 \n",
      "| iter   2672 | 26.02 ms/step | loss 251594.344 \n",
      "| iter   2673 | 26.00 ms/step | loss 266542.781 \n",
      "| iter   2674 | 24.01 ms/step | loss 451647.719 \n",
      "| iter   2675 | 28.01 ms/step | loss 152674.156 \n",
      "| iter   2676 | 27.01 ms/step | loss 18798.609 \n",
      "| iter   2677 | 24.01 ms/step | loss 6970.469 \n",
      "| iter   2678 | 107.03 ms/step | loss 120717.594 \n",
      "| iter   2679 | 26.01 ms/step | loss 38596.059 \n",
      "| iter   2680 | 24.01 ms/step | loss 75974.578 \n",
      "| iter   2681 | 28.01 ms/step | loss 26428.953 \n",
      "| iter   2682 | 26.01 ms/step | loss 31332.438 \n",
      "| iter   2683 | 23.01 ms/step | loss 234308.547 \n",
      "| iter   2684 | 22.01 ms/step | loss 36017.289 \n",
      "| iter   2685 | 22.01 ms/step | loss 9933.798 \n",
      "| iter   2686 | 27.01 ms/step | loss 80624.297 \n",
      "| iter   2687 | 23.00 ms/step | loss 9353.819 \n",
      "| iter   2688 | 103.04 ms/step | loss 340504.344 \n",
      "| iter   2689 | 25.01 ms/step | loss 36799.180 \n",
      "| iter   2690 | 27.01 ms/step | loss 25812.672 \n",
      "| iter   2691 | 24.01 ms/step | loss 16513.293 \n",
      "| iter   2692 | 22.01 ms/step | loss 10424.289 \n",
      "| iter   2693 | 21.01 ms/step | loss 89467.414 \n",
      "| iter   2694 | 24.01 ms/step | loss 197157.594 \n",
      "| iter   2695 | 26.01 ms/step | loss 7716.386 \n",
      "| iter   2696 | 33.00 ms/step | loss 56775.453 \n",
      "| iter   2697 | 20.00 ms/step | loss 19018.822 \n",
      "| iter   2698 | 97.02 ms/step | loss 23754.668 \n",
      "| iter   2699 | 26.01 ms/step | loss 6298.643 \n",
      "| iter   2700 | 25.01 ms/step | loss 11380.753 \n",
      "| iter   2701 | 20.01 ms/step | loss 207582.312 \n",
      "| iter   2702 | 24.01 ms/step | loss 20142.531 \n",
      "| iter   2703 | 24.01 ms/step | loss 13062.625 \n",
      "| iter   2704 | 22.01 ms/step | loss 6173.558 \n",
      "| iter   2705 | 22.01 ms/step | loss 143822.219 \n",
      "| iter   2706 | 26.37 ms/step | loss 8615.252 \n",
      "| iter   2707 | 111.03 ms/step | loss 12134.935 \n",
      "| iter   2708 | 23.01 ms/step | loss 37972.039 \n",
      "| iter   2709 | 24.01 ms/step | loss 206759.578 \n",
      "| iter   2710 | 28.01 ms/step | loss 6481.745 \n",
      "| iter   2711 | 24.01 ms/step | loss 14838.209 \n",
      "| iter   2712 | 26.01 ms/step | loss 187409.125 \n",
      "| iter   2713 | 23.01 ms/step | loss 2202790.750 \n",
      "| iter   2714 | 25.01 ms/step | loss 39599.359 \n",
      "| iter   2715 | 26.01 ms/step | loss 245597.234 \n",
      "| iter   2716 | 24.01 ms/step | loss 48427.875 \n",
      "| iter   2717 | 24.01 ms/step | loss 135597.141 \n",
      "| iter   2718 | 109.03 ms/step | loss 130802.992 \n",
      "| iter   2719 | 25.01 ms/step | loss 98456.375 \n",
      "| iter   2720 | 25.01 ms/step | loss 14190.303 \n",
      "| iter   2721 | 24.01 ms/step | loss 11936.263 \n",
      "| iter   2722 | 25.02 ms/step | loss 10663.557 \n",
      "| iter   2723 | 28.01 ms/step | loss 16581.551 \n",
      "| iter   2724 | 26.05 ms/step | loss 99234.828 \n",
      "| iter   2725 | 24.00 ms/step | loss 63737.699 \n",
      "| iter   2726 | 27.01 ms/step | loss 47810.910 \n",
      "| iter   2727 | 27.01 ms/step | loss 42171.508 \n",
      "| iter   2728 | 109.04 ms/step | loss 155751.641 \n",
      "| iter   2729 | 21.00 ms/step | loss 15885.255 \n",
      "| iter   2730 | 30.01 ms/step | loss 25272.523 \n",
      "| iter   2731 | 23.01 ms/step | loss 41455.969 \n",
      "| iter   2732 | 22.02 ms/step | loss 188643.734 \n",
      "| iter   2733 | 24.01 ms/step | loss 12615.698 \n",
      "| iter   2734 | 25.01 ms/step | loss 10955.816 \n",
      "| iter   2735 | 27.01 ms/step | loss 17273.598 \n",
      "| iter   2736 | 105.24 ms/step | loss 25099.184 \n",
      "| iter   2737 | 23.01 ms/step | loss 356870.781 \n",
      "| iter   2738 | 26.01 ms/step | loss 29476.613 \n",
      "| iter   2739 | 25.01 ms/step | loss 10532.088 \n",
      "| iter   2740 | 24.00 ms/step | loss 10563.948 \n",
      "| iter   2741 | 26.01 ms/step | loss 195953.781 \n",
      "| iter   2742 | 25.00 ms/step | loss 21632.199 \n",
      "| iter   2743 | 23.02 ms/step | loss 130246.250 \n",
      "| iter   2744 | 26.01 ms/step | loss 16825.236 \n",
      "| iter   2745 | 104.02 ms/step | loss 47407.160 \n",
      "| iter   2746 | 24.01 ms/step | loss 43582.500 \n",
      "| iter   2747 | 27.01 ms/step | loss 104015.414 \n",
      "| iter   2748 | 23.00 ms/step | loss 25767.236 \n",
      "| iter   2749 | 23.01 ms/step | loss 140965.781 \n",
      "| iter   2750 | 24.01 ms/step | loss 22885.332 \n",
      "| iter   2751 | 27.01 ms/step | loss 127976.672 \n",
      "| iter   2752 | 22.01 ms/step | loss 417080704.000 \n",
      "| iter   2753 | 27.01 ms/step | loss 54228.180 \n",
      "| iter   2754 | 106.03 ms/step | loss 44538.234 \n",
      "| iter   2755 | 26.02 ms/step | loss 117094.422 \n",
      "| iter   2756 | 25.00 ms/step | loss 10127.082 \n",
      "| iter   2757 | 24.01 ms/step | loss 9899.726 \n",
      "| iter   2758 | 26.01 ms/step | loss 117647648.000 \n",
      "| iter   2759 | 26.01 ms/step | loss 237254.000 \n",
      "| iter   2760 | 21.02 ms/step | loss 33695.859 \n",
      "| iter   2761 | 22.01 ms/step | loss 108620.992 \n",
      "| iter   2762 | 27.01 ms/step | loss 30986.201 \n",
      "| iter   2763 | 23.01 ms/step | loss 41848.914 \n",
      "| iter   2764 | 120.01 ms/step | loss 3745589.250 \n",
      "| iter   2765 | 21.00 ms/step | loss 494453.375 \n",
      "| iter   2766 | 22.02 ms/step | loss 302271.375 \n",
      "| iter   2767 | 24.01 ms/step | loss 2007961.125 \n",
      "| iter   2768 | 25.01 ms/step | loss 199407.953 \n",
      "| iter   2769 | 23.01 ms/step | loss 18913.035 \n",
      "| iter   2770 | 28.01 ms/step | loss 103799.922 \n",
      "| iter   2771 | 26.01 ms/step | loss 27296.375 \n",
      "| iter   2772 | 25.01 ms/step | loss 196336.531 \n",
      "| iter   2773 | 24.01 ms/step | loss 273548.656 \n",
      "| iter   2774 | 109.02 ms/step | loss 949997.000 \n",
      "| iter   2775 | 26.01 ms/step | loss 42989.375 \n",
      "| iter   2776 | 21.00 ms/step | loss 96519.742 \n",
      "| iter   2777 | 24.01 ms/step | loss 349632.469 \n",
      "| iter   2778 | 27.01 ms/step | loss 22655.785 \n",
      "| iter   2779 | 24.00 ms/step | loss 363928.188 \n",
      "| iter   2780 | 23.01 ms/step | loss 62664596.000 \n",
      "| iter   2781 | 23.01 ms/step | loss 731651.188 \n",
      "| iter   2782 | 27.06 ms/step | loss 228322.094 \n",
      "| iter   2783 | 111.03 ms/step | loss 602991.000 \n",
      "| iter   2784 | 25.02 ms/step | loss 81478.445 \n",
      "| iter   2785 | 24.01 ms/step | loss 40867.250 \n",
      "| iter   2786 | 24.01 ms/step | loss 153900.719 \n",
      "| iter   2787 | 26.01 ms/step | loss 26759.520 \n",
      "| iter   2788 | 21.02 ms/step | loss 482887.250 \n",
      "| iter   2789 | 25.01 ms/step | loss 223286.547 \n",
      "| iter   2790 | 28.01 ms/step | loss 374253.344 \n",
      "| iter   2791 | 22.01 ms/step | loss 243169.312 \n",
      "| iter   2792 | 23.00 ms/step | loss 22859.377 \n",
      "| iter   2793 | 21.01 ms/step | loss 41030.668 \n",
      "| iter   2794 | 107.06 ms/step | loss 29632.414 \n",
      "| iter   2795 | 26.01 ms/step | loss 5315121.500 \n",
      "| iter   2796 | 56.00 ms/step | loss 262941.375 \n",
      "| iter   2797 | 29.01 ms/step | loss 52204.641 \n",
      "| iter   2798 | 23.01 ms/step | loss 315676.562 \n",
      "| iter   2799 | 21.00 ms/step | loss 60112.551 \n",
      "| iter   2800 | 26.02 ms/step | loss 31951.041 \n",
      "| iter   2801 | 25.01 ms/step | loss 52999.531 \n",
      "| iter   2802 | 26.01 ms/step | loss 48112.762 \n",
      "| iter   2803 | 24.01 ms/step | loss 92088.469 \n",
      "| iter   2804 | 105.03 ms/step | loss 81174856.000 \n",
      "| iter   2805 | 23.01 ms/step | loss 139104.625 \n",
      "| iter   2806 | 29.01 ms/step | loss 7154025.000 \n",
      "| iter   2807 | 24.01 ms/step | loss 178775.547 \n",
      "| iter   2808 | 21.02 ms/step | loss 19319.301 \n",
      "| iter   2809 | 23.01 ms/step | loss 22206.914 \n",
      "| iter   2810 | 23.01 ms/step | loss 43340.695 \n",
      "| iter   2811 | 25.04 ms/step | loss 80336.086 \n",
      "| iter   2812 | 109.03 ms/step | loss 310069.656 \n",
      "| iter   2813 | 22.01 ms/step | loss 4646614.500 \n",
      "| iter   2814 | 26.01 ms/step | loss 603115.625 \n",
      "| iter   2815 | 24.01 ms/step | loss 41202.312 \n",
      "| iter   2816 | 25.01 ms/step | loss 35445.211 \n",
      "| iter   2817 | 25.01 ms/step | loss 369716.562 \n",
      "| iter   2818 | 25.01 ms/step | loss 258200.281 \n",
      "| iter   2819 | 25.01 ms/step | loss 56517.523 \n",
      "| iter   2820 | 27.01 ms/step | loss 1530424.875 \n",
      "| iter   2821 | 101.01 ms/step | loss 26716.205 \n",
      "| iter   2822 | 26.01 ms/step | loss 25798.867 \n",
      "| iter   2823 | 22.02 ms/step | loss 251915.969 \n",
      "| iter   2824 | 27.07 ms/step | loss 70236.641 \n",
      "| iter   2825 | 23.01 ms/step | loss 93005816.000 \n",
      "| iter   2826 | 27.01 ms/step | loss 203224.188 \n",
      "| iter   2827 | 26.01 ms/step | loss 320559.938 \n",
      "| iter   2828 | 26.01 ms/step | loss 28080.531 \n",
      "| iter   2829 | 23.01 ms/step | loss 57068.762 \n",
      "| iter   2830 | 107.02 ms/step | loss 19565.004 \n",
      "| iter   2831 | 26.01 ms/step | loss 254534.078 \n",
      "| iter   2832 | 27.01 ms/step | loss 37333.766 \n",
      "| iter   2833 | 21.00 ms/step | loss 68362.297 \n",
      "| iter   2834 | 27.01 ms/step | loss 91865.195 \n",
      "| iter   2835 | 26.01 ms/step | loss 50499772.000 \n",
      "| iter   2836 | 25.01 ms/step | loss 128957.406 \n",
      "| iter   2837 | 23.01 ms/step | loss 1112009.375 \n",
      "| iter   2838 | 35.99 ms/step | loss 116327.109 \n",
      "| iter   2839 | 22.00 ms/step | loss 11999.166 \n",
      "| iter   2840 | 93.04 ms/step | loss 57809.105 \n",
      "| iter   2841 | 25.02 ms/step | loss 38775.297 \n",
      "| iter   2842 | 28.01 ms/step | loss 26007.160 \n",
      "| iter   2843 | 23.01 ms/step | loss 1212196.125 \n",
      "| iter   2844 | 25.01 ms/step | loss 423187.625 \n",
      "| iter   2845 | 24.01 ms/step | loss 10562305.000 \n",
      "| iter   2846 | 24.01 ms/step | loss 101192.711 \n",
      "| iter   2847 | 26.01 ms/step | loss 12574686.000 \n",
      "| iter   2848 | 109.02 ms/step | loss 34565.875 \n",
      "| iter   2849 | 24.01 ms/step | loss 7938368.000 \n",
      "| iter   2850 | 25.01 ms/step | loss 33540.242 \n",
      "| iter   2851 | 25.01 ms/step | loss 132664.672 \n",
      "| iter   2852 | 26.01 ms/step | loss 54432.664 \n",
      "| iter   2853 | 25.01 ms/step | loss 30361.725 \n",
      "| iter   2854 | 23.03 ms/step | loss 43707316.000 \n",
      "| iter   2855 | 22.01 ms/step | loss 209587.906 \n",
      "| iter   2856 | 24.01 ms/step | loss 52737.625 \n",
      "| iter   2857 | 24.01 ms/step | loss 26240.506 \n",
      "| iter   2858 | 108.02 ms/step | loss 16047.889 \n",
      "| iter   2859 | 26.01 ms/step | loss 302448.219 \n",
      "| iter   2860 | 26.01 ms/step | loss 108051.570 \n",
      "| iter   2861 | 22.01 ms/step | loss 77483.570 \n",
      "| iter   2862 | 25.01 ms/step | loss 17400.324 \n",
      "| iter   2863 | 25.01 ms/step | loss 217437.016 \n",
      "| iter   2864 | 22.99 ms/step | loss 251326.906 \n",
      "| iter   2865 | 22.00 ms/step | loss 30949.525 \n",
      "| iter   2866 | 24.01 ms/step | loss 58467.004 \n",
      "| iter   2867 | 104.04 ms/step | loss 37811.164 \n",
      "| iter   2868 | 24.01 ms/step | loss 417923.219 \n",
      "| iter   2869 | 23.01 ms/step | loss 60433.312 \n",
      "| iter   2870 | 22.01 ms/step | loss 40224.762 \n",
      "| iter   2871 | 22.00 ms/step | loss 177585.812 \n",
      "| iter   2872 | 26.01 ms/step | loss 20721.227 \n",
      "| iter   2873 | 23.01 ms/step | loss 124597.836 \n",
      "| iter   2874 | 25.01 ms/step | loss 285262.219 \n",
      "| iter   2875 | 26.01 ms/step | loss 579856.750 \n",
      "| iter   2876 | 23.01 ms/step | loss 19402.680 \n",
      "| iter   2877 | 23.01 ms/step | loss 28102.793 \n",
      "| iter   2878 | 106.01 ms/step | loss 39617.555 \n",
      "| iter   2879 | 22.00 ms/step | loss 28829.730 \n",
      "| iter   2880 | 22.00 ms/step | loss 15810.642 \n",
      "| iter   2881 | 21.00 ms/step | loss 78530.883 \n",
      "| iter   2882 | 26.01 ms/step | loss 27016.418 \n",
      "| iter   2883 | 23.00 ms/step | loss 17561952.000 \n",
      "| iter   2884 | 22.01 ms/step | loss 51887.246 \n",
      "| iter   2885 | 24.01 ms/step | loss 315717.156 \n",
      "| iter   2886 | 28.01 ms/step | loss 6182392.500 \n",
      "| iter   2887 | 26.01 ms/step | loss 43885.559 \n",
      "| iter   2888 | 112.03 ms/step | loss 18374.355 \n",
      "| iter   2889 | 26.01 ms/step | loss 68042.539 \n",
      "| iter   2890 | 29.01 ms/step | loss 63367.242 \n",
      "| iter   2891 | 28.01 ms/step | loss 150294.141 \n",
      "| iter   2892 | 26.01 ms/step | loss 146658.469 \n",
      "| iter   2893 | 25.99 ms/step | loss 4312604.500 \n",
      "| iter   2894 | 27.01 ms/step | loss 40247.176 \n",
      "| iter   2895 | 26.01 ms/step | loss 325554.938 \n",
      "| iter   2896 | 113.02 ms/step | loss 99417.102 \n",
      "| iter   2897 | 25.01 ms/step | loss 38572.707 \n",
      "| iter   2898 | 29.00 ms/step | loss 382551.844 \n",
      "| iter   2899 | 26.01 ms/step | loss 40862.000 \n",
      "| iter   2900 | 25.01 ms/step | loss 14987.290 \n",
      "| iter   2901 | 25.01 ms/step | loss 698147.188 \n",
      "| iter   2902 | 28.01 ms/step | loss 386475.219 \n",
      "| iter   2903 | 28.00 ms/step | loss 36445.668 \n",
      "| iter   2904 | 27.01 ms/step | loss 27312.195 \n",
      "| iter   2905 | 111.03 ms/step | loss 14834.215 \n",
      "| iter   2906 | 38.99 ms/step | loss 59133.379 \n",
      "| iter   2907 | 25.01 ms/step | loss 371960.062 \n",
      "| iter   2908 | 25.01 ms/step | loss 34072.875 \n",
      "| iter   2909 | 22.00 ms/step | loss 38689.543 \n",
      "| iter   2910 | 27.02 ms/step | loss 43798.703 \n",
      "| iter   2911 | 26.00 ms/step | loss 5849788.500 \n",
      "| iter   2912 | 26.01 ms/step | loss 70755.203 \n",
      "| iter   2913 | 21.01 ms/step | loss 29754.146 \n",
      "| iter   2914 | 106.04 ms/step | loss 19193.934 \n",
      "| iter   2915 | 25.99 ms/step | loss 403023.719 \n",
      "| iter   2916 | 33.01 ms/step | loss 13396.902 \n",
      "| iter   2917 | 25.01 ms/step | loss 52688.965 \n",
      "| iter   2918 | 22.00 ms/step | loss 112068.430 \n",
      "| iter   2919 | 29.00 ms/step | loss 1016567.812 \n",
      "| iter   2920 | 27.01 ms/step | loss 81063.102 \n",
      "| iter   2921 | 24.00 ms/step | loss 654962.438 \n",
      "| iter   2922 | 25.01 ms/step | loss 47104.734 \n",
      "| iter   2923 | 25.01 ms/step | loss 406682.812 \n",
      "| iter   2924 | 111.03 ms/step | loss 28373.484 \n",
      "| iter   2925 | 28.01 ms/step | loss 432756.344 \n",
      "| iter   2926 | 23.03 ms/step | loss 42928.777 \n",
      "| iter   2927 | 26.99 ms/step | loss 16713.516 \n",
      "| iter   2928 | 21.99 ms/step | loss 42194.906 \n",
      "| iter   2929 | 24.01 ms/step | loss 401635.094 \n",
      "| iter   2930 | 24.01 ms/step | loss 369960.406 \n",
      "| iter   2931 | 25.01 ms/step | loss 70846.008 \n",
      "| iter   2932 | 23.01 ms/step | loss 181546.688 \n",
      "| iter   2933 | 22.00 ms/step | loss 38321.852 \n",
      "| iter   2934 | 110.03 ms/step | loss 48559.832 \n",
      "| iter   2935 | 23.01 ms/step | loss 187285.172 \n",
      "| iter   2936 | 26.01 ms/step | loss 8264.085 \n",
      "| iter   2937 | 27.01 ms/step | loss 400915.062 \n",
      "| iter   2938 | 26.01 ms/step | loss 412170.219 \n",
      "| iter   2939 | 25.01 ms/step | loss 35309.164 \n",
      "| iter   2940 | 25.01 ms/step | loss 68603.195 \n",
      "| iter   2941 | 26.01 ms/step | loss 21832.145 \n",
      "| iter   2942 | 27.01 ms/step | loss 66130.594 \n",
      "| iter   2943 | 108.30 ms/step | loss 9707.641 \n",
      "| iter   2944 | 22.01 ms/step | loss 462581.625 \n",
      "| iter   2945 | 21.01 ms/step | loss 20495.389 \n",
      "| iter   2946 | 27.01 ms/step | loss 300361.594 \n",
      "| iter   2947 | 24.00 ms/step | loss 9332929.000 \n",
      "| iter   2948 | 26.00 ms/step | loss 163660.438 \n",
      "| iter   2949 | 24.01 ms/step | loss 67306.266 \n",
      "| iter   2950 | 27.01 ms/step | loss 362059.625 \n",
      "| iter   2951 | 26.02 ms/step | loss 15286.207 \n",
      "| iter   2952 | 27.01 ms/step | loss 12678.465 \n",
      "| iter   2953 | 24.01 ms/step | loss 8969.316 \n",
      "| iter   2954 | 112.03 ms/step | loss 319181.031 \n",
      "| iter   2955 | 27.01 ms/step | loss 37210.785 \n",
      "| iter   2956 | 25.01 ms/step | loss 26795.859 \n",
      "| iter   2957 | 25.01 ms/step | loss 894768.688 \n",
      "| iter   2958 | 27.00 ms/step | loss 1232377.250 \n",
      "| iter   2959 | 25.01 ms/step | loss 22062.602 \n",
      "| iter   2960 | 23.01 ms/step | loss 1649680.750 \n",
      "| iter   2961 | 27.01 ms/step | loss 31192.611 \n",
      "| iter   2962 | 27.01 ms/step | loss 171847.141 \n",
      "| iter   2963 | 26.01 ms/step | loss 81819.617 \n",
      "| iter   2964 | 106.02 ms/step | loss 47766.512 \n",
      "| iter   2965 | 22.01 ms/step | loss 405076.219 \n",
      "| iter   2966 | 25.01 ms/step | loss 39356.703 \n",
      "| iter   2967 | 26.01 ms/step | loss 156978.875 \n",
      "| iter   2968 | 25.00 ms/step | loss 9152.157 \n",
      "| iter   2969 | 25.01 ms/step | loss 54332.578 \n",
      "| iter   2970 | 24.01 ms/step | loss 595172.500 \n",
      "| iter   2971 | 26.00 ms/step | loss 9659.789 \n",
      "| iter   2972 | 111.09 ms/step | loss 311989.688 \n",
      "| iter   2973 | 25.02 ms/step | loss 19765.279 \n",
      "| iter   2974 | 26.01 ms/step | loss 346012.312 \n",
      "| iter   2975 | 25.01 ms/step | loss 212462.984 \n",
      "| iter   2976 | 25.01 ms/step | loss 384725.688 \n",
      "| iter   2977 | 22.02 ms/step | loss 18642.225 \n",
      "| iter   2978 | 27.01 ms/step | loss 202402.688 \n",
      "| iter   2979 | 27.01 ms/step | loss 174090.156 \n",
      "| iter   2980 | 29.01 ms/step | loss 184785.031 \n",
      "| iter   2981 | 103.02 ms/step | loss 18644.664 \n",
      "| iter   2982 | 24.00 ms/step | loss 67759.656 \n",
      "| iter   2983 | 24.01 ms/step | loss 576301.438 \n",
      "| iter   2984 | 26.01 ms/step | loss 59542.176 \n",
      "| iter   2985 | 25.01 ms/step | loss 4437.262 \n",
      "| iter   2986 | 29.02 ms/step | loss 812149.000 \n",
      "| iter   2987 | 23.00 ms/step | loss 56558.711 \n",
      "| iter   2988 | 22.00 ms/step | loss 16245.707 \n",
      "| iter   2989 | 19.00 ms/step | loss 8721.106 \n",
      "| iter   2990 | 105.04 ms/step | loss 30744.031 \n",
      "| iter   2991 | 27.01 ms/step | loss 332072.125 \n",
      "| iter   2992 | 22.01 ms/step | loss 73365.625 \n",
      "| iter   2993 | 25.01 ms/step | loss 73307.445 \n",
      "| iter   2994 | 24.01 ms/step | loss 224634.734 \n",
      "| iter   2995 | 26.01 ms/step | loss 57045.355 \n",
      "| iter   2996 | 27.01 ms/step | loss 533831072.000 \n",
      "| iter   2997 | 24.05 ms/step | loss 54948.012 \n",
      "| iter   2998 | 25.01 ms/step | loss 413274.156 \n",
      "| iter   2999 | 26.01 ms/step | loss 69048.867 \n",
      "| iter   3000 | 108.03 ms/step | loss 10376.017 \n",
      "| iter   3001 | 25.01 ms/step | loss 24951.441 \n",
      "| iter   3002 | 25.01 ms/step | loss 560695.812 \n",
      "| iter   3003 | 26.01 ms/step | loss 102866.266 \n",
      "| iter   3004 | 22.01 ms/step | loss 21144.570 \n",
      "| iter   3005 | 22.00 ms/step | loss 34893.934 \n",
      "| iter   3006 | 28.01 ms/step | loss 26428.871 \n",
      "| iter   3007 | 25.02 ms/step | loss 577830.188 \n",
      "| iter   3008 | 22.01 ms/step | loss 1747141.625 \n",
      "| iter   3009 | 23.02 ms/step | loss 22516.217 \n",
      "| iter   3010 | 106.04 ms/step | loss 1527497.750 \n",
      "| iter   3011 | 25.01 ms/step | loss 18941.863 \n",
      "| iter   3012 | 25.01 ms/step | loss 17164.680 \n",
      "| iter   3013 | 24.01 ms/step | loss 126799.641 \n",
      "| iter   3014 | 27.01 ms/step | loss 48887.797 \n",
      "| iter   3015 | 26.02 ms/step | loss 72637.367 \n",
      "| iter   3016 | 24.02 ms/step | loss 107362.438 \n",
      "| iter   3017 | 21.08 ms/step | loss 2137899.500 \n",
      "| iter   3018 | 28.00 ms/step | loss 81283.289 \n",
      "| iter   3019 | 26.01 ms/step | loss 19451.801 \n",
      "| iter   3020 | 108.02 ms/step | loss 1279086.500 \n",
      "| iter   3021 | 23.01 ms/step | loss 28129.439 \n",
      "| iter   3022 | 28.01 ms/step | loss 1186166.375 \n",
      "| iter   3023 | 24.01 ms/step | loss 160609.094 \n",
      "| iter   3024 | 22.01 ms/step | loss 14195.836 \n",
      "| iter   3025 | 24.00 ms/step | loss 32000.295 \n",
      "| iter   3026 | 24.00 ms/step | loss 2640624.750 \n",
      "| iter   3027 | 26.01 ms/step | loss 11884.384 \n",
      "| iter   3028 | 26.00 ms/step | loss 106926.539 \n",
      "| iter   3029 | 23.00 ms/step | loss 4609844.500 \n",
      "| iter   3030 | 127.05 ms/step | loss 27583.293 \n",
      "| iter   3031 | 29.01 ms/step | loss 115182.844 \n",
      "| iter   3032 | 32.01 ms/step | loss 451954.750 \n",
      "| iter   3033 | 33.01 ms/step | loss 1033013.750 \n",
      "| iter   3034 | 26.01 ms/step | loss 2275452.000 \n",
      "| iter   3035 | 22.01 ms/step | loss 2934374.250 \n",
      "| iter   3036 | 28.01 ms/step | loss 106007.875 \n",
      "| iter   3037 | 23.01 ms/step | loss 51760.297 \n",
      "| iter   3038 | 23.01 ms/step | loss 115094.180 \n",
      "| iter   3039 | 27.01 ms/step | loss 68026.320 \n",
      "| iter   3040 | 107.04 ms/step | loss 454571.750 \n",
      "| iter   3041 | 23.01 ms/step | loss 16039.809 \n",
      "| iter   3042 | 28.00 ms/step | loss 29607.133 \n",
      "| iter   3043 | 24.00 ms/step | loss 54072.750 \n",
      "| iter   3044 | 28.01 ms/step | loss 28240.680 \n",
      "| iter   3045 | 22.01 ms/step | loss 1744222.375 \n",
      "| iter   3046 | 25.01 ms/step | loss 61184.992 \n",
      "| iter   3047 | 23.02 ms/step | loss 9453.480 \n",
      "| iter   3048 | 103.01 ms/step | loss 11257.919 \n",
      "| iter   3049 | 22.00 ms/step | loss 2182999.500 \n",
      "| iter   3050 | 25.01 ms/step | loss 119413.086 \n",
      "| iter   3051 | 27.01 ms/step | loss 17663.172 \n",
      "| iter   3052 | 25.01 ms/step | loss 12146.739 \n",
      "| iter   3053 | 23.01 ms/step | loss 1011365.438 \n",
      "| iter   3054 | 28.01 ms/step | loss 24497.020 \n",
      "| iter   3055 | 27.01 ms/step | loss 19767.492 \n",
      "| iter   3056 | 25.01 ms/step | loss 24661.215 \n",
      "| iter   3057 | 108.02 ms/step | loss 3152952.250 \n",
      "| iter   3058 | 27.02 ms/step | loss 21267.852 \n",
      "| iter   3059 | 27.99 ms/step | loss 13112353.000 \n",
      "| iter   3060 | 22.00 ms/step | loss 89688.375 \n",
      "| iter   3061 | 21.00 ms/step | loss 17285.145 \n",
      "| iter   3062 | 24.01 ms/step | loss 10207609.000 \n",
      "| iter   3063 | 24.03 ms/step | loss 485929.625 \n",
      "| iter   3064 | 25.01 ms/step | loss 46228.816 \n",
      "| iter   3065 | 24.01 ms/step | loss 96096.578 \n",
      "| iter   3066 | 112.03 ms/step | loss 94569.969 \n",
      "| iter   3067 | 26.01 ms/step | loss 28866.195 \n",
      "| iter   3068 | 26.01 ms/step | loss 48490.512 \n",
      "| iter   3069 | 25.01 ms/step | loss 1441559.250 \n",
      "| iter   3070 | 28.01 ms/step | loss 20895.525 \n",
      "| iter   3071 | 26.06 ms/step | loss 84411.477 \n",
      "| iter   3072 | 25.00 ms/step | loss 75057.984 \n",
      "| iter   3073 | 26.01 ms/step | loss 2168080.500 \n",
      "| iter   3074 | 26.01 ms/step | loss 14491.934 \n",
      "| iter   3075 | 24.02 ms/step | loss 20708.762 \n",
      "| iter   3076 | 110.03 ms/step | loss 1600219.625 \n",
      "| iter   3077 | 21.01 ms/step | loss 99916.859 \n",
      "| iter   3078 | 28.01 ms/step | loss 11937.317 \n",
      "| iter   3079 | 26.01 ms/step | loss 34402.508 \n",
      "| iter   3080 | 23.99 ms/step | loss 1759101.500 \n",
      "| iter   3081 | 23.01 ms/step | loss 14729.436 \n",
      "| iter   3082 | 25.01 ms/step | loss 65904.938 \n",
      "| iter   3083 | 22.01 ms/step | loss 49854.129 \n",
      "| iter   3084 | 24.01 ms/step | loss 56280.477 \n",
      "| iter   3085 | 25.01 ms/step | loss 1635915.500 \n",
      "| iter   3086 | 153.02 ms/step | loss 208686.328 \n",
      "| iter   3087 | 23.02 ms/step | loss 57481.750 \n",
      "| iter   3088 | 28.01 ms/step | loss 56659.098 \n",
      "| iter   3089 | 21.01 ms/step | loss 2015280.375 \n",
      "| iter   3090 | 25.99 ms/step | loss 870549.625 \n",
      "| iter   3091 | 24.01 ms/step | loss 59240.641 \n",
      "| iter   3092 | 29.99 ms/step | loss 1468053.125 \n",
      "| iter   3093 | 24.01 ms/step | loss 30538.451 \n",
      "| iter   3094 | 25.02 ms/step | loss 21631.090 \n",
      "| iter   3095 | 103.03 ms/step | loss 213552.203 \n",
      "| iter   3096 | 23.00 ms/step | loss 27570.391 \n",
      "| iter   3097 | 35.99 ms/step | loss 1489090.750 \n",
      "| iter   3098 | 23.01 ms/step | loss 1133791.125 \n",
      "| iter   3099 | 24.01 ms/step | loss 431944.219 \n",
      "| iter   3100 | 25.00 ms/step | loss 14461.966 \n",
      "| iter   3101 | 26.02 ms/step | loss 15247.957 \n",
      "| iter   3102 | 27.01 ms/step | loss 7927.602 \n",
      "| iter   3103 | 26.01 ms/step | loss 1133190.500 \n",
      "| iter   3104 | 25.01 ms/step | loss 95950.758 \n",
      "| iter   3105 | 24.01 ms/step | loss 22479.377 \n",
      "| iter   3106 | 134.01 ms/step | loss 1420944.500 \n",
      "| iter   3107 | 25.99 ms/step | loss 20539.809 \n",
      "| iter   3108 | 23.00 ms/step | loss 12145.477 \n",
      "| iter   3109 | 23.01 ms/step | loss 6996.209 \n",
      "| iter   3110 | 24.01 ms/step | loss 21971.977 \n",
      "| iter   3111 | 27.01 ms/step | loss 229455.906 \n",
      "| iter   3112 | 26.01 ms/step | loss 61030.398 \n",
      "| iter   3113 | 24.01 ms/step | loss 709740.625 \n",
      "| iter   3114 | 28.01 ms/step | loss 15478.749 \n",
      "| iter   3115 | 29.01 ms/step | loss 37368.285 \n",
      "| iter   3116 | 111.04 ms/step | loss 1390561.375 \n",
      "| iter   3117 | 22.99 ms/step | loss 44920.973 \n",
      "| iter   3118 | 24.01 ms/step | loss 16529.910 \n",
      "| iter   3119 | 25.00 ms/step | loss 1541746.750 \n",
      "| iter   3120 | 25.01 ms/step | loss 8109.691 \n",
      "| iter   3121 | 23.01 ms/step | loss 147237.312 \n",
      "| iter   3122 | 28.01 ms/step | loss 11790.465 \n",
      "| iter   3123 | 28.01 ms/step | loss 910036.750 \n",
      "| iter   3124 | 26.01 ms/step | loss 12961.979 \n",
      "| iter   3125 | 114.03 ms/step | loss 116971.289 \n",
      "| iter   3126 | 29.01 ms/step | loss 46269.133 \n",
      "| iter   3127 | 25.01 ms/step | loss 64263.805 \n",
      "| iter   3128 | 26.01 ms/step | loss 934472.125 \n",
      "| iter   3129 | 24.00 ms/step | loss 15790.015 \n",
      "| iter   3130 | 26.99 ms/step | loss 44925.805 \n",
      "| iter   3131 | 23.01 ms/step | loss 13358.754 \n",
      "| iter   3132 | 22.99 ms/step | loss 765327.500 \n",
      "| iter   3133 | 105.02 ms/step | loss 18395.625 \n",
      "| iter   3134 | 28.01 ms/step | loss 16387.723 \n",
      "| iter   3135 | 26.03 ms/step | loss 84376.336 \n",
      "| iter   3136 | 28.01 ms/step | loss 1106086.250 \n",
      "| iter   3137 | 23.01 ms/step | loss 6321.823 \n",
      "| iter   3138 | 32.99 ms/step | loss 25901.385 \n",
      "| iter   3139 | 30.01 ms/step | loss 18869.941 \n",
      "| iter   3140 | 25.01 ms/step | loss 85025.328 \n",
      "| iter   3141 | 23.01 ms/step | loss 1229244.625 \n",
      "| iter   3142 | 110.04 ms/step | loss 586013.875 \n",
      "| iter   3143 | 26.99 ms/step | loss 90673.656 \n",
      "| iter   3144 | 27.01 ms/step | loss 143066.938 \n",
      "| iter   3145 | 24.01 ms/step | loss 7145.792 \n",
      "| iter   3146 | 23.01 ms/step | loss 48952.711 \n",
      "| iter   3147 | 27.00 ms/step | loss 15035.119 \n",
      "| iter   3148 | 30.01 ms/step | loss 815207.188 \n",
      "| iter   3149 | 26.00 ms/step | loss 93043.703 \n",
      "| iter   3150 | 26.01 ms/step | loss 12261.262 \n",
      "| iter   3151 | 23.01 ms/step | loss 12077.592 \n",
      "| iter   3152 | 124.04 ms/step | loss 437703.156 \n",
      "| iter   3153 | 24.99 ms/step | loss 27753.910 \n",
      "| iter   3154 | 24.00 ms/step | loss 16355.343 \n",
      "| iter   3155 | 29.01 ms/step | loss 518932.938 \n",
      "| iter   3156 | 25.01 ms/step | loss 26515.811 \n",
      "| iter   3157 | 26.00 ms/step | loss 17280.141 \n",
      "| iter   3158 | 25.01 ms/step | loss 416993.969 \n",
      "| iter   3159 | 27.01 ms/step | loss 470239.781 \n",
      "| iter   3160 | 25.00 ms/step | loss 29557.014 \n",
      "| iter   3161 | 24.02 ms/step | loss 43129.020 \n",
      "| iter   3162 | 31.00 ms/step | loss 26654.658 \n",
      "| iter   3163 | 146.04 ms/step | loss 363664.250 \n",
      "| iter   3164 | 23.01 ms/step | loss 54075.375 \n",
      "| iter   3165 | 22.01 ms/step | loss 12052.118 \n",
      "| iter   3166 | 24.01 ms/step | loss 31061.805 \n",
      "| iter   3167 | 33.01 ms/step | loss 348531.188 \n",
      "| iter   3168 | 28.01 ms/step | loss 19276.766 \n",
      "| iter   3169 | 27.01 ms/step | loss 19781.146 \n",
      "| iter   3170 | 28.01 ms/step | loss 335476.375 \n",
      "| iter   3171 | 123.03 ms/step | loss 19398.004 \n",
      "| iter   3172 | 32.01 ms/step | loss 107533.438 \n",
      "| iter   3173 | 27.00 ms/step | loss 699851.625 \n",
      "| iter   3174 | 34.01 ms/step | loss 37086.613 \n",
      "| iter   3175 | 24.01 ms/step | loss 12831.218 \n",
      "| iter   3176 | 26.02 ms/step | loss 291606.719 \n",
      "| iter   3177 | 28.02 ms/step | loss 8219.324 \n",
      "| iter   3178 | 25.00 ms/step | loss 21219.828 \n",
      "| iter   3179 | 27.01 ms/step | loss 232570.391 \n",
      "| iter   3180 | 102.04 ms/step | loss 29934.918 \n",
      "| iter   3181 | 22.02 ms/step | loss 93983.344 \n",
      "| iter   3182 | 24.02 ms/step | loss 40927.699 \n",
      "| iter   3183 | 27.01 ms/step | loss 42346.922 \n",
      "| iter   3184 | 22.00 ms/step | loss 336653.281 \n",
      "| iter   3185 | 23.00 ms/step | loss 7259.462 \n",
      "| iter   3186 | 25.02 ms/step | loss 16147.721 \n",
      "| iter   3187 | 23.01 ms/step | loss 41956.102 \n",
      "| iter   3188 | 23.01 ms/step | loss 10313.373 \n",
      "| iter   3189 | 115.01 ms/step | loss 270892.875 \n",
      "| iter   3190 | 26.01 ms/step | loss 6778.721 \n",
      "| iter   3191 | 26.01 ms/step | loss 144594.469 \n",
      "| iter   3192 | 23.01 ms/step | loss 2715871.000 \n",
      "| iter   3193 | 24.01 ms/step | loss 765154.000 \n",
      "| iter   3194 | 26.02 ms/step | loss 12926.800 \n",
      "| iter   3195 | 24.01 ms/step | loss 176745.859 \n",
      "| iter   3196 | 25.01 ms/step | loss 16920.039 \n",
      "| iter   3197 | 24.01 ms/step | loss 50028.684 \n",
      "| iter   3198 | 132.03 ms/step | loss 16504.973 \n",
      "| iter   3199 | 28.01 ms/step | loss 199075.859 \n",
      "| iter   3200 | 28.01 ms/step | loss 26479.078 \n",
      "| iter   3201 | 25.01 ms/step | loss 493504.938 \n"
     ]
    }
   ],
   "source": [
    "jo=0 #counter\n",
    "# instantiate an affine path object\n",
    "path = AffineProbPath(scheduler=CondOTScheduler())\n",
    "\n",
    "# init optimizer\n",
    "optim = torch.optim.Adam(vf.parameters(), lr=lr/10)\n",
    "# optim2=torch.optim.Adam(data2noiseTransform.parameters(), lr=lr)\n",
    "# train\n",
    "start_time = time.time()\n",
    "for i in range(iterations):\n",
    "    for data in train_loader:\n",
    "        optim.zero_grad()\n",
    "        # optim2.zero_grad()\n",
    "        # sample data : in this case, (X_0,X_1) ~ pi(X_0,X_1) = N(X_0|0,I)q(X_1)\n",
    "        # print(data[0].shape)\n",
    "        x_1_H=data[0].float().to(device)\n",
    "        x_0,ob =sample_stdentt([0,0],[[1,0],[0,1]],Data_Splt['metadata']['dfs'][0],x_1_H.shape[0]) #torch.randn_like(x_1_H).float().to(device)\n",
    "        x_0=x_0.float().to(device)\n",
    "        x_1=x_1_H#data2noiseTransform(x_1_H)[0] #Heavy tail to light tail\n",
    "       \n",
    "\n",
    "\n",
    "        # sample time \n",
    "        t = torch.rand(x_1.shape[0]).to(device)\n",
    "\n",
    "        # sample probability path\n",
    "        path_sample = path.sample(t=t, x_0=x_0, x_1=x_1)\n",
    "        # print(vf(path_sample.x_t,path_sample.t))\n",
    "\n",
    "        # flow matching l2 loss\n",
    "        \n",
    "        loss = torch.pow( vf(path_sample.x_t,path_sample.t) - path_sample.dx_t, 2).mean()\n",
    "\n",
    "        # optimizer step\n",
    "        loss.backward() # backward\n",
    "        optim.step() # update\n",
    "        # optim2.step()\n",
    "\n",
    "    # log loss\n",
    "        jo=jo+1\n",
    "        if (jo+1) % print_every == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| iter {:6d} | {:5.2f} ms/step | loss {:8.3f} '\n",
    "                .format(jo+1, elapsed*1000/print_every, loss.item()))\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24130f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedModel(ModelWrapper):\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, **extras):\n",
    "        return self.model(x, t)\n",
    "\n",
    "wrapped_vf = WrappedModel(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48dd6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step size for ode solver\n",
    "step_size = 0.05\n",
    "\n",
    "norm = cm.colors.Normalize(vmax=50, vmin=0)\n",
    "\n",
    "batch_size = 10000  # batch size\n",
    "eps_time = 1e-2\n",
    "T = torch.linspace(0,1,10)  # sample times\n",
    "T = T.to(device=device)\n",
    "\n",
    "x_init,ob = sample_stdentt([0,0],[[1,0],[0,1]],Data_Splt['metadata']['dfs'][0],batch_size) #torch.randn((batch_size, 2), dtype=torch.float32, device=device)\n",
    "x_init=x_init.to(device)\n",
    "solver = ODESolver(velocity_model=wrapped_vf)  # create an ODESolver class\n",
    "sol = solver.sample(time_grid=T, x_init=x_init, method='midpoint', step_size=step_size, return_intermediates=True)  # sample from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cf1ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # sol[i]=reverse_normalization(data2noiseTransform.inverse(torch.tensor(sol[i]).to(device))[0].detach().cpu())\n",
    "    sol[i]=sol[i].to('cpu')\n",
    "    # sol = sol.cpu().numpy()\n",
    "T = T.cpu()\n",
    "# sol[i].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5046eb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1369.0267, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sol[9]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "755b730d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 11 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#VISUALIZE PATH\n",
    "\n",
    "RR=100\n",
    "fig, axs = plt.subplots(1, 11,figsize=(20,20))\n",
    "\n",
    "for i in range(11):\n",
    "    # H= axs[i].hist2d(sol[i,:,0].cpu(), sol[i,:,1].cpu(), 300, range=((-1*RR,RR), (-1*RR,RR)))\n",
    "\n",
    "    cmin = 0.0\n",
    "    cmax = 0.99#torch.quantile(torch.from_numpy(H[0]), 0.99).item()\n",
    "\n",
    "    norm = cm.colors.Normalize(vmax=cmax, vmin=cmin)\n",
    "    #data2noiseTransform.inverse(torch.tensor(sol[i]).to(device))[0].detach().cpu()\n",
    "    # print(img.shape)\n",
    "    if i==10:\n",
    "        img=x_tst\n",
    "        _ = axs[i].hist2d(img[:,0].cpu(), img[:,1].cpu(), 300, range=((-1*RR,RR), (-1*RR,RR)), norm=norm)\n",
    "\n",
    "    else:\n",
    "        img=sol[i]\n",
    "        _ = axs[i].hist2d(img[:,0].cpu(), img[:,1].cpu(), 300, range=((-1*RR,RR), (-1*RR,RR)), norm=norm)\n",
    "\n",
    "    axs[i].set_aspect('equal')\n",
    "    axs[i].axis('off')\n",
    "    if i==10:\n",
    "        axs[i].set_title('target')\n",
    "    else:\n",
    "        axs[i].set_title('t= %.2f' % (T[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.text(0.5, 0.45, 'VANILLA +Stduent T base DOF'+str(dof_og), ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cdcdf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "# sample with likelihood\n",
    "\n",
    "T = torch.tensor([1., 0.])  # sample times\n",
    "T = T.to(device=device)\n",
    "\n",
    "# grid_size = 200\n",
    "# x_1 = torch.meshgrid(torch.linspace(-60, 60, grid_size), torch.linspace(-60, 60, grid_size))\n",
    "# x_1 = torch.stack([x_1[0].flatten(), x_1[1].flatten()], dim=1).to(device)\n",
    "x_1=x_tst[0:1000,:].to(device)\n",
    "print(x_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ba55cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd.functional import jacobian\n",
    "from torch.distributions import Independent, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "223728c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def noise2data_and_reverse_norm(x): #Beware we want likelihood in the space of the actual distribution\n",
    "#     return(reverse_normalization(data2noiseTransform.inverse(x)[0])) \n",
    "\n",
    "# mean=mean.to(device)\n",
    "# scale=scale.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b567fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31ee5b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultivariateStudentT(df: 0.9689141511917114, loc: torch.Size([2]), scale_tril: torch.Size([2, 2]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  noise2data_and_reverse_norm(x_1).shape\n",
    "ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "034d03fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adity\\OneDrive\\Desktop\\mscThesis\\Code3\\Thesis_FLow\\Distributions2.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ob=MultivariateStudentT(torch.tensor(df).float(),torch.tensor(m).float(),torch.tensor(S).float())\n"
     ]
    }
   ],
   "source": [
    "S1,ob1=sample_stdentt(torch.tensor([0,0]).to(device),torch.tensor([[1,0],[0,1]]).to(device),torch.tensor(Data_Splt['metadata']['dfs'][0]).to(device),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "635a907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functorch import vmap, jacrev\n",
    "# source distribution is an isotropic gaussian\n",
    "st_log_density = ob1.log_prob#Independent(Normal(torch.zeros(2, device=device), torch.ones(2, device=device)), 1).log_prob\n",
    "\n",
    "# compute log likelihood with unbiased hutchinson estimator, average over num_acc\n",
    "num_acc = 10\n",
    "log_p_acc = 0\n",
    "\n",
    "for i in range(num_acc):\n",
    "    _, log_p = solver.compute_likelihood(x_1=x_1, method='midpoint', step_size=step_size, exact_divergence=False, log_p0=st_log_density)\n",
    "    # jacobian = jacrev(noise2data_and_reverse_norm)  # Differentiate param wrt t\n",
    "    # jacobian_term = vmap(jacobian)(data2noiseTransform(x_1)[0]) \n",
    "    # jacobian_term=jacobian(noise2data_and_reverse_norm,data2noiseTransform(x_1)[0]) \n",
    "    # print(jacobian_term.mean())\n",
    "    correct_log_p=log_p#-torch.log(0.000001+torch.abs(torch.linalg.det(jacobian_term)))  #MUST CHECCK AGAIN\n",
    "\n",
    "    log_p_acc += correct_log_p\n",
    "\n",
    "log_p_acc /= num_acc\n",
    "\n",
    "# compute with exact divergence not the hutchinson estimator\n",
    "# _, exact_log_p = solver.compute_likelihood(x_1=x_1, method='midpoint', step_size=step_size, exact_divergence=True, log_p0=gaussian_log_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6f49717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x17fb9765f70>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_1[:,0].cpu(),x_1[:,1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0576435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loglikelihood tensor(-14.8096, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Loglikelihood\",log_p_acc.mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
